# üçΩÔ∏è Restaurant Object Detection with YOLOv11

> **üìñ README Languages / –Ø–∑—ã–∫–∏ README**  
> This README is available in two languages:  
> ‚Ä¢ [üá∑üá∫ Russian Version](#russian-version) (–†—É—Å—Å–∫–∞—è –≤–µ—Ä—Å–∏—è)  
> ‚Ä¢ [üá∫üá∏ English Version](#english-version) (English –≤–µ—Ä—Å–∏—è)

---

## üõ†Ô∏è **Technology Stack**

<div align="center">

![Python](https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white)
![PyTorch](https://img.shields.io/badge/PyTorch-EE4C2C?style=for-the-badge&logo=pytorch&logoColor=white)
![OpenCV](https://img.shields.io/badge/OpenCV-27338e?style=for-the-badge&logo=OpenCV&logoColor=white)
![CUDA](https://img.shields.io/badge/CUDA-76B900?style=for-the-badge&logo=nvidia&logoColor=white)
![Ultralytics](https://img.shields.io/badge/YOLOv11-00FFFF?style=for-the-badge&logo=yolo&logoColor=black)

![NumPy](https://img.shields.io/badge/NumPy-013243?style=for-the-badge&logo=numpy&logoColor=white)
![Pandas](https://img.shields.io/badge/Pandas-150458?style=for-the-badge&logo=pandas&logoColor=white)
![Matplotlib](https://img.shields.io/badge/Matplotlib-11557c?style=for-the-badge&logo=python&logoColor=white)
![GroundingDINO](https://img.shields.io/badge/GroundingDINO-FF6B35?style=for-the-badge&logo=ai&logoColor=white)

</div>

---

# Russian Version

## üß† –ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–µ—Ç–µ–∫—Ü–∏–∏ –æ–±—ä–µ–∫—Ç–æ–≤ –≤ —Ä–µ—Å—Ç–æ—Ä–∞–Ω–∞—Ö

**–í—ã—Å–æ–∫–æ–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–µ—Ç–µ–∫—Ü–∏–∏ –æ–±—ä–µ–∫—Ç–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º YOLOv11 –∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ GroundingDINO –¥–ª—è —Ä–µ—Å—Ç–æ—Ä–∞–Ω–Ω–æ–π —Å—Ä–µ–¥—ã**

**–ù–∞–π–¥–∏—Ç–µ –æ—Ç—á–µ—Ç [–∑–¥–µ—Å—å](/final_report.md)**

[![GitHub](https://img.shields.io/badge/GitHub-amir2628-181717?style=flat-square&logo=github)](https://github.com/amir2628/restaurant-object-detection)
[![Python](https://img.shields.io/badge/Python-3.8+-3776AB?style=flat-square&logo=python&logoColor=white)](https://python.org)
[![YOLOv11](https://img.shields.io/badge/YOLOv11-Ultralytics-00FFFF?style=flat-square)](https://github.com/ultralytics/ultralytics)
[![GroundingDINO](https://img.shields.io/badge/GroundingDINO-IDEA--Research-FF6B35?style=flat-square)](https://github.com/IDEA-Research/GroundingDINO)
[![License](https://img.shields.io/badge/License-MIT-green?style=flat-square)](LICENSE)

## üìã –û–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞

–ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –¥–µ—Ç–µ–∫—Ü–∏–∏ –æ–±—ä–µ–∫—Ç–æ–≤, —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è –¥–ª—è —Ä–µ—Å—Ç–æ—Ä–∞–Ω–Ω–æ–π —Å—Ä–µ–¥—ã. –°–∏—Å—Ç–µ–º–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É YOLOv11 –∏ –≤–∫–ª—é—á–∞–µ—Ç **—Ä–µ–≤–æ–ª—é—Ü–∏–æ–Ω–Ω—ã–π –º–µ—Ö–∞–Ω–∏–∑–º –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ —Å GroundingDINO**, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å —Ç—ã—Å—è—á–∏ –∫–∞–¥—Ä–æ–≤ –≤–∏–¥–µ–æ –±–µ–∑ —Ä—É—á–Ω–æ–π —Ä–∞–∑–º–µ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö.

### üåü –ö–ª—é—á–µ–≤—ã–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏

- **ü§ñ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è GroundingDINO**: –£—Å—Ç—Ä–∞–Ω—è–µ—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å —Ä—É—á–Ω–æ–π —Ä–∞–∑–º–µ—Ç–∫–∏ —Ç—ã—Å—è—á –∫–∞–¥—Ä–æ–≤
- **‚ö° YOLOv11**: –ü–æ—Å–ª–µ–¥–Ω—è—è –≤–µ—Ä—Å–∏—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã YOLO –¥–ª—è –≤—ã—Å–æ–∫–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏ –∏ —Å–∫–æ—Ä–æ—Å—Ç–∏
- **üéØ –°–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞ —Ä–µ—Å—Ç–æ—Ä–∞–Ω–∞—Ö**: –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏ –µ–¥—ã –∏ –ø–æ—Å—É–¥—ã
- **üìä –ü–æ–ª–Ω—ã–π ML –ø–∞–π–ø–ª–∞–π–Ω**: –û—Ç –≤–∏–¥–µ–æ –¥–æ –≥–æ—Ç–æ–≤–æ–π –º–æ–¥–µ–ª–∏
- **üöÄ GPU —É—Å–∫–æ—Ä–µ–Ω–∏–µ**: –ü–æ–¥–¥–µ—Ä–∂–∫–∞ CUDA –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏

### üçï –î–µ—Ç–µ–∫—Ç–∏—Ä—É–µ–º—ã–µ –æ–±—ä–µ–∫—Ç—ã

–°–∏—Å—Ç–µ–º–∞ –æ–±—É—á–µ–Ω–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞—Ç—å 10 –∫–ª—é—á–µ–≤—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π —Ä–µ—Å—Ç–æ—Ä–∞–Ω–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤:
- **–ï–¥–∞**: `chicken` (–∫—É—Ä–∏—Ü–∞), `meat` (–º—è—Å–æ), `salad` (—Å–∞–ª–∞—Ç), `soup` (—Å—É–ø)
- **–ü–æ—Å—É–¥–∞**: `cup` (—á–∞—à–∫–∞), `plate` (—Ç–∞—Ä–µ–ª–∫–∞), `bowl` (–º–∏—Å–∫–∞)
- **–ü—Ä–∏–±–æ—Ä—ã**: `spoon` (–ª–æ–∂–∫–∞), `fork` (–≤–∏–ª–∫–∞), `knife` (–Ω–æ–∂)

## üìÅ –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞

```
restaurant-object-detection/
‚îú‚îÄ‚îÄ üìÅ config/
‚îÇ   ‚îú‚îÄ‚îÄ pipeline_config.json           # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø–∞–π–ø–ª–∞–π–Ω–∞
‚îÇ   ‚îî‚îÄ‚îÄ model_config.yaml             # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏
‚îú‚îÄ‚îÄ üìÅ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ fix_annotations.py            # üîß –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π
‚îÇ   ‚îú‚îÄ‚îÄ prepare_data.py               # üìä –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö —Å GroundingDINO
‚îÇ   ‚îú‚îÄ‚îÄ train_model.py                # üöÄ –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
‚îÇ   ‚îî‚îÄ‚îÄ run_inference.py              # üéØ –ò–Ω—Ñ–µ—Ä–µ–Ω—Å
‚îú‚îÄ‚îÄ üìÅ src/
‚îÇ   ‚îú‚îÄ‚îÄ data/                         # –ú–æ–¥—É–ª–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö
‚îÇ   ‚îú‚îÄ‚îÄ models/                       # –ú–æ–¥–µ–ª–∏ –∏ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å
‚îÇ   ‚îî‚îÄ‚îÄ utils/                        # –£—Ç–∏–ª–∏—Ç—ã
‚îú‚îÄ‚îÄ üìÅ data/
‚îÇ   ‚îú‚îÄ‚îÄ raw/                          # –ò—Å—Ö–æ–¥–Ω—ã–µ –≤–∏–¥–µ–æ
‚îÇ   ‚îú‚îÄ‚îÄ processed/dataset/            # –ì–æ—Ç–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç —Å –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è–º–∏
‚îÇ   ‚îî‚îÄ‚îÄ annotations/                  # –ê–Ω–Ω–æ—Ç–∞—Ü–∏–∏ GroundingDINO
‚îú‚îÄ‚îÄ üìÅ outputs/
‚îÇ   ‚îú‚îÄ‚îÄ experiments/                  # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—É—á–µ–Ω–∏—è
‚îÇ   ‚îî‚îÄ‚îÄ inference/                    # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞
‚îú‚îÄ‚îÄ üìÑ groundingdino_swinb_cogcoor.pth # –ú–æ–¥–µ–ª—å GroundingDINO
‚îú‚îÄ‚îÄ üìÅ GroundingDINO/                 # –ò—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥ GroundingDINO
‚îî‚îÄ‚îÄ üìÑ requirements.txt               # –ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
```

## üöÄ –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç

### 1. –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π

```bash
# –ö–ª–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è
git clone https://github.com/amir2628/restaurant-object-detection.git
cd restaurant-object-detection

# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –æ—Å–Ω–æ–≤–Ω—ã—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
pip install -r requirements.txt

# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ GroundingDINO
pip install groundingdino-py

# –ö–ª–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –∫–æ–¥–∞ GroundingDINO
git clone https://github.com/IDEA-Research/GroundingDINO.git

# –°–∫–∞—á–∏–≤–∞–Ω–∏–µ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ GroundingDINO
wget https://github.com/IDEA-Research/GroundingDINO/releases/download/v0.1.0-alpha2/groundingdino_swinb_cogcoor.pth
```

### 2. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∏—Å—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö

**‚ö†Ô∏è –í–ê–ñ–ù–û: –ü–æ–º–µ—Å—Ç–∏—Ç–µ –≤–∞—à–∏ –≤–∏–¥–µ–æ—Ñ–∞–π–ª—ã –≤ –ø—Ä–∞–≤–∏–ª—å–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é!**

```bash
# –°–æ–∑–¥–∞–π—Ç–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è –∏—Å—Ö–æ–¥–Ω—ã—Ö –≤–∏–¥–µ–æ (–µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç)
mkdir -p data/raw

# –ü–æ–º–µ—Å—Ç–∏—Ç–µ –≤–∞—à–∏ –≤–∏–¥–µ–æ—Ñ–∞–π–ª—ã —Ä–µ—Å—Ç–æ—Ä–∞–Ω–∞ –≤ data/raw/
# –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã: .mp4, .avi, .mov, .mkv, .wmv
# –ü—Ä–∏–º–µ—Ä —Å—Ç—Ä—É–∫—Ç—É—Ä—ã:
# data/raw/
# ‚îú‚îÄ‚îÄ restaurant_video_1.mp4
# ‚îú‚îÄ‚îÄ restaurant_video_2.mp4
# ‚îî‚îÄ‚îÄ restaurant_video_3.avi
```

**‚ö†Ô∏è –í–ê–ñ–ù–û: –ú–æ–¥–µ–ª–∏ YOLO11l (Large: `yolo_restaurant_detection_1750941635`) –∏ YOLO11n (nano: `yolo_restaurant_detection_1750973996`) —É–∂–µ –±—ã–ª–∏ –æ–±—É—á–µ–Ω—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –í—ã –º–æ–∂–µ—Ç–µ –ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å –ø–æ–¥–≥–æ—Ç–æ–≤–∫—É –¥–∞–Ω–Ω—ã—Ö –∏ –≤—ã–ø–æ–ª–Ω–∏—Ç—å –∏–Ω—Ñ–µ—Ä–µ–Ω—Å. –ï—Å–ª–∏ –≤—ã —Ö–æ—Ç–∏—Ç–µ –æ–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å YOLO11 —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ, —Ç–æ –≤–∞–º –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –≤—ã–ø–æ–ª–Ω–∏—Ç—å –ø–æ–¥–≥–æ—Ç–æ–≤–∫—É –¥–∞–Ω–Ω—ã—Ö –∏ –∑–∞–ø—É—Å—Ç–∏—Ç—å –æ–±—É—á–µ–Ω–∏–µ –∑–∞–Ω–æ–≤–æ.**

### 3. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –∞–Ω–Ω–æ—Ç–∞—Ü–∏–µ–π GroundingDINO

```bash
# –ü–æ–ª–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω: –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–∞–¥—Ä–æ–≤ + –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è
python scripts/prepare_data.py --input "data/raw" --config "config/pipeline_config.json" --confidence 0.1

# –° –Ω–∞—Å—Ç—Ä–æ–π–∫–æ–π FPS –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–∞–¥—Ä–æ–≤
python scripts/prepare_data.py --input "data/raw" --config "config/pipeline_config.json" --fps 1.5 --confidence 0.2

# –£–≤–µ–ª–∏—á–µ–Ω–∏–µ –ø–æ—Ä–æ–≥–∞ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –¥–ª—è –±–æ–ª–µ–µ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π
python scripts/prepare_data.py --input "data/raw" --config "config/pipeline_config.json" --confidence 0.3
```

**üß† –ß—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –Ω–∞ —ç—Ç–∞–ø–µ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö:**
1. **–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–∞–¥—Ä–æ–≤** –∏–∑ –≤–∏–¥–µ–æ—Ñ–∞–π–ª–æ–≤ —Å –∑–∞–¥–∞–Ω–Ω—ã–º FPS
2. **–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è** –∫–∞–∂–¥–æ–≥–æ –∫–∞–¥—Ä–∞ —Å –ø–æ–º–æ—â—å—é GroundingDINO
3. **–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –¥–µ—Ç–µ–∫—Ü–∏–π** –ø–æ –ø–æ—Ä–æ–≥—É —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
4. **–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/val/test** splits (70%/20%/10%)
5. **–ì–µ–Ω–µ—Ä–∞—Ü–∏—è dataset.yaml** –¥–ª—è –æ–±—É—á–µ–Ω–∏—è YOLO
6. **–°–æ–∑–¥–∞–Ω–∏–µ –æ—Ç—á–µ—Ç–∞** –æ –∫–∞—á–µ—Å—Ç–≤–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π

### 4. –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ YOLOv11

```bash
# –ë–∞–∑–æ–≤–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–æ–∑–¥–∞–Ω–Ω—ã–º –¥–∞—Ç–∞—Å–µ—Ç–æ–º
python scripts/train_model.py --data "data\processed\dataset\dataset.yaml" --device cuda

# –û–±—É—á–µ–Ω–∏–µ —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–æ–π –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —ç–ø–æ—Ö
python scripts/train_model.py --data "data\processed\dataset\dataset.yaml" --device cuda --epochs 200

# –û–±—É—á–µ–Ω–∏–µ —Å –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–æ–º Weights & Biases
python scripts/train_model.py --data "data\processed\dataset\dataset.yaml" --device cuda --wandb

# –û–±—É—á–µ–Ω–∏–µ —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–æ–π —Ä–∞–∑–º–µ—Ä–∞ –±–∞—Ç—á–∞
python scripts/train_model.py --data "data\processed\dataset\dataset.yaml" --device cuda --batch-size 32
```

### 5. –ó–∞–ø—É—Å–∫ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞

```bash
# –ò–Ω—Ñ–µ—Ä–µ–Ω—Å –Ω–∞ –≤–∏–¥–µ–æ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
python scripts/run_inference.py --model "outputs\experiments\yolo_restaurant_detection_[EXPERIMENT_ID]\weights\best.pt" --video "[SOME_VIDEO_OR_DIRECTORY_OF_VIDEOS]" --output "outputs\final_demo" --device cuda

# –ò–Ω—Ñ–µ—Ä–µ–Ω—Å –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö
python scripts/run_inference.py --model "outputs\experiments\yolo_restaurant_detection_[EXPERIMENT_ID]\weights\best.pt" --input-dir "path/to/images" --output "outputs\inference_results"

# Real-time –∏–Ω—Ñ–µ—Ä–µ–Ω—Å —Å –≤–µ–±-–∫–∞–º–µ—Ä—ã
python scripts/run_inference.py --model "outputs\experiments\yolo_restaurant_detection_[EXPERIMENT_ID]\weights\best.pt" --realtime --camera 0

# –ò–Ω—Ñ–µ—Ä–µ–Ω—Å —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–æ–π –ø–æ—Ä–æ–≥–∞ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
python scripts/run_inference.py --model "outputs\experiments\yolo_restaurant_detection_[EXPERIMENT_ID]\weights\best.pt" --video "test_video.mp4" --confidence 0.3 --iou 0.5
```

## ü§ñ GroundingDINO: –†–µ–≤–æ–ª—é—Ü–∏—è –≤ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏

### –ü–æ—á–µ–º—É GroundingDINO?

–¢—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω–∞—è –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è –≤–∏–¥–µ–æ–¥–∞–Ω–Ω—ã—Ö –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏ –æ–±—ä–µ–∫—Ç–æ–≤ - —ç—Ç–æ **–∫—Ä–∞–π–Ω–µ —Ç—Ä—É–¥–æ–∑–∞—Ç—Ä–∞—Ç–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å**:
- üìπ –û–¥–∏–Ω —á–∞—Å –≤–∏–¥–µ–æ = ~108,000 –∫–∞–¥—Ä–æ–≤ (–ø—Ä–∏ 30 FPS)
- ‚è±Ô∏è –†—É—á–Ω–∞—è —Ä–∞–∑–º–µ—Ç–∫–∞ –æ–¥–Ω–æ–≥–æ –∫–∞–¥—Ä–∞ = 2-5 –º–∏–Ω—É—Ç
- üí∞ –û–±—â–µ–µ –≤—Ä–µ–º—è: **3,600-9,000 —á–∞—Å–æ–≤** –Ω–∞ –æ–¥–∏–Ω —á–∞—Å –≤–∏–¥–µ–æ!

**GroundingDINO —Ä–µ—à–∞–µ—Ç —ç—Ç—É –ø—Ä–æ–±–ª–µ–º—É –ø–æ–ª–Ω–æ—Å—Ç—å—é –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏:**

### üîß –ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è GroundingDINO

1. **–¢–µ–∫—Å—Ç–æ–≤—ã–µ –ø—Ä–æ–º–ø—Ç—ã**: –°–∏—Å—Ç–µ–º–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –æ–ø–∏—Å–∞–Ω–∏—è –æ–±—ä–µ–∫—Ç–æ–≤
   ```
   "chicken . meat . salad . soup . cup . plate . bowl . spoon . fork . knife ."
   ```

2. **–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –¥–µ—Ç–µ–∫—Ü–∏—è**: GroundingDINO –Ω–∞—Ö–æ–¥–∏—Ç –æ–±—ä–µ–∫—Ç—ã –ø–æ —Ç–µ–∫—Å—Ç–æ–≤–æ–º—É –æ–ø–∏—Å–∞–Ω–∏—é

3. **–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –∫–∞—á–µ—Å—Ç–≤–∞**: –£–¥–∞–ª–µ–Ω–∏–µ –¥–µ—Ç–µ–∫—Ü–∏–π —Å –Ω–∏–∑–∫–æ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é

4. **YOLO —Ñ–æ—Ä–º–∞—Ç**: –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ —Ñ–æ—Ä–º–∞—Ç –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π YOLO

### ‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏ GroundingDINO (config/pipeline_config.json)

```json
{
  "annotation": {
    "method": "groundingdino",
    "confidence_threshold": 0.25,
    "text_threshold": 0.25,
    "box_threshold": 0.25,
    "detection_prompt": "chicken . meat . salad . soup . cup . plate . bowl . spoon . fork . knife .",
    "iou_threshold": 0.6
  },
  "groundingdino": {
    "checkpoint_path": "groundingdino_swinb_cogcoor.pth",
    "config_paths": [
      "GroundingDINO/groundingdino/config/GroundingDINO_SwinB_cfg.py"
    ]
  }
}
```

### üéØ –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏

- **‚ö° –°–∫–æ—Ä–æ—Å—Ç—å**: 1000+ –∫–∞–¥—Ä–æ–≤ –≤ —á–∞—Å –≤–º–µ—Å—Ç–æ 10-20 –ø—Ä–∏ —Ä—É—á–Ω–æ–π —Ä–∞–∑–º–µ—Ç–∫–µ
- **üí∞ –≠–∫–æ–Ω–æ–º–∏—è**: –°–Ω–∏–∂–µ–Ω–∏–µ –∑–∞—Ç—Ä–∞—Ç –Ω–∞ –∞–Ω–Ω–æ—Ç–∞—Ü–∏—é –≤ 100+ —Ä–∞–∑
- **üéØ –ö–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å**: –ï–¥–∏–Ω—ã–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç—ã —Ä–∞–∑–º–µ—Ç–∫–∏ –¥–ª—è –≤—Å–µ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞  
- **üìà –ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å**: –õ–µ–≥–∫–æ–µ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –Ω–æ–≤—ã—Ö –≤–∏–¥–µ–æ –∏ –∫–ª–∞—Å—Å–æ–≤

## üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—É—á–µ–Ω–∏—è

### üèÜ –î–æ—Å—Ç–∏–≥–Ω—É—Ç—ã–µ –º–µ—Ç—Ä–∏–∫–∏

–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—É—á–µ–Ω–∏—è YOLOv11 –Ω–∞ GPU (500 —ç–ø–æ—Ö, 87.3 –º–∏–Ω—É—Ç—ã):

```
============================================================
üéØ –ò–¢–û–ì–ò –û–ë–£–ß–ï–ù–ò–Ø
============================================================
üìÅ –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç: yolo_restaurant_detection_1750973996
‚è±Ô∏è –í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: 87.3 –º–∏–Ω—É—Ç
üîÑ –≠–ø–æ—Ö –∑–∞–≤–µ—Ä—à–µ–Ω–æ: 500
üíª –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: 0
üìä –§–∏–Ω–∞–ª—å–Ω—ã–π mAP@0.5: 0.7478
üìä –§–∏–Ω–∞–ª—å–Ω—ã–π mAP@0.5:0.95: 0.7055
üíé –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å: outputs\experiments\yolo_restaurant_detection_1750973996\weights\best.pt
============================================================

==================================================
üéâ –û–ë–£–ß–ï–ù–ò–ï YOLO11 –£–°–ü–ï–®–ù–û –ó–ê–í–ï–†–®–ï–ù–û!
==================================================
üíé –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å: outputs\experiments\yolo_restaurant_detection_1750973996\weights\best.pt
üìä mAP@0.5: 74.8%
‚è±Ô∏è –í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: 87.3 –º–∏–Ω
```

| –ú–µ—Ç—Ä–∏–∫–∞ | –ó–Ω–∞—á–µ–Ω–∏–µ | –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π |
|---------|----------|-------------|
| **mAP@0.5** | **74.8%** | ü•á –û—Ç–ª–∏—á–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç |
| **mAP@0.5:0.95** | **70.6%** | ü•à –í—ã—Å–æ–∫–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å |
| **–°–∫–æ—Ä–æ—Å—Ç—å –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞** | **~2ms** | ‚ö° Real-time –æ–±—Ä–∞–±–æ—Ç–∫–∞ |
| **–†–∞–∑–º–µ—Ä –º–æ–¥–µ–ª–∏** | **~6MB** | üì¶ –ö–æ–º–ø–∞–∫—Ç–Ω–∞—è |
| **–í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è** | **87.3 –º–∏–Ω** | üöÄ –ë—ã—Å—Ç—Ä–æ–µ –æ–±—É—á–µ–Ω–∏–µ |

<img width="1280" alt="Image" src="https://github.com/user-attachments/assets/acc152e7-2ed4-485e-a824-da97a6c7bef3" />

### üìà –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏

- **ü§ñ GroundingDINO –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è** - –ü–æ–ª–Ω–æ—Å—Ç—å—é –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Ä–∞–∑–º–µ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö
- **üéØ –£–º–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è** - –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —É–¥–∞–ª–µ–Ω–∏–µ –Ω–µ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –¥–µ—Ç–µ–∫—Ü–∏–π
- **‚ö° GPU –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è** - CUDA, AMP, –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –±–∞—Ç—á–∏
- **üìä Comprehensive –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥** - –î–µ—Ç–∞–ª—å–Ω—ã–µ –æ—Ç—á–µ—Ç—ã –æ –ø—Ä–æ—Ü–µ—Å—Å–µ

## üõ†Ô∏è –°–∏—Å—Ç–µ–º–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è

- **Python:** 3.8+
- **GPU:** CUDA 11.0+ (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)
- **RAM:** 8GB+
- **GPU –ø–∞–º—è—Ç—å:** 4GB+ (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)
- **–ú–µ—Å—Ç–æ –Ω–∞ –¥–∏—Å–∫–µ:** 10GB+
- **GroundingDINO –º–æ–¥–µ–ª—å:** ~1.8GB

## üìù –õ–∏—Ü–µ–Ω–∑–∏—è

MIT License - —Å–º. [LICENSE](LICENSE) —Ñ–∞–π–ª.

## üë• –ê–≤—Ç–æ—Ä

**Amir** - [@amir2628](https://github.com/amir2628)

---

# English Version

## üß† Professional Restaurant Object Detection System

**High-performance object detection system using YOLOv11 with automated GroundingDINO annotation for restaurant environments**

**Find the report [here](/final_report.md)**

[![GitHub](https://img.shields.io/badge/GitHub-amir2628-181717?style=flat-square&logo=github)](https://github.com/amir2628/restaurant-object-detection)
[![Python](https://img.shields.io/badge/Python-3.8+-3776AB?style=flat-square&logo=python&logoColor=white)](https://python.org)
[![YOLOv11](https://img.shields.io/badge/YOLOv11-Ultralytics-00FFFF?style=flat-square)](https://github.com/ultralytics/ultralytics)
[![GroundingDINO](https://img.shields.io/badge/GroundingDINO-IDEA--Research-FF6B35?style=flat-square)](https://github.com/IDEA-Research/GroundingDINO)
[![License](https://img.shields.io/badge/License-MIT-green?style=flat-square)](LICENSE)

## üìã Project Description

Professional automatic object detection system specifically designed for restaurant environments. The system uses the modern YOLOv11 architecture and includes a **revolutionary automatic annotation mechanism with GroundingDINO**, eliminating the need for manual annotation of thousands of video frames.

### üåü Key Features

- **ü§ñ Automatic GroundingDINO Annotation**: Eliminates the need for manual annotation of thousands of frames
- **‚ö° YOLOv11**: Latest YOLO architecture for high accuracy and speed
- **üéØ Restaurant Specialization**: Optimized for food and tableware detection
- **üìä Complete ML Pipeline**: From video to ready-to-use model
- **üöÄ GPU Acceleration**: CUDA support for fast processing

### üçï Detectable Objects

The system is trained to recognize 10 key categories of restaurant objects:
- **Food**: `chicken`, `meat`, `salad`, `soup`
- **Tableware**: `cup`, `plate`, `bowl`
- **Utensils**: `spoon`, `fork`, `knife`

## üìÅ Project Structure

```
restaurant-object-detection/
‚îú‚îÄ‚îÄ üìÅ config/
‚îÇ   ‚îú‚îÄ‚îÄ pipeline_config.json           # Pipeline configuration
‚îÇ   ‚îî‚îÄ‚îÄ model_config.yaml             # Model parameters
‚îú‚îÄ‚îÄ üìÅ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ fix_annotations.py            # üîß Annotation fixing
‚îÇ   ‚îú‚îÄ‚îÄ prepare_data.py               # üìä Data preparation with GroundingDINO
‚îÇ   ‚îú‚îÄ‚îÄ train_model.py                # üöÄ Model training
‚îÇ   ‚îî‚îÄ‚îÄ run_inference.py              # üéØ Inference
‚îú‚îÄ‚îÄ üìÅ src/
‚îÇ   ‚îú‚îÄ‚îÄ data/                         # Data processing modules
‚îÇ   ‚îú‚îÄ‚îÄ models/                       # Models and inference
‚îÇ   ‚îî‚îÄ‚îÄ utils/                        # Utilities
‚îú‚îÄ‚îÄ üìÅ data/
‚îÇ   ‚îú‚îÄ‚îÄ raw/                          # Source videos
‚îÇ   ‚îú‚îÄ‚îÄ processed/dataset/            # Ready dataset with annotations
‚îÇ   ‚îî‚îÄ‚îÄ annotations/                  # GroundingDINO annotations
‚îú‚îÄ‚îÄ üìÅ outputs/
‚îÇ   ‚îú‚îÄ‚îÄ experiments/                  # Training results
‚îÇ   ‚îî‚îÄ‚îÄ inference/                    # Inference results
‚îú‚îÄ‚îÄ üìÑ groundingdino_swinb_cogcoor.pth # GroundingDINO model
‚îú‚îÄ‚îÄ üìÅ GroundingDINO/                 # GroundingDINO source code
‚îî‚îÄ‚îÄ üìÑ requirements.txt               # Dependencies
```

## üöÄ Quick Start

### 1. Installation

```bash
# Clone repository
git clone https://github.com/amir2628/restaurant-object-detection.git
cd restaurant-object-detection

# Install main dependencies
pip install -r requirements.txt

# Install GroundingDINO
pip install groundingdino-py

# Clone GroundingDINO source code
git clone https://github.com/IDEA-Research/GroundingDINO.git

# Download pre-trained GroundingDINO model
wget https://github.com/IDEA-Research/GroundingDINO/releases/download/v0.1.0-alpha2/groundingdino_swinb_cogcoor.pth
```

### 2. Prepare Source Data

**‚ö†Ô∏è IMPORTANT: Place your video files in the correct directory!**

```bash
# Create directory for source videos (if it doesn't exist)
mkdir -p data/raw

# Place your restaurant video files in data/raw/
# Supported formats: .mp4, .avi, .mov, .mkv, .wmv
# Example structure:
# data/raw/
# ‚îú‚îÄ‚îÄ restaurant_video_1.mp4
# ‚îú‚îÄ‚îÄ restaurant_video_2.mp4
# ‚îî‚îÄ‚îÄ restaurant_video_3.avi
```

**‚ö†Ô∏è IMPORTANT: A YOLO11l (Large: `yolo_restaurant_detection_1750941635`) and a YOLO11n (nano: `yolo_restaurant_detection_1750973996`) has already been trained on the test data which was provided. You can skip the data preparation and run the inference. If you want to train a YOLO 11 model yourself, then you have to do the data preparation and run the training again.**

### 3. Data Preparation with Automatic GroundingDINO Annotation

```bash
# Full pipeline: frame extraction + automatic annotation
python scripts/prepare_data.py --input "data/raw" --config "config/pipeline_config.json" --confidence 0.1

# With FPS setting for frame extraction
python scripts/prepare_data.py --input "data/raw" --config "config/pipeline_config.json" --fps 1.5 --confidence 0.2

# Increase confidence threshold for higher quality annotations
python scripts/prepare_data.py --input "data/raw" --config "config/pipeline_config.json" --confidence 0.3
```

**üß† What happens during data preparation:**
1. **Frame extraction** from video files at specified FPS
2. **Automatic annotation** of each frame using GroundingDINO
3. **Detection filtering** by confidence threshold
4. **Train/val/test split** (70%/20%/10%)
5. **dataset.yaml generation** for YOLO training
6. **Quality report creation** about annotations

### 4. YOLOv11 Model Training

```bash
# Basic training with automatically created dataset
python scripts/train_model.py --data "data\processed\dataset\dataset.yaml" --device cuda

# Training with custom epoch count
python scripts/train_model.py --data "data\processed\dataset\dataset.yaml" --device cuda --epochs 200

# Training with Weights & Biases monitoring
python scripts/train_model.py --data "data\processed\dataset\dataset.yaml" --device cuda --wandb

# Training with custom batch size
python scripts/train_model.py --data "data\processed\dataset\dataset.yaml" --device cuda --batch-size 32
```

### 5. Run Inference

```bash
# Video inference with result saving
python scripts/run_inference.py --model "outputs\experiments\yolo_restaurant_detection_[EXPERIMENT_ID]\weights\best.pt" --video "[SOME_VIDEO_OR_DIRECTORY_OF_VIDEOS]" --output "outputs\final_demo" --device cuda

# Image inference
python scripts/run_inference.py --model "outputs\experiments\yolo_restaurant_detection_[EXPERIMENT_ID]\weights\best.pt" --input-dir "path/to/images" --output "outputs\inference_results"

# Real-time inference from webcam
python scripts/run_inference.py --model "outputs\experiments\yolo_restaurant_detection_[EXPERIMENT_ID]\weights\best.pt" --realtime --camera 0

# Inference with confidence threshold settings
python scripts/run_inference.py --model "outputs\experiments\yolo_restaurant_detection_[EXPERIMENT_ID]\weights\best.pt" --video "test_video.mp4" --confidence 0.3 --iou 0.5
```

## ü§ñ GroundingDINO: Revolution in Automatic Annotation

### Why GroundingDINO?

Traditional video data annotation for object detection is an **extremely labor-intensive process**:
- üìπ One hour of video = ~108,000 frames (at 30 FPS)
- ‚è±Ô∏è Manual annotation of one frame = 2-5 minutes
- üí∞ Total time: **3,600-9,000 hours** for one hour of video!

**GroundingDINO solves this problem completely automatically:**

### üîß How GroundingDINO Integration Works

1. **Text Prompts**: System uses natural object descriptions
   ```
   "chicken . meat . salad . soup . cup . plate . bowl . spoon . fork . knife ."
   ```

2. **Automatic Detection**: GroundingDINO finds objects based on text descriptions

3. **Quality Filtering**: Removal of low-confidence detections

4. **YOLO Format**: Automatic conversion to YOLO annotation format

### ‚öôÔ∏è GroundingDINO Settings (config/pipeline_config.json)

```json
{
  "annotation": {
    "method": "groundingdino",
    "confidence_threshold": 0.25,
    "text_threshold": 0.25,
    "box_threshold": 0.25,
    "detection_prompt": "chicken . meat . salad . soup . cup . plate . bowl . spoon . fork . knife .",
    "iou_threshold": 0.6
  },
  "groundingdino": {
    "checkpoint_path": "groundingdino_swinb_cogcoor.pth",
    "config_paths": [
      "GroundingDINO/groundingdino/config/GroundingDINO_SwinB_cfg.py"
    ]
  }
}
```

### üéØ Advantages of Automatic Annotation

- **‚ö° Speed**: 1000+ frames per hour instead of 10-20 with manual annotation
- **üí∞ Cost Savings**: 100+ times reduction in annotation costs
- **üéØ Consistency**: Uniform annotation standards across the entire dataset
- **üìà Scalability**: Easy addition of new videos and classes

## üìä Training Results

### üèÜ Achieved Metrics

YOLOv11 training results on GPU (500 epochs, 87.3 minutes):

```
============================================================
üéØ TRAINING SUMMARY
============================================================
üìÅ Experiment: yolo_restaurant_detection_1750973996
‚è±Ô∏è Training time: 87.3 minutes
üîÑ Epochs completed: 500
üíª Device: 0
üìä Final mAP@0.5: 0.7478
üìä Final mAP@0.5:0.95: 0.7055
üíé Best model: outputs\experiments\yolo_restaurant_detection_1750973996\weights\best.pt
============================================================

==================================================
üéâ YOLO11 TRAINING COMPLETED SUCCESSFULLY!
==================================================
üíé Best model: outputs\experiments\yolo_restaurant_detection_1750973996\weights\best.pt
üìä mAP@0.5: 74.8%
‚è±Ô∏è Training time: 87.3 min
```

| Metric | Value | Comment |
|--------|-------|---------|
| **mAP@0.5** | **74.8%** | ü•á Excellent result |
| **mAP@0.5:0.95** | **70.6%** | ü•à High accuracy |
| **Inference Speed** | **~2ms** | ‚ö° Real-time processing |
| **Model Size** | **~6MB** | üì¶ Compact |
| **Training Time** | **87.3 min** | üöÄ Fast training |

<img width="1280" alt="Image" src="https://github.com/user-attachments/assets/acc152e7-2ed4-485e-a824-da97a6c7bef3" />

### üìà Implementation Features

- **ü§ñ GroundingDINO Annotation** - Fully automatic data annotation
- **üéØ Smart Filtering** - Automatic removal of low-quality detections
- **‚ö° GPU Optimization** - CUDA, AMP, optimized batching
- **üìä Comprehensive Monitoring** - Detailed process reports

## üõ†Ô∏è System Requirements

- **Python:** 3.8+
- **GPU:** CUDA 11.0+ (recommended)
- **RAM:** 8GB+
- **GPU Memory:** 4GB+ (recommended)
- **Disk Space:** 10GB+
- **GroundingDINO Model:** ~1.8GB

## ü§ù Contributing

1. Fork the repository
2. Create feature branch (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)
5. Open Pull Request

## üìù License

MIT License - see [LICENSE](LICENSE) file.

## üôè Acknowledgments

- [Ultralytics](https://github.com/ultralytics/ultralytics) for YOLOv11
- [IDEA-Research](https://github.com/IDEA-Research/GroundingDINO) for GroundingDINO
- Open-source community for tools and libraries

## üë• Author

**Amir** - [@amir2628](https://github.com/amir2628)

## üöÄ Advanced Usage

### Additional Command Options

#### Data Preparation Advanced Options

```bash
# Extract frames at different FPS rates
python scripts/prepare_data.py --input "data/raw" --config "config/pipeline_config.json" --fps 0.5  # Lower FPS for fewer frames
python scripts/prepare_data.py --input "data/raw" --config "config/pipeline_config.json" --fps 5.0  # Higher FPS for more frames

# Adjust confidence threshold for annotation quality
python scripts/prepare_data.py --input "data/raw" --config "config/pipeline_config.json" --confidence 0.05  # More detections, lower quality
python scripts/prepare_data.py --input "data/raw" --config "config/pipeline_config.json" --confidence 0.4   # Fewer detections, higher quality

# Custom output directory
python scripts/prepare_data.py --input "data/raw" --config "config/pipeline_config.json" --output "data/custom_dataset"
```

#### Training Advanced Options

```bash
# Training with different model sizes
python scripts/train_model.py --data "data/processed/dataset/dataset.yaml" --device cuda --model yolo11n  # Nano (fastest)
python scripts/train_model.py --data "data/processed/dataset/dataset.yaml" --device cuda --model yolo11s  # Small
python scripts/train_model.py --data "data/processed/dataset/dataset.yaml" --device cuda --model yolo11m  # Medium
python scripts/train_model.py --data "data/processed/dataset/dataset.yaml" --device cuda --model yolo11l  # Large
python scripts/train_model.py --data "data/processed/dataset/dataset.yaml" --device cuda --model yolo11x  # Extra Large

# Training with custom image size
python scripts/train_model.py --data "data/processed/dataset/dataset.yaml" --device cuda --imgsz 512   # Smaller images, faster training
python scripts/train_model.py --data "data/processed/dataset/dataset.yaml" --device cuda --imgsz 1024  # Larger images, better accuracy

# Training with custom learning rate and optimization
python scripts/train_model.py --data "data/processed/dataset/dataset.yaml" --device cuda --lr0 0.001 --optimizer AdamW

# Resume training from checkpoint
python scripts/train_model.py --data "data/processed/dataset/dataset.yaml" --device cuda --resume "outputs/experiments/yolo_*/weights/last.pt"

# Training with data augmentation settings
python scripts/train_model.py --data "data/processed/dataset/dataset.yaml" --device cuda --augment --mixup 0.2 --copy_paste 0.1
```

#### Inference Advanced Options

```bash
# Batch inference on multiple videos
python scripts/run_inference.py --model "outputs/experiments/yolo_*/weights/best.pt" --input-dir "path/to/videos" --output "results" --device cuda

# Inference with custom confidence and IoU thresholds
python scripts/run_inference.py --model "outputs/experiments/yolo_*/weights/best.pt" --video "test.mp4" --confidence 0.1 --iou 0.3 --output "low_confidence_results"

# Inference with specific classes only
python scripts/run_inference.py --model "outputs/experiments/yolo_*/weights/best.pt" --video "test.mp4" --classes 0 1 2  # Only detect first 3 classes

# Save inference results in different formats
python scripts/run_inference.py --model "outputs/experiments/yolo_*/weights/best.pt" --video "test.mp4" --save-json --save-txt --save-crop

# Inference with video output settings
python scripts/run_inference.py --model "outputs/experiments/yolo_*/weights/best.pt" --video "test.mp4" --output "results" --fps 15 --quality high
```

#### Annotation Fixing Advanced Options

```bash
# Fix annotations for specific splits only
python scripts/fix_annotations.py --dataset "data/processed/dataset" --splits train val --auto-annotate

# Fix with different confidence thresholds
python scripts/fix_annotations.py --dataset "data/processed/dataset" --auto-annotate --confidence 0.15

# Create dataset structure and annotate in one step
python scripts/fix_annotations.py --dataset "data/new_dataset" --create-structure --auto-annotate --confidence 0.2

# Overwrite existing annotations
python scripts/fix_annotations.py --dataset "data/processed/dataset" --auto-annotate --overwrite --confidence 0.3
```

### üîß Configuration Customization

#### Custom Detection Classes

To detect different objects, modify `config/pipeline_config.json`:

```json
{
  "annotation": {
    "detection_prompt": "pizza . burger . fries . drink . napkin . menu .",
    "target_classes": ["pizza", "burger", "fries", "drink", "napkin", "menu"]
  },
  "dataset": {
    "class_names": ["pizza", "burger", "fries", "drink", "napkin", "menu"]
  }
}
```

#### Performance Optimization Settings

```json
{
  "video_processing": {
    "fps_extraction": 1.0,
    "max_frames_per_video": 500,
    "target_size": [416, 416]
  },
  "annotation": {
    "confidence_threshold": 0.3,
    "iou_threshold": 0.5
  },
  "quality_control": {
    "min_detection_size": 0.02,
    "max_detection_size": 0.9
  }
}
```

## üêõ Troubleshooting

### Common Issues and Solutions

#### 1. GroundingDINO Model Not Found
```bash
Error: groundingdino_swinb_cogcoor.pth not found
```
**Solution:**
```bash
wget https://github.com/IDEA-Research/GroundingDINO/releases/download/v0.1.0-alpha2/groundingdino_swinb_cogcoor.pth
```

#### 2. CUDA Out of Memory
```bash
Error: CUDA out of memory
```
**Solutions:**
- Reduce batch size: `--batch-size 8`
- Use smaller image size: `--imgsz 416`
- Use CPU: `--device cpu`

#### 3. No Video Files Found
```bash
Error: No supported video files found in data/raw/
```
**Solution:**
- Check file formats: `.mp4`, `.avi`, `.mov`, `.mkv`, `.wmv`
- Verify files are in `data/raw/` directory

#### 4. Empty Annotations
```bash
Warning: Many empty annotation files created
```
**Solutions:**
- Lower confidence threshold: `--confidence 0.1`
- Check video quality and lighting
- Verify detection prompt matches objects in videos

### üìä Performance Monitoring

Monitor training progress with:
- **TensorBoard**: `tensorboard --logdir outputs/experiments/`
- **Weights & Biases**: Add `--wandb` flag to training
- **Live plots**: Check `outputs/experiments/*/` for training curves

### üí° Tips for Better Results

1. **Video Quality**: Use well-lit, clear videos for better annotations
2. **Frame Rate**: Start with 1-2 FPS for initial experiments
3. **Confidence Tuning**: Lower thresholds (0.1-0.2) for more detections
4. **Class Balance**: Ensure diverse examples of all object types
5. **Validation**: Always check a sample of annotations manually

---

<div align="center">

**üåü If this project helped you, please give it a star! üåü**

[![GitHub stars](https://img.shields.io/github/stars/amir2628/restaurant-object-detection?style=social)](https://github.com/amir2628/restaurant-object-detection)
[![GitHub forks](https://img.shields.io/github/forks/amir2628/restaurant-object-detection?style=social)](https://github.com/amir2628/restaurant-object-detection)

</div>