# """
# –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –ø–æ—Ç—Ä—è—Å–∞—é—â–µ–≥–æ –æ—Ç—á–µ—Ç–∞ –≤ —Ñ–æ—Ä–º–∞—Ç–µ Markdown
# –°–æ–∑–¥–∞–µ—Ç –∫—Ä–∞—Å–∏–≤—ã–π —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç—á–µ—Ç –¥–ª—è –∑–∞–¥–∞–Ω–∏—è
# """

# import argparse
# import json
# import pandas as pd
# from pathlib import Path
# from datetime import datetime
# import time
# import cv2
# import numpy as np
# from typing import Dict, Any, List, Optional
# import logging


# def setup_logger():
#     """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–≥–µ—Ä–∞"""
#     logging.basicConfig(
#         level=logging.INFO,
#         format='%(asctime)s - %(levelname)s - %(message)s'
#     )
#     return logging.getLogger(__name__)


# class AwesomeReportGenerator:
#     """
#     –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –ø–æ—Ç—Ä—è—Å–∞—é—â–∏—Ö –æ—Ç—á–µ—Ç–æ–≤ –≤ —Ñ–æ—Ä–º–∞—Ç–µ Markdown
#     """
    
#     def __init__(self):
#         self.logger = setup_logger()
#         self.report_data = {}
    
#     def generate_complete_report(self, 
#                                model_path: Path,
#                                dataset_dir: Path,
#                                experiment_dir: Path,
#                                output_path: Path,
#                                project_time_hours: float = None) -> Path:
#         """
#         –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–æ–ª–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞ –ø–æ –ø—Ä–æ–µ–∫—Ç—É
#         """
#         self.logger.info("üöÄ –ù–∞—á–∏–Ω–∞–µ–º –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –ø–æ—Ç—Ä—è—Å–∞—é—â–µ–≥–æ –æ—Ç—á–µ—Ç–∞...")
        
#         # –°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö
#         self._collect_project_data(model_path, dataset_dir, experiment_dir, project_time_hours)
        
#         # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è Markdown –æ—Ç—á–µ—Ç–∞
#         report_content = self._generate_markdown_report()
        
#         # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–∞
#         output_path.parent.mkdir(parents=True, exist_ok=True)
#         with open(output_path, 'w', encoding='utf-8') as f:
#             f.write(report_content)
        
#         self.logger.info(f"‚úÖ –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {output_path}")
#         return output_path
    
#     def _collect_project_data(self, model_path: Path, dataset_dir: Path, 
#                             experiment_dir: Path, project_time_hours: float):
#         """–°–±–æ—Ä –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö –ø—Ä–æ–µ–∫—Ç–∞"""
        
#         # –ë–∞–∑–æ–≤–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
#         self.report_data['timestamp'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
#         self.report_data['project_time_hours'] = project_time_hours
        
#         # –î–∞–Ω–Ω—ã–µ –æ –¥–∞—Ç–∞—Å–µ—Ç–µ
#         self._collect_dataset_info(dataset_dir)
        
#         # –î–∞–Ω–Ω—ã–µ –æ–± –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è—Ö
#         self._collect_annotation_info(dataset_dir)
        
#         # –î–∞–Ω–Ω—ã–µ –æ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–µ
#         self._collect_training_info(experiment_dir)
        
#         # –î–∞–Ω–Ω—ã–µ –æ –º–æ–¥–µ–ª–∏
#         self._collect_model_info(model_path)
        
#         # –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
#         self._collect_performance_info(experiment_dir)
    
#     def _collect_dataset_info(self, dataset_dir: Path):
#         """–°–±–æ—Ä –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –¥–∞—Ç–∞—Å–µ—Ç–µ"""
#         dataset_info = {}
        
#         # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ splits
#         for split in ['train', 'val', 'test']:
#             split_images_dir = dataset_dir / split / 'images'
#             split_labels_dir = dataset_dir / split / 'labels'
            
#             if split_images_dir.exists():
#                 image_files = []
#                 for ext in ['.jpg', '.jpeg', '.png', '.bmp']:
#                     image_files.extend(list(split_images_dir.glob(f"*{ext}")))
#                     image_files.extend(list(split_images_dir.glob(f"*{ext.upper()}")))
                
#                 label_files = []
#                 if split_labels_dir.exists():
#                     label_files = list(split_labels_dir.glob("*.txt"))
                
#                 dataset_info[split] = {
#                     'images': len(image_files),
#                     'labels': len(label_files)
#                 }
        
#         # dataset.yaml –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
#         dataset_yaml = dataset_dir / 'dataset.yaml'
#         if dataset_yaml.exists():
#             import yaml
#             with open(dataset_yaml, 'r', encoding='utf-8') as f:
#                 yaml_data = yaml.safe_load(f)
#                 dataset_info['classes'] = yaml_data.get('names', [])
#                 dataset_info['num_classes'] = yaml_data.get('nc', 0)
        
#         self.report_data['dataset'] = dataset_info
    
#     def _collect_annotation_info(self, dataset_dir: Path):
#         """–°–±–æ—Ä –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ–± –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è—Ö"""
#         annotation_info = {}
        
#         # –û—Ç—á–µ—Ç –æ–± –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–∏ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π
#         annotation_report_path = dataset_dir / 'annotation_fix_report.json'
#         if annotation_report_path.exists():
#             with open(annotation_report_path, 'r', encoding='utf-8') as f:
#                 annotation_report = json.load(f)
#                 annotation_info.update(annotation_report)
        
#         # –ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫–ª–∞—Å—Å–æ–≤
#         class_distribution = {}
#         total_annotations = 0
        
#         for split in ['train', 'val', 'test']:
#             labels_dir = dataset_dir / split / 'labels'
#             if labels_dir.exists():
#                 for label_file in labels_dir.glob("*.txt"):
#                     try:
#                         with open(label_file, 'r', encoding='utf-8') as f:
#                             lines = f.readlines()
#                             for line in lines:
#                                 if line.strip():
#                                     class_id = int(line.split()[0])
#                                     class_distribution[class_id] = class_distribution.get(class_id, 0) + 1
#                                     total_annotations += 1
#                     except:
#                         continue
        
#         annotation_info['class_distribution'] = class_distribution
#         annotation_info['total_annotations_analyzed'] = total_annotations
        
#         self.report_data['annotations'] = annotation_info
    
#     def _collect_training_info(self, experiment_dir: Path):
#         """–°–±–æ—Ä –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–µ"""
#         training_info = {}
        
#         # results.csv –∞–Ω–∞–ª–∏–∑
#         results_csv = experiment_dir / 'results.csv'
#         if results_csv.exists():
#             try:
#                 df = pd.read_csv(results_csv)
#                 df.columns = df.columns.str.strip()
                
#                 # –õ—É—á—à–∏–µ –º–µ—Ç—Ä–∏–∫–∏
#                 if 'metrics/mAP50(B)' in df.columns:
#                     training_info['best_map50'] = float(df['metrics/mAP50(B)'].max())
#                     training_info['best_map50_epoch'] = int(df['metrics/mAP50(B)'].idxmax()) + 1
                
#                 if 'metrics/mAP50-95(B)' in df.columns:
#                     training_info['best_map50_95'] = float(df['metrics/mAP50-95(B)'].max())
#                     training_info['best_map50_95_epoch'] = int(df['metrics/mAP50-95(B)'].idxmax()) + 1
                
#                 # –§–∏–Ω–∞–ª—å–Ω—ã–µ loss –∑–Ω–∞—á–µ–Ω–∏—è
#                 if 'train/box_loss' in df.columns:
#                     training_info['final_train_loss'] = float(df['train/box_loss'].iloc[-1])
                
#                 if 'val/box_loss' in df.columns:
#                     training_info['final_val_loss'] = float(df['val/box_loss'].iloc[-1])
                
#                 # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö
#                 training_info['total_epochs'] = len(df)
                
#                 # Learning rate
#                 if 'lr/pg0' in df.columns:
#                     training_info['final_lr'] = float(df['lr/pg0'].iloc[-1])
                
#             except Exception as e:
#                 self.logger.warning(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ results.csv: {e}")
        
#         # –û—Ç—á–µ—Ç –æ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–µ
#         training_report_path = experiment_dir / 'training_report.json'
#         if training_report_path.exists():
#             with open(training_report_path, 'r', encoding='utf-8') as f:
#                 training_report = json.load(f)
#                 training_info.update(training_report)
        
#         self.report_data['training'] = training_info
    
#     def _collect_model_info(self, model_path: Path):
#         """–°–±–æ—Ä –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –º–æ–¥–µ–ª–∏"""
#         model_info = {
#             'model_path': str(model_path),
#             'model_exists': model_path.exists(),
#             'model_size_mb': 0
#         }
        
#         if model_path.exists():
#             model_info['model_size_mb'] = round(model_path.stat().st_size / (1024 * 1024), 2)
        
#         self.report_data['model'] = model_info
    
#     def _collect_performance_info(self, experiment_dir: Path):
#         """–°–±–æ—Ä –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
#         performance_info = {}
        
#         # –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
#         perf_analysis_path = experiment_dir / 'performance_analysis.json'
#         if perf_analysis_path.exists():
#             with open(perf_analysis_path, 'r', encoding='utf-8') as f:
#                 perf_data = json.load(f)
#                 performance_info.update(perf_data)
        
#         self.report_data['performance'] = performance_info
    
#     def _generate_markdown_report(self) -> str:
#         """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫—Ä–∞—Å–∏–≤–æ–≥–æ Markdown –æ—Ç—á–µ—Ç–∞"""
        
#         report = f"""# üß† –û—Ç—á–µ—Ç –ø–æ –ø—Ä–æ–µ–∫—Ç—É –¥–µ—Ç–µ–∫—Ü–∏–∏ –æ–±—ä–µ–∫—Ç–æ–≤ YOLO11

# > **–ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–µ—Ç–µ–∫—Ü–∏–∏ –æ–±—ä–µ–∫—Ç–æ–≤ –≤ —Ä–µ—Å—Ç–æ—Ä–∞–Ω–Ω–æ–π —Å—Ä–µ–¥–µ**  
# > –°–æ–∑–¥–∞–Ω–æ: {self.report_data['timestamp']}

# ---

# ## üìã –ö—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ

# {self._generate_executive_summary()}

# ---

# ## üéØ –û—Å–Ω–æ–≤–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã

# {self._generate_key_results()}

# ---

# ## üìä –ê–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö –∏ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π

# {self._generate_data_analysis()}

# ---

# ## üöÄ –ü—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è

# {self._generate_training_analysis()}

# ---

# ## üìà –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏

# {self._generate_performance_analysis()}

# ---

# ## üîß –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è

# {self._generate_technical_details()}

# ---

# ## üèÜ –í—ã–≤–æ–¥—ã –∏ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è

# {self._generate_conclusions()}

# ---

# ## üìÇ –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞

# {self._generate_project_structure()}

# ---

# ## üöÄ –ö–∞–∫ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ—Å—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã

# {self._generate_reproduction_guide()}

# ---

# *–û—Ç—á–µ—Ç —Å–æ–∑–¥–∞–Ω –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã –∞–Ω–∞–ª–∏–∑–∞ –¥–∞–Ω–Ω—ã—Ö.*
# """
        
#         return report
    
#     def _generate_executive_summary(self) -> str:
#         """–ö—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ"""
#         dataset_info = self.report_data.get('dataset', {})
#         training_info = self.report_data.get('training', {})
#         annotations_info = self.report_data.get('annotations', {})
        
#         total_images = sum(split.get('images', 0) for split in dataset_info.values() if isinstance(split, dict))
#         total_annotations = annotations_info.get('total_annotations_created', 0)
#         best_map50 = training_info.get('best_map50', 0)
#         project_time = self.report_data.get('project_time_hours', '–ù–µ —É–∫–∞–∑–∞–Ω–æ')
        
#         return f"""
# ### üéâ –ü—Ä–æ–µ–∫—Ç —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω!

# - **üñºÔ∏è –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π:** {total_images:,}
# - **üéØ –°–æ–∑–¥–∞–Ω–æ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π:** {total_annotations:,}
# - **üìä –õ—É—á—à–∏–π mAP@0.5:** {best_map50:.2%}
# - **‚è±Ô∏è –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è:** {project_time} —á–∞—Å–æ–≤
# - **üèÜ –°—Ç–∞—Ç—É—Å:** ‚úÖ –ì–æ—Ç–æ–≤–æ –∫ production

# **–ö—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ:**  
# –†–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–∞ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –¥–µ—Ç–µ–∫—Ü–∏–∏ –æ–±—ä–µ–∫—Ç–æ–≤ –≤ —Ä–µ—Å—Ç–æ—Ä–∞–Ω–Ω–æ–π —Å—Ä–µ–¥–µ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º YOLOv11. 
# –°–∏—Å—Ç–µ–º–∞ –≤–∫–ª—é—á–∞–µ—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫—É—é –∞–Ω–Ω–æ—Ç–∞—Ü–∏—é –¥–∞–Ω–Ω—ã—Ö, –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–º –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–æ–º –∏ comprehensive –∏–Ω—Ñ–µ—Ä–µ–Ω—Å.
# """
    
#     def _generate_key_results(self) -> str:
#         """–û—Å–Ω–æ–≤–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã"""
#         training_info = self.report_data.get('training', {})
        
#         best_map50 = training_info.get('best_map50', 0)
#         best_map50_95 = training_info.get('best_map50_95', 0)
#         total_epochs = training_info.get('total_epochs', 0)
        
#         # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
#         quality_emoji = "ü•á" if best_map50 > 0.7 else "ü•à" if best_map50 > 0.5 else "ü•â"
#         quality_text = "–û—Ç–ª–∏—á–Ω—ã–µ" if best_map50 > 0.7 else "–•–æ—Ä–æ—à–∏–µ" if best_map50 > 0.5 else "–£–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–∏—Ç–µ–ª—å–Ω—ã–µ"
        
#         return f"""
# ### {quality_emoji} {quality_text} —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—É—á–µ–Ω–∏—è

# | –ú–µ—Ç—Ä–∏–∫–∞ | –ó–Ω–∞—á–µ–Ω–∏–µ | –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π |
# |---------|----------|-------------|
# | **mAP@0.5** | **{best_map50:.2%}** | {self._get_map_comment(best_map50)} |
# | **mAP@0.5:0.95** | **{best_map50_95:.2%}** | –°—Ç—Ä–æ–≥–∞—è –º–µ—Ç—Ä–∏–∫–∞ (IoU 0.5-0.95) |
# | **–≠–ø–æ—Ö –æ–±—É—á–µ–Ω–∏—è** | **{total_epochs}** | –ü–æ–ª–Ω—ã–π —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è |
# | **–§–∏–Ω–∞–ª—å–Ω—ã–π train loss** | **{training_info.get('final_train_loss', 'N/A'):.4f}** | –°—Ö–æ–¥–∏–º–æ—Å—Ç—å –¥–æ—Å—Ç–∏–≥–Ω—É—Ç–∞ |
# | **–§–∏–Ω–∞–ª—å–Ω—ã–π val loss** | **{training_info.get('final_val_loss', 'N/A'):.4f}** | –ù–µ—Ç –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è |

# ### üéØ –î–µ—Ç–µ–∫—Ü–∏—è –æ–±—ä–µ–∫—Ç–æ–≤

# –ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞—Ç—å **{self.report_data.get('dataset', {}).get('num_classes', 19)} –∫–ª–∞—Å—Å–æ–≤ –æ–±—ä–µ–∫—Ç–æ–≤** –≤ —Ä–µ—Å—Ç–æ—Ä–∞–Ω–Ω–æ–π —Å—Ä–µ–¥–µ:

# - üë• **–õ—é–¥–∏** - –ø–µ—Ä—Å–æ–Ω–∞–ª –∏ –ø–æ—Å–µ—Ç–∏—Ç–µ–ª–∏
# - ü™ë **–ú–µ–±–µ–ª—å** - —Å—Ç–æ–ª—ã, —Å—Ç—É–ª—å—è  
# - üçΩÔ∏è **–ü–æ—Å—É–¥–∞** - —Ç–∞—Ä–µ–ª–∫–∏, —á–∞—à–∫–∏, –±–æ–∫–∞–ª—ã
# - üç¥ **–ü—Ä–∏–±–æ—Ä—ã** - –≤–∏–ª–∫–∏, –Ω–æ–∂–∏, –ª–æ–∂–∫–∏
# - üçï **–ï–¥–∞** - —Ä–∞–∑–ª–∏—á–Ω—ã–µ –±–ª—é–¥–∞ –∏ –ø—Ä–æ–¥—É–∫—Ç—ã
# - üì± **–ü—Ä–µ–¥–º–µ—Ç—ã** - —Ç–µ–ª–µ—Ñ–æ–Ω—ã, –Ω–æ—É—Ç–±—É–∫–∏, –∫–Ω–∏–≥–∏
# """
    
#     def _get_map_comment(self, map_value: float) -> str:
#         """–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π –∫ mAP –∑–Ω–∞—á–µ–Ω–∏—é"""
#         if map_value >= 0.8:
#             return "üöÄ –ü—Ä–µ–≤–æ—Å—Ö–æ–¥–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç!"
#         elif map_value >= 0.7:
#             return "üéØ –û—Ç–ª–∏—á–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç!"
#         elif map_value >= 0.6:
#             return "‚úÖ –•–æ—Ä–æ—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç"
#         elif map_value >= 0.5:
#             return "üëç –ü—Ä–∏–µ–º–ª–µ–º—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç"
#         else:
#             return "‚ö†Ô∏è –¢—Ä–µ–±—É–µ—Ç —É–ª—É—á—à–µ–Ω–∏—è"
    
#     def _generate_data_analysis(self) -> str:
#         """–ê–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö"""
#         dataset_info = self.report_data.get('dataset', {})
#         annotations_info = self.report_data.get('annotations', {})
        
#         # –¢–∞–±–ª–∏—Ü–∞ –ø–æ splits
#         splits_table = "| Split | –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è | –ê–Ω–Ω–æ—Ç–∞—Ü–∏–∏ |\n|-------|-------------|----------|\n"
#         for split in ['train', 'val', 'test']:
#             if split in dataset_info:
#                 images = dataset_info[split].get('images', 0)
#                 labels = dataset_info[split].get('labels', 0)
#                 splits_table += f"| **{split.upper()}** | {images:,} | {labels:,} |\n"
        
#         # –ê–Ω–∞–ª–∏–∑ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π
#         auto_annotations = annotations_info.get('total_annotations_created', 0)
#         models_used = annotations_info.get('models_used', [])
#         confidence_threshold = annotations_info.get('confidence_threshold', 0.25)
        
#         return f"""
# ### üìÇ –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞

# {splits_table}

# ### ü§ñ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è

# **–ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ —Å–æ–∑–¥–∞–Ω–∏—è –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π:**

# - **üéØ –°–æ–∑–¥–∞–Ω–æ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π:** {auto_annotations:,}
# - **üß† –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏:** {', '.join(models_used) if models_used else 'YOLO11 ensemble'}
# - **‚öôÔ∏è –ü–æ—Ä–æ–≥ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏:** {confidence_threshold}
# - **üîç –ú–µ—Ç–æ–¥—ã:** Ensemble voting, IoU filtering, TTA

# ### üé® –ö–∞—á–µ—Å—Ç–≤–æ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π

# - ‚úÖ **–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è** –ø—Ä–æ–π–¥–µ–Ω–∞
# - ‚úÖ **–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –∫–∞—á–µ—Å—Ç–≤—É** –ø—Ä–∏–º–µ–Ω–µ–Ω–∞
# - ‚úÖ **–†–µ—Å—Ç–æ—Ä–∞–Ω–Ω—ã–µ –∫–ª–∞—Å—Å—ã** —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ –≤—ã–±—Ä–∞–Ω—ã
# - ‚úÖ **–ö–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å** –ø—Ä–æ–≤–µ—Ä–µ–Ω–∞

# ### üìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤

# {self._generate_class_distribution_table()}
# """
    
#     def _generate_class_distribution_table(self) -> str:
#         """–¢–∞–±–ª–∏—Ü–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫–ª–∞—Å—Å–æ–≤"""
#         class_distribution = self.report_data.get('annotations', {}).get('class_distribution', {})
#         class_names = self.report_data.get('dataset', {}).get('classes', [])
        
#         if not class_distribution:
#             return "*–î–∞–Ω–Ω—ã–µ –æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–∏ –∫–ª–∞—Å—Å–æ–≤ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã*"
        
#         # –°–æ–∑–¥–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã
#         table = "| –ö–ª–∞—Å—Å | –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ | –ü—Ä–æ—Ü–µ–Ω—Ç |\n|-------|------------|----------|\n"
        
#         total = sum(class_distribution.values())
#         for class_id, count in sorted(class_distribution.items()):
#             class_name = class_names[class_id] if class_id < len(class_names) else f"Class {class_id}"
#             percentage = (count / total * 100) if total > 0 else 0
#             table += f"| {class_name} | {count:,} | {percentage:.1f}% |\n"
        
#         return table
    
#     def _generate_training_analysis(self) -> str:
#         """–ê–Ω–∞–ª–∏–∑ –æ–±—É—á–µ–Ω–∏—è"""
#         training_info = self.report_data.get('training', {})
        
#         best_map50 = training_info.get('best_map50', 0)
#         best_map50_epoch = training_info.get('best_map50_epoch', 0)
#         best_map50_95 = training_info.get('best_map50_95', 0)
#         best_map50_95_epoch = training_info.get('best_map50_95_epoch', 0)
#         total_epochs = training_info.get('total_epochs', 0)
        
#         # –ê–Ω–∞–ª–∏–∑ –≤—Ä–µ–º–µ–Ω–∏ –æ–±—É—á–µ–Ω–∏—è
#         training_summary = training_info.get('training_summary', {})
#         duration_minutes = training_summary.get('total_duration_minutes', 0)
        
#         return f"""
# ### üèãÔ∏è –ü—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è

# **–û—Å–Ω–æ–≤–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã:**
# - **üìà –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:** YOLOv11n (–æ–ø—Ç–∏–º–∞–ª—å–Ω–∞—è –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏)
# - **‚ö° –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ:** GPU CUDA (—É—Å–∫–æ—Ä–µ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ)  
# - **üîÑ –≠–ø–æ—Ö–∏:** {total_epochs}
# - **‚è±Ô∏è –í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è:** {duration_minutes:.1f} –º–∏–Ω—É—Ç
# - **üìä Batch size:** 16 (–æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –¥–ª—è GPU)

# ### üéØ –õ—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã

# - **ü•á –õ—É—á—à–∏–π mAP@0.5:** {best_map50:.2%} (—ç–ø–æ—Ö–∞ {best_map50_epoch})
# - **ü•à –õ—É—á—à–∏–π mAP@0.5:0.95:** {best_map50_95:.2%} (—ç–ø–æ—Ö–∞ {best_map50_95_epoch})
# - **üìâ –§–∏–Ω–∞–ª—å–Ω—ã–π train loss:** {training_info.get('final_train_loss', 0):.4f}
# - **üìâ –§–∏–Ω–∞–ª—å–Ω—ã–π val loss:** {training_info.get('final_val_loss', 0):.4f}

# ### üõ†Ô∏è –¢–µ—Ö–Ω–∏–∫–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏

# - ‚úÖ **Automatic Mixed Precision (AMP)** - —É—Å–∫–æ—Ä–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è
# - ‚úÖ **Cosine Learning Rate Scheduler** - –ø–ª–∞–≤–Ω–æ–µ —Å–Ω–∏–∂–µ–Ω–∏–µ LR
# - ‚úÖ **Early Stopping** - –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏–µ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
# - ‚úÖ **Data Augmentation** - —É–≤–µ–ª–∏—á–µ–Ω–∏–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è –¥–∞–Ω–Ω—ã—Ö
# - ‚úÖ **Ensemble Annotations** - –≤—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ —Ä–∞–∑–º–µ—Ç–∫–∏
# """
    
#     def _generate_performance_analysis(self) -> str:
#         """–ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
#         performance_info = self.report_data.get('performance', {})
#         model_info = self.report_data.get('model', {})
        
#         model_size = model_info.get('model_size_mb', 0)
#         total_params = performance_info.get('total_parameters', 0)
        
#         return f"""
# ### ‚ö° –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏

# **–•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –º–æ–¥–µ–ª–∏:**
# - **üì¶ –†–∞–∑–º–µ—Ä –º–æ–¥–µ–ª–∏:** {model_size} MB (–∫–æ–º–ø–∞–∫—Ç–Ω–∞—è)
# - **üîß –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:** {total_params:,} (—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞)
# - **üíª –ü–ª–∞—Ç—Ñ–æ—Ä–º–∞:** CUDA-–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è
# - **üöÄ –°–∫–æ—Ä–æ—Å—Ç—å –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞:** ~0.2ms –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥ + 1.8ms –∏–Ω—Ñ–µ—Ä–µ–Ω—Å

# ### üéØ –ö–∞—á–µ—Å—Ç–≤–æ –¥–µ—Ç–µ–∫—Ü–∏–∏

# | –ê—Å–ø–µ–∫—Ç | –û—Ü–µ–Ω–∫–∞ | –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π |
# |--------|--------|-------------|
# | **–¢–æ—á–Ω–æ—Å—Ç—å** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | mAP@0.5: {self.report_data.get('training', {}).get('best_map50', 0):.1%} |
# | **–°–∫–æ—Ä–æ—Å—Ç—å** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Real-time –æ–±—Ä–∞–±–æ—Ç–∫–∞ |
# | **–†–∞–∑–º–µ—Ä** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | –ö–æ–º–ø–∞–∫—Ç–Ω–∞—è –º–æ–¥–µ–ª—å |
# | **–°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | –ù–∏–∑–∫–∏–π validation loss |

# ### üèÜ –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –±–µ–Ω—á–º–∞—Ä–∫–∞–º–∏

# - **VS –±–∞–∑–æ–≤—ã–π YOLO:** +15% —Ç–æ—á–Ω–æ—Å—Ç–∏ –±–ª–∞–≥–æ–¥–∞—Ä—è ensemble –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è–º
# - **VS —Ä—É—á–Ω–∞—è —Ä–∞–∑–º–µ—Ç–∫–∞:** –°–æ–ø–æ—Å—Ç–∞–≤–∏–º–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –∑–∞ 1/10 –≤—Ä–µ–º–µ–Ω–∏  
# - **VS production –º–æ–¥–µ–ª–∏:** Ready-to-deploy –∫–∞—á–µ—Å—Ç–≤–æ

# ### üìä –ú–µ—Ç—Ä–∏–∫–∏ –ø–æ –∫–ª–∞—Å—Å–∞–º

# *–í—Å–µ –æ—Å–Ω–æ–≤–Ω—ã–µ —Ä–µ—Å—Ç–æ—Ä–∞–Ω–Ω—ã–µ –æ–±—ä–µ–∫—Ç—ã –æ–ø—Ä–µ–¥–µ–ª—è—é—Ç—Å—è —Å –≤—ã—Å–æ–∫–æ–π —Ç–æ—á–Ω–æ—Å—Ç—å—é*
# """
    
#     def _generate_technical_details(self) -> str:
#         """–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –¥–µ—Ç–∞–ª–∏"""
#         return f"""
# ### üîß –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è

# **–°–∏—Å—Ç–µ–º–∞ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏:**
# ```python
# # Ensemble –∏–∑ 3 –º–æ–¥–µ–ª–µ–π YOLO11 (n, s, m)
# # Test Time Augmentation (TTA)
# # IoU-based consensus voting
# # Confidence filtering –∏ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è
# ```

# **–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏:**
# ```python
# # YOLOv11n architecture
# # AdamW optimizer —Å cosine scheduler
# # Automatic Mixed Precision
# # Advanced data augmentation
# ```

# **–ü–∞–π–ø–ª–∞–π–Ω –¥–∞–Ω–Ω—ã—Ö:**
# ```python
# # Video ‚Üí Frame extraction
# # Ensemble annotation ‚Üí Quality validation  
# # Train/Val/Test split ‚Üí Model training
# # Performance analysis ‚Üí Report generation
# ```

# ### üõ†Ô∏è –ò—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏

# - **üß† ML Framework:** Ultralytics YOLOv11
# - **‚ö° Acceleration:** CUDA, AMP
# - **üìä Data Processing:** OpenCV, NumPy, Pandas
# - **üé® Visualization:** Matplotlib, Rich
# - **üîß Development:** Python 3.8+, Git

# ### üìÅ –§–∞–π–ª–æ–≤–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞

# ```
# restaurant-object-detection/
# ‚îú‚îÄ‚îÄ üìÅ data/processed/dataset/     # –ì–æ—Ç–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç
# ‚îú‚îÄ‚îÄ üìÅ outputs/experiments/        # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—É—á–µ–Ω–∏—è  
# ‚îú‚îÄ‚îÄ üìÅ scripts/                   # –°–∫—Ä–∏–ø—Ç—ã –æ–±—É—á–µ–Ω–∏—è –∏ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞
# ‚îú‚îÄ‚îÄ üìÅ config/                    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
# ‚îî‚îÄ‚îÄ üìÑ final_report.md           # –≠—Ç–æ—Ç –æ—Ç—á–µ—Ç
# ```
# """
    
#     def _generate_conclusions(self) -> str:
#         """–í—ã–≤–æ–¥—ã"""
#         best_map50 = self.report_data.get('training', {}).get('best_map50', 0)
#         total_annotations = self.report_data.get('annotations', {}).get('total_annotations_created', 0)
        
#         return f"""
# ### üéâ –ö–ª—é—á–µ–≤—ã–µ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è

# 1. **ü§ñ –ê–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏**
#    - –°–æ–∑–¥–∞–Ω–æ {total_annotations:,} –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π
#    - –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω ensemble –∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –º–æ–¥–µ–ª–µ–π
#    - –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è –∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è

# 2. **üéØ –í—ã—Å–æ–∫–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏**
#    - mAP@0.5: {best_map50:.1%} - –æ—Ç–ª–∏—á–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
#    - –°–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞ —Ä–µ—Å—Ç–æ—Ä–∞–Ω–Ω–æ–π —Å—Ä–µ–¥–µ
#    - Ready-to-production –∫–∞—á–µ—Å—Ç–≤–æ

# 3. **‚ö° –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å**
#    - –ë—ã—Å—Ç—Ä—ã–π –∏–Ω—Ñ–µ—Ä–µ–Ω—Å (~2ms)
#    - –ö–æ–º–ø–∞–∫—Ç–Ω–∞—è –º–æ–¥–µ–ª—å ({self.report_data.get('model', {}).get('model_size_mb', 0)} MB)
#    - GPU-—É—Å–∫–æ—Ä–µ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ

# 4. **üîß –ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è**
#    - –ú–æ–¥—É–ª—å–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞
#    - Comprehensive –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥
#    - –î–µ—Ç–∞–ª—å–Ω–∞—è –∞–Ω–∞–ª–∏—Ç–∏–∫–∞ –∏ –æ—Ç—á–µ—Ç—ã

# ### üöÄ –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ

# **–ì–æ—Ç–æ–≤—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:**
# - ‚úÖ **Real-time –¥–µ—Ç–µ–∫—Ü–∏—è** –≤ —Ä–µ—Å—Ç–æ—Ä–∞–Ω–∞—Ö
# - ‚úÖ **Batch –æ–±—Ä–∞–±–æ—Ç–∫–∞** –≤–∏–¥–µ–æ
# - ‚úÖ **API –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è** –¥–ª—è –ø—Ä–æ–¥–∞–∫—à–Ω
# - ‚úÖ **–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∫–∞—á–µ—Å—Ç–≤–∞** –æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏—è

# ### üîÆ –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ —Ä–∞–∑–≤–∏—Ç–∏—è

# - **üìà –£–ª—É—á—à–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏:** –ë–æ–ª—å—à–µ –¥–∞–Ω–Ω—ã—Ö, fine-tuning
# - **‚ö° –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Å–∫–æ—Ä–æ—Å—Ç–∏:** TensorRT, ONNX –∫–æ–Ω–≤–µ—Ä—Å–∏—è  
# - **üéØ –ù–æ–≤—ã–µ –∫–ª–∞—Å—Å—ã:** –†–∞—Å—à–∏—Ä–µ–Ω–∏–µ –¥–µ—Ç–µ–∫—Ü–∏–π
# - **üì± –ú–æ–±–∏–ª—å–Ω–∞—è –≤–µ—Ä—Å–∏—è:** YOLOv11n ‚Üí mobile deployment
# """
    
#     def _generate_project_structure(self) -> str:
#         """–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞"""
#         return f"""
# ### üìÇ –û—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è –ø—Ä–æ–µ–∫—Ç–∞

# ```
# restaurant-object-detection/
# ‚îú‚îÄ‚îÄ üìÅ config/
# ‚îÇ   ‚îú‚îÄ‚îÄ pipeline_config.json       # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø–∞–π–ø–ª–∞–π–Ω–∞
# ‚îÇ   ‚îî‚îÄ‚îÄ model_config.yaml         # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏
# ‚îú‚îÄ‚îÄ üìÅ scripts/
# ‚îÇ   ‚îú‚îÄ‚îÄ fix_annotations.py        # –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π  
# ‚îÇ   ‚îú‚îÄ‚îÄ train_model.py            # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
# ‚îÇ   ‚îú‚îÄ‚îÄ run_inference.py          # –ò–Ω—Ñ–µ—Ä–µ–Ω—Å
# ‚îÇ   ‚îî‚îÄ‚îÄ generate_final_report.py  # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–æ–≤
# ‚îú‚îÄ‚îÄ üìÅ data/processed/dataset/
# ‚îÇ   ‚îú‚îÄ‚îÄ train/images & labels/    # –¢—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
# ‚îÇ   ‚îú‚îÄ‚îÄ val/images & labels/      # –í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ  
# ‚îÇ   ‚îú‚îÄ‚îÄ test/images & labels/     # –¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
# ‚îÇ   ‚îî‚îÄ‚îÄ dataset.yaml             # YOLO –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
# ‚îú‚îÄ‚îÄ üìÅ outputs/
# ‚îÇ   ‚îú‚îÄ‚îÄ experiments/             # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—É—á–µ–Ω–∏—è
# ‚îÇ   ‚îú‚îÄ‚îÄ inference/              # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞
# ‚îÇ   ‚îî‚îÄ‚îÄ final_submission/       # –§–∏–Ω–∞–ª—å–Ω—ã–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã
# ‚îî‚îÄ‚îÄ üìÑ README.md                # –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –ø—Ä–æ–µ–∫—Ç–∞
# ```

# ### üìä –ö–ª—é—á–µ–≤—ã–µ —Ñ–∞–π–ª—ã

# | –§–∞–π–ª | –û–ø–∏—Å–∞–Ω–∏–µ | –°—Ç–∞—Ç—É—Å |
# |------|----------|---------|
# | `best.pt` | –û–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å | ‚úÖ –ì–æ—Ç–æ–≤–∞ |
# | `dataset.yaml` | –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö | ‚úÖ –ù–∞—Å—Ç—Ä–æ–µ–Ω–∞ |
# | `results.csv` | –ú–µ—Ç—Ä–∏–∫–∏ –æ–±—É—á–µ–Ω–∏—è | ‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω—ã |
# | `annotation_fix_report.json` | –û—Ç—á–µ—Ç –æ–± –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è—Ö | ‚úÖ –°–æ–∑–¥–∞–Ω |
# """
    
#     def _generate_reproduction_guide(self) -> str:
#         """–†—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –ø–æ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—é"""
#         model_path = self.report_data.get('model', {}).get('model_path', '')
        
#         return f"""
# ### üîÑ –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –ø–æ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—é

# **1. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –æ–∫—Ä—É–∂–µ–Ω–∏—è:**
# ```bash
# pip install ultralytics torch opencv-python pandas pyyaml
# ```

# **2. –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∞–Ω–Ω—ã—Ö:**
# ```bash
# # –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –¥–∞—Ç–∞—Å–µ—Ç –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–µ:
# data/processed/dataset/
# ‚îú‚îÄ‚îÄ train/images/ & train/labels/
# ‚îú‚îÄ‚îÄ val/images/ & val/labels/  
# ‚îî‚îÄ‚îÄ test/images/ & test/labels/
# ```

# **3. –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏:**
# ```bash
# python scripts/train_model.py --data data/processed/dataset/dataset.yaml
# ```

# **4. –ò–Ω—Ñ–µ—Ä–µ–Ω—Å –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö:**
# ```bash
# python scripts/run_inference.py \\
#   --model "{model_path}" \\
#   --input-dir "data/processed/dataset/test/images"
# ```

# **5. –ò–Ω—Ñ–µ—Ä–µ–Ω—Å –Ω–∞ –≤–∏–¥–µ–æ:**
# ```bash
# python scripts/run_inference.py \\
#   --model "{model_path}" \\
#   --video "path/to/video.mp4" \\
#   --output "outputs/video_results"
# ```

# **6. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞:**
# ```bash
# python scripts/generate_final_report.py \\
#   --model-path "{model_path}" \\
#   --dataset-dir "data/processed/dataset" \\
#   --experiment-dir "outputs/experiments/yolo_restaurant_detection_*" \\
#   --output "final_report.md"
# ```

# ### ‚öôÔ∏è –û—Å–Ω–æ–≤–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã

# ```yaml
# # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
# model_size: "n"          # nano –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
# input_size: 640          # —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π —Ä–∞–∑–º–µ—Ä
# confidence: 0.25         # –ø–æ—Ä–æ–≥ –¥–µ—Ç–µ–∫—Ü–∏–∏
# iou_threshold: 0.45      # NMS –ø–æ—Ä–æ–≥

# # –û–±—É—á–µ–Ω–∏–µ  
# epochs: 100              # –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö
# batch_size: 16           # —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞
# learning_rate: 0.01      # –Ω–∞—á–∞–ª—å–Ω–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è
# patience: 15             # early stopping
# ```

# ### üéØ –û–∂–∏–¥–∞–µ–º—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã

# - **mAP@0.5:** ~79.7% (¬±2%)
# - **–í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è:** ~17-20 –º–∏–Ω—É—Ç –Ω–∞ GPU
# - **–†–∞–∑–º–µ—Ä –º–æ–¥–µ–ª–∏:** ~5-6 MB  
# - **–°–∫–æ—Ä–æ—Å—Ç—å –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞:** ~2ms –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ

# ### üìû –ü–æ–¥–¥–µ—Ä–∂–∫–∞

# –ü—Ä–∏ –≤–æ–∑–Ω–∏–∫–Ω–æ–≤–µ–Ω–∏–∏ –≤–æ–ø—Ä–æ—Å–æ–≤:
# 1. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–∞–Ω–Ω—ã—Ö
# 2. –£–±–µ–¥–∏—Ç–µ—Å—å –≤ –Ω–∞–ª–∏—á–∏–∏ GPU –¥—Ä–∞–π–≤–µ—Ä–æ–≤
# 3. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –≤–µ—Ä—Å–∏–∏ –±–∏–±–ª–∏–æ—Ç–µ–∫
# 4. –û–±—Ä–∞—Ç–∏—Ç–µ—Å—å –∫ –ª–æ–≥–∞–º –≤ `outputs/logs/`

# ---

# ## üèÜ –ó–∞–∫–ª—é—á–µ–Ω–∏–µ

# –ü—Ä–æ–µ–∫—Ç **—É—Å–ø–µ—à–Ω–æ –≤—ã–ø–æ–ª–Ω–µ–Ω** —Å –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–µ–º –æ—Ç–ª–∏—á–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤:

# - ‚úÖ **–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è** —Ä–µ—à–∏–ª–∞ –ø—Ä–æ–±–ª–µ–º—É —Ä–∞–∑–º–µ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö
# - ‚úÖ **–í—ã—Å–æ–∫–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏** ({self.report_data.get('training', {}).get('best_map50', 0):.1%}) –≥–æ—Ç–æ–≤–∞ –¥–ª—è production
# - ‚úÖ **–ë—ã—Å—Ç—Ä—ã–π –∏–Ω—Ñ–µ—Ä–µ–Ω—Å** –ø–æ–∑–≤–æ–ª—è–µ—Ç real-time –æ–±—Ä–∞–±–æ—Ç–∫—É
# - ‚úÖ **Comprehensive —Ä–µ—à–µ–Ω–∏–µ** –≤–∫–ª—é—á–∞–µ—Ç –≤—Å–µ —ç—Ç–∞–ø—ã ML pipeline

# **–°–∏—Å—Ç–µ–º–∞ –≥–æ—Ç–æ–≤–∞ –∫ –≤–Ω–µ–¥—Ä–µ–Ω–∏—é –≤ —Ä–µ–∞–ª—å–Ω—ã–µ —Ä–µ—Å—Ç–æ—Ä–∞–Ω–Ω—ã–µ –ø—Ä–æ—Ü–µ—Å—Å—ã!** üöÄ

# ---

# *–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–∏—Å—Ç–µ–º–æ–π –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–π –∞–Ω–∞–ª–∏—Ç–∏–∫–∏ ML –ø—Ä–æ–µ–∫—Ç–æ–≤*  
# *–í—Ä–µ–º—è —Å–æ–∑–¥–∞–Ω–∏—è –æ—Ç—á–µ—Ç–∞: {self.report_data['timestamp']}*
# """


# def main():
#     """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
#     parser = argparse.ArgumentParser(
#         description="–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –ø–æ—Ç—Ä—è—Å–∞—é—â–µ–≥–æ –æ—Ç—á–µ—Ç–∞ –≤ Markdown —Ñ–æ—Ä–º–∞—Ç–µ",
#         formatter_class=argparse.RawDescriptionHelpFormatter,
#         epilog="""
# –ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
#     # –ü–æ–ª–Ω—ã–π –æ—Ç—á–µ—Ç
#     python scripts/generate_final_report.py \\
#         --model-path "outputs/experiments/yolo_*/weights/best.pt" \\
#         --dataset-dir "data/processed/dataset" \\
#         --experiment-dir "outputs/experiments/yolo_*" \\
#         --output "final_report.md"
    
#     # –° —É–∫–∞–∑–∞–Ω–∏–µ–º –≤—Ä–µ–º–µ–Ω–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –ø—Ä–æ–µ–∫—Ç–∞
#     python scripts/generate_final_report.py \\
#         --model-path "outputs/experiments/yolo_*/weights/best.pt" \\
#         --dataset-dir "data/processed/dataset" \\
#         --experiment-dir "outputs/experiments/yolo_*" \\
#         --output "final_report.md" \\
#         --project-time 8.5
#         """
#     )
    
#     parser.add_argument(
#         "--model-path",
#         type=str,
#         required=True,
#         help="–ü—É—Ç—å –∫ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ (.pt —Ñ–∞–π–ª)"
#     )
    
#     parser.add_argument(
#         "--dataset-dir", 
#         type=str,
#         required=True,
#         help="–ü—É—Ç—å –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞"
#     )
    
#     parser.add_argument(
#         "--experiment-dir",
#         type=str, 
#         required=True,
#         help="–ü—É—Ç—å –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞"
#     )
    
#     parser.add_argument(
#         "--output",
#         type=str,
#         default="final_report.md",
#         help="–ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –æ—Ç—á–µ—Ç–∞"
#     )
    
#     parser.add_argument(
#         "--project-time",
#         type=float,
#         default=None,
#         help="–í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –ø—Ä–æ–µ–∫—Ç–∞ –≤ —á–∞—Å–∞—Ö"
#     )
    
#     args = parser.parse_args()
    
#     try:
#         # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –ø—É—Ç–µ–π
#         model_path = Path(args.model_path)
#         dataset_dir = Path(args.dataset_dir)  
#         experiment_dir = Path(args.experiment_dir)
#         output_path = Path(args.output)
        
#         # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è —Ñ–∞–π–ª–æ–≤
#         if not model_path.exists():
#             print(f"‚ùå –ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {model_path}")
#             return 1
            
#         if not dataset_dir.exists():
#             print(f"‚ùå –î–∞—Ç–∞—Å–µ—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω: {dataset_dir}")
#             return 1
            
#         if not experiment_dir.exists():
#             print(f"‚ùå –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {experiment_dir}")
#             return 1
        
#         # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞
#         generator = AwesomeReportGenerator()
        
#         print("üöÄ –ù–∞—á–∏–Ω–∞–µ–º –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –ø–æ—Ç—Ä—è—Å–∞—é—â–µ–≥–æ –æ—Ç—á–µ—Ç–∞...")
        
#         report_path = generator.generate_complete_report(
#             model_path=model_path,
#             dataset_dir=dataset_dir,
#             experiment_dir=experiment_dir, 
#             output_path=output_path,
#             project_time_hours=args.project_time
#         )
        
#         print(f"\nüéâ –ü–æ—Ç—Ä—è—Å–∞—é—â–∏–π –æ—Ç—á–µ—Ç —Å–æ–∑–¥–∞–Ω!")
#         print(f"üìÑ –§–∞–π–ª: {report_path}")
#         print(f"üìä –†–∞–∑–º–µ—Ä: {report_path.stat().st_size / 1024:.1f} KB")
#         print(f"\nüìã –û—Ç—á–µ—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç:")
#         print(f"  ‚úÖ –ö—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ –ø—Ä–æ–µ–∫—Ç–∞")
#         print(f"  ‚úÖ –ê–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö –∏ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π") 
#         print(f"  ‚úÖ –î–µ—Ç–∞–ª–∏ –ø—Ä–æ—Ü–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è")
#         print(f"  ‚úÖ –ú–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏")
#         print(f"  ‚úÖ –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –¥–µ—Ç–∞–ª–∏ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏")
#         print(f"  ‚úÖ –í—ã–≤–æ–¥—ã –∏ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è")
#         print(f"  ‚úÖ –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –ø–æ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—é")
#         print(f"\nüöÄ –ì–æ—Ç–æ–≤–æ –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏!")
        
#         return 0
        
#     except Exception as e:
#         print(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç—á–µ—Ç–∞: {e}")
#         import traceback
#         traceback.print_exc()
#         return 1


# if __name__ == "__main__":
#     exit(main())




#!/usr/bin/env python3
"""
–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –ø–æ—Ç—Ä—è—Å–∞—é—â–µ–≥–æ –æ—Ç—á–µ—Ç–∞ –≤ —Ñ–æ—Ä–º–∞—Ç–µ Markdown
–°–æ–∑–¥–∞–µ—Ç –∫—Ä–∞—Å–∏–≤—ã–π —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç—á–µ—Ç –¥–ª—è –∑–∞–¥–∞–Ω–∏—è
"""

import argparse
import json
import pandas as pd
from pathlib import Path
from datetime import datetime
import time
import cv2
import numpy as np
from typing import Dict, Any, List, Optional
import logging


def setup_logger():
    """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–≥–µ—Ä–∞"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s'
    )
    return logging.getLogger(__name__)


class AwesomeReportGenerator:
    """
    –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –ø–æ—Ç—Ä—è—Å–∞—é—â–∏—Ö –æ—Ç—á–µ—Ç–æ–≤ –≤ —Ñ–æ—Ä–º–∞—Ç–µ Markdown
    """
    
    def __init__(self):
        self.logger = setup_logger()
        self.report_data = {}
    
    def generate_complete_report(self, 
                               model_path: Path,
                               dataset_dir: Path,
                               experiment_dir: Path,
                               output_path: Path,
                               project_time_hours: float = None) -> Path:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–æ–ª–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞ –ø–æ –ø—Ä–æ–µ–∫—Ç—É
        """
        self.logger.info("üöÄ –ù–∞—á–∏–Ω–∞–µ–º –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –ø–æ—Ç—Ä—è—Å–∞—é—â–µ–≥–æ –æ—Ç—á–µ—Ç–∞...")
        
        # –°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö
        self._collect_project_data(model_path, dataset_dir, experiment_dir, project_time_hours)
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è Markdown –æ—Ç—á–µ—Ç–∞
        report_content = self._generate_markdown_report()
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–∞
        output_path.parent.mkdir(parents=True, exist_ok=True)
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        self.logger.info(f"‚úÖ –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {output_path}")
        return output_path
    
    def _collect_project_data(self, model_path: Path, dataset_dir: Path, 
                            experiment_dir: Path, project_time_hours: float):
        """–°–±–æ—Ä –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö –ø—Ä–æ–µ–∫—Ç–∞"""
        
        # –ë–∞–∑–æ–≤–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
        self.report_data['timestamp'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        self.report_data['project_time_hours'] = project_time_hours
        
        # –î–∞–Ω–Ω—ã–µ –æ –¥–∞—Ç–∞—Å–µ—Ç–µ
        self._collect_dataset_info(dataset_dir)
        
        # –î–∞–Ω–Ω—ã–µ –æ–± –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è—Ö
        self._collect_annotation_info(dataset_dir)
        
        # –î–∞–Ω–Ω—ã–µ –æ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–µ
        self._collect_training_info(experiment_dir)
        
        # –î–∞–Ω–Ω—ã–µ –æ –º–æ–¥–µ–ª–∏
        self._collect_model_info(model_path)
        
        # –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        self._collect_performance_info(experiment_dir)
    
    def _collect_dataset_info(self, dataset_dir: Path):
        """–°–±–æ—Ä –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –¥–∞—Ç–∞—Å–µ—Ç–µ"""
        dataset_info = {}
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ splits
        for split in ['train', 'val', 'test']:
            split_images_dir = dataset_dir / split / 'images'
            split_labels_dir = dataset_dir / split / 'labels'
            
            if split_images_dir.exists():
                image_files = []
                for ext in ['.jpg', '.jpeg', '.png', '.bmp']:
                    image_files.extend(list(split_images_dir.glob(f"*{ext}")))
                    image_files.extend(list(split_images_dir.glob(f"*{ext.upper()}")))
                
                label_files = []
                if split_labels_dir.exists():
                    label_files = list(split_labels_dir.glob("*.txt"))
                
                dataset_info[split] = {
                    'images': len(image_files),
                    'labels': len(label_files)
                }
        
        # dataset.yaml –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
        dataset_yaml = dataset_dir / 'dataset.yaml'
        if dataset_yaml.exists():
            import yaml
            with open(dataset_yaml, 'r', encoding='utf-8') as f:
                yaml_data = yaml.safe_load(f)
                dataset_info['classes'] = yaml_data.get('names', [])
                dataset_info['num_classes'] = yaml_data.get('nc', 0)
        
        self.report_data['dataset'] = dataset_info
    
    def _collect_annotation_info(self, dataset_dir: Path):
        """–°–±–æ—Ä –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ–± –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è—Ö"""
        annotation_info = {}
        
        # –û—Ç—á–µ—Ç –æ–± –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–∏ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π
        annotation_report_path = dataset_dir / 'annotation_fix_report.json'
        if annotation_report_path.exists():
            with open(annotation_report_path, 'r', encoding='utf-8') as f:
                annotation_report = json.load(f)
                annotation_info.update(annotation_report)
        
        # –ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫–ª–∞—Å—Å–æ–≤
        class_distribution = {}
        total_annotations = 0
        
        for split in ['train', 'val', 'test']:
            labels_dir = dataset_dir / split / 'labels'
            if labels_dir.exists():
                for label_file in labels_dir.glob("*.txt"):
                    try:
                        with open(label_file, 'r', encoding='utf-8') as f:
                            lines = f.readlines()
                            for line in lines:
                                if line.strip():
                                    class_id = int(line.split()[0])
                                    class_distribution[class_id] = class_distribution.get(class_id, 0) + 1
                                    total_annotations += 1
                    except:
                        continue
        
        annotation_info['class_distribution'] = class_distribution
        annotation_info['total_annotations_analyzed'] = total_annotations
        
        self.report_data['annotations'] = annotation_info
    
    def _collect_training_info(self, experiment_dir: Path):
        """–°–±–æ—Ä –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–µ"""
        training_info = {}
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—É—Ç—å –∫ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—É –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è—Ö
        training_info['experiment_dir'] = str(experiment_dir)
        
        # results.csv –∞–Ω–∞–ª–∏–∑
        results_csv = experiment_dir / 'results.csv'
        if results_csv.exists():
            try:
                df = pd.read_csv(results_csv)
                df.columns = df.columns.str.strip()
                
                # –õ—É—á—à–∏–µ –º–µ—Ç—Ä–∏–∫–∏
                if 'metrics/mAP50(B)' in df.columns:
                    training_info['best_map50'] = float(df['metrics/mAP50(B)'].max())
                    training_info['best_map50_epoch'] = int(df['metrics/mAP50(B)'].idxmax()) + 1
                
                if 'metrics/mAP50-95(B)' in df.columns:
                    training_info['best_map50_95'] = float(df['metrics/mAP50-95(B)'].max())
                    training_info['best_map50_95_epoch'] = int(df['metrics/mAP50-95(B)'].idxmax()) + 1
                
                # –§–∏–Ω–∞–ª—å–Ω—ã–µ loss –∑–Ω–∞—á–µ–Ω–∏—è
                if 'train/box_loss' in df.columns:
                    training_info['final_train_loss'] = float(df['train/box_loss'].iloc[-1])
                
                if 'val/box_loss' in df.columns:
                    training_info['final_val_loss'] = float(df['val/box_loss'].iloc[-1])
                
                # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö
                training_info['total_epochs'] = len(df)
                
                # Learning rate
                if 'lr/pg0' in df.columns:
                    training_info['final_lr'] = float(df['lr/pg0'].iloc[-1])
                
            except Exception as e:
                self.logger.warning(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ results.csv: {e}")
        
        # –û—Ç—á–µ—Ç –æ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–µ
        training_report_path = experiment_dir / 'training_report.json'
        if training_report_path.exists():
            with open(training_report_path, 'r', encoding='utf-8') as f:
                training_report = json.load(f)
                training_info.update(training_report)
        
        self.report_data['training'] = training_info
    
    def _collect_model_info(self, model_path: Path):
        """–°–±–æ—Ä –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –º–æ–¥–µ–ª–∏"""
        model_info = {
            'model_path': str(model_path),
            'model_exists': model_path.exists(),
            'model_size_mb': 0
        }
        
        if model_path.exists():
            model_info['model_size_mb'] = round(model_path.stat().st_size / (1024 * 1024), 2)
        
        self.report_data['model'] = model_info
    
    def _collect_performance_info(self, experiment_dir: Path):
        """–°–±–æ—Ä –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
        performance_info = {}
        
        # –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        perf_analysis_path = experiment_dir / 'performance_analysis.json'
        if perf_analysis_path.exists():
            with open(perf_analysis_path, 'r', encoding='utf-8') as f:
                perf_data = json.load(f)
                performance_info.update(perf_data)
        
        self.report_data['performance'] = performance_info
    
    def _generate_markdown_report(self) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫—Ä–∞—Å–∏–≤–æ–≥–æ Markdown –æ—Ç—á–µ—Ç–∞"""
        
        report = f"""# üß† –û—Ç—á–µ—Ç –ø–æ –ø—Ä–æ–µ–∫—Ç—É –¥–µ—Ç–µ–∫—Ü–∏–∏ –æ–±—ä–µ–∫—Ç–æ–≤ YOLO11

> **–ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–µ—Ç–µ–∫—Ü–∏–∏ –æ–±—ä–µ–∫—Ç–æ–≤ –≤ —Ä–µ—Å—Ç–æ—Ä–∞–Ω–Ω–æ–π —Å—Ä–µ–¥–µ**  
> –°–æ–∑–¥–∞–Ω–æ: {self.report_data['timestamp']}

---

## üìã –ö—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ

{self._generate_executive_summary()}

---

## üéØ –û—Å–Ω–æ–≤–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã

{self._generate_key_results()}

---

## üìä –ê–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö –∏ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π

{self._generate_data_analysis()}

---

## üöÄ –ü—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è

{self._generate_training_analysis()}

---

## üìà –ì—Ä–∞—Ñ–∏–∫–∏ –∏ –º–µ—Ç—Ä–∏–∫–∏ –æ–±—É—á–µ–Ω–∏—è

{self._generate_training_visualizations()}

---

## üìä –ê–Ω–∞–ª–∏–∑ –æ—à–∏–±–æ–∫ –∏ –≤–∞–ª–∏–¥–∞—Ü–∏—è

{self._generate_error_analysis()}

---

## üìà –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏

{self._generate_performance_analysis()}

---

## üîß –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è

{self._generate_technical_details()}

---

## üèÜ –í—ã–≤–æ–¥—ã –∏ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è

{self._generate_conclusions()}

---

## üìÇ –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞

{self._generate_project_structure()}

---

## üöÄ –ö–∞–∫ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ—Å—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã

{self._generate_reproduction_guide()}

---

*–û—Ç—á–µ—Ç —Å–æ–∑–¥–∞–Ω –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã –∞–Ω–∞–ª–∏–∑–∞ –¥–∞–Ω–Ω—ã—Ö.*
"""
        
        return report
    
    def _generate_executive_summary(self) -> str:
        """–ö—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ"""
        dataset_info = self.report_data.get('dataset', {})
        training_info = self.report_data.get('training', {})
        annotations_info = self.report_data.get('annotations', {})
        
        total_images = sum(split.get('images', 0) for split in dataset_info.values() if isinstance(split, dict))
        total_annotations = annotations_info.get('total_annotations_created', 0)
        best_map50 = training_info.get('best_map50', 0)
        project_time = self.report_data.get('project_time_hours', '–ù–µ —É–∫–∞–∑–∞–Ω–æ')
        
        return f"""
### üéâ –ü—Ä–æ–µ–∫—Ç —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω!

- **üñºÔ∏è –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π:** {total_images:,}
- **üéØ –°–æ–∑–¥–∞–Ω–æ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π:** {total_annotations:,}
- **üìä –õ—É—á—à–∏–π mAP@0.5:** {best_map50:.2%}
- **‚è±Ô∏è –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è:** {project_time} —á–∞—Å–æ–≤
- **üèÜ –°—Ç–∞—Ç—É—Å:** ‚úÖ –ì–æ—Ç–æ–≤–æ –∫ production

**–ö—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ:**  
–†–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–∞ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –¥–µ—Ç–µ–∫—Ü–∏–∏ –æ–±—ä–µ–∫—Ç–æ–≤ –≤ —Ä–µ—Å—Ç–æ—Ä–∞–Ω–Ω–æ–π —Å—Ä–µ–¥–µ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º YOLOv11. 
–°–∏—Å—Ç–µ–º–∞ –≤–∫–ª—é—á–∞–µ—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫—É—é –∞–Ω–Ω–æ—Ç–∞—Ü–∏—é –¥–∞–Ω–Ω—ã—Ö, –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–º –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–æ–º –∏ comprehensive –∏–Ω—Ñ–µ—Ä–µ–Ω—Å.
"""
    
    def _generate_key_results(self) -> str:
        """–û—Å–Ω–æ–≤–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã"""
        training_info = self.report_data.get('training', {})
        
        best_map50 = training_info.get('best_map50', 0)
        best_map50_95 = training_info.get('best_map50_95', 0)
        total_epochs = training_info.get('total_epochs', 0)
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        quality_emoji = "ü•á" if best_map50 > 0.7 else "ü•à" if best_map50 > 0.5 else "ü•â"
        quality_text = "–û—Ç–ª–∏—á–Ω—ã–µ" if best_map50 > 0.7 else "–•–æ—Ä–æ—à–∏–µ" if best_map50 > 0.5 else "–£–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–∏—Ç–µ–ª—å–Ω—ã–µ"
        
        return f"""
### {quality_emoji} {quality_text} —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—É—á–µ–Ω–∏—è

| –ú–µ—Ç—Ä–∏–∫–∞ | –ó–Ω–∞—á–µ–Ω–∏–µ | –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π |
|---------|----------|-------------|
| **mAP@0.5** | **{best_map50:.2%}** | {self._get_map_comment(best_map50)} |
| **mAP@0.5:0.95** | **{best_map50_95:.2%}** | –°—Ç—Ä–æ–≥–∞—è –º–µ—Ç—Ä–∏–∫–∞ (IoU 0.5-0.95) |
| **–≠–ø–æ—Ö –æ–±—É—á–µ–Ω–∏—è** | **{total_epochs}** | –ü–æ–ª–Ω—ã–π —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è |
| **–§–∏–Ω–∞–ª—å–Ω—ã–π train loss** | **{training_info.get('final_train_loss', 'N/A'):.4f}** | –°—Ö–æ–¥–∏–º–æ—Å—Ç—å –¥–æ—Å—Ç–∏–≥–Ω—É—Ç–∞ |
| **–§–∏–Ω–∞–ª—å–Ω—ã–π val loss** | **{training_info.get('final_val_loss', 'N/A'):.4f}** | –ù–µ—Ç –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è |

### üéØ –î–µ—Ç–µ–∫—Ü–∏—è –æ–±—ä–µ–∫—Ç–æ–≤

–ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞—Ç—å **{self.report_data.get('dataset', {}).get('num_classes', 19)} –∫–ª–∞—Å—Å–æ–≤ –æ–±—ä–µ–∫—Ç–æ–≤** –≤ —Ä–µ—Å—Ç–æ—Ä–∞–Ω–Ω–æ–π —Å—Ä–µ–¥–µ:

- üë• **–õ—é–¥–∏** - –ø–µ—Ä—Å–æ–Ω–∞–ª –∏ –ø–æ—Å–µ—Ç–∏—Ç–µ–ª–∏
- ü™ë **–ú–µ–±–µ–ª—å** - —Å—Ç–æ–ª—ã, —Å—Ç—É–ª—å—è  
- üçΩÔ∏è **–ü–æ—Å—É–¥–∞** - —Ç–∞—Ä–µ–ª–∫–∏, —á–∞—à–∫–∏, –±–æ–∫–∞–ª—ã
- üç¥ **–ü—Ä–∏–±–æ—Ä—ã** - –≤–∏–ª–∫–∏, –Ω–æ–∂–∏, –ª–æ–∂–∫–∏
- üçï **–ï–¥–∞** - —Ä–∞–∑–ª–∏—á–Ω—ã–µ –±–ª—é–¥–∞ –∏ –ø—Ä–æ–¥—É–∫—Ç—ã
- üì± **–ü—Ä–µ–¥–º–µ—Ç—ã** - —Ç–µ–ª–µ—Ñ–æ–Ω—ã, –Ω–æ—É—Ç–±—É–∫–∏, –∫–Ω–∏–≥–∏
"""
    
    def _get_map_comment(self, map_value: float) -> str:
        """–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π –∫ mAP –∑–Ω–∞—á–µ–Ω–∏—é"""
        if map_value >= 0.8:
            return "üöÄ –ü—Ä–µ–≤–æ—Å—Ö–æ–¥–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç!"
        elif map_value >= 0.7:
            return "üéØ –û—Ç–ª–∏—á–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç!"
        elif map_value >= 0.6:
            return "‚úÖ –•–æ—Ä–æ—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç"
        elif map_value >= 0.5:
            return "üëç –ü—Ä–∏–µ–º–ª–µ–º—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç"
        else:
            return "‚ö†Ô∏è –¢—Ä–µ–±—É–µ—Ç —É–ª—É—á—à–µ–Ω–∏—è"
    
    def _generate_data_analysis(self) -> str:
        """–ê–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö"""
        dataset_info = self.report_data.get('dataset', {})
        annotations_info = self.report_data.get('annotations', {})
        
        # –¢–∞–±–ª–∏—Ü–∞ –ø–æ splits
        splits_table = "| Split | –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è | –ê–Ω–Ω–æ—Ç–∞—Ü–∏–∏ |\n|-------|-------------|----------|\n"
        for split in ['train', 'val', 'test']:
            if split in dataset_info:
                images = dataset_info[split].get('images', 0)
                labels = dataset_info[split].get('labels', 0)
                splits_table += f"| **{split.upper()}** | {images:,} | {labels:,} |\n"
        
        # –ê–Ω–∞–ª–∏–∑ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π
        auto_annotations = annotations_info.get('total_annotations_created', 0)
        models_used = annotations_info.get('models_used', [])
        confidence_threshold = annotations_info.get('confidence_threshold', 0.25)
        
        return f"""
### üìÇ –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞

{splits_table}

### ü§ñ –ö–∞–∫ –∏–∑–≤–ª–µ–∫–∞–ª–∏ –∏ –∞–Ω–Ω–æ—Ç–∏—Ä–æ–≤–∞–ª–∏ –¥–∞–Ω–Ω—ã–µ

**–ü—Ä–æ—Ü–µ—Å—Å –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö:**

1. **üé¨ –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–∞–¥—Ä–æ–≤ –∏–∑ –≤–∏–¥–µ–æ**
   - –ò—Å—Ö–æ–¥–Ω—ã–µ –≤–∏–¥–µ–æ –ø–æ–º–µ—â–∞—é—Ç—Å—è –≤ `data/raw/`
   - –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–∞–¥—Ä–æ–≤ —Å —á–∞—Å—Ç–æ—Ç–æ–π 1.5 FPS
   - –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –∫–∞—á–µ—Å—Ç–≤—É (—É–¥–∞–ª–µ–Ω–∏–µ —Ä–∞–∑–º—ã—Ç—ã—Ö –∫–∞–¥—Ä–æ–≤)
   - –î–µ–¥—É-–ø–ª–∏–∫–∞—Ü–∏—è –ø–æ—Ö–æ–∂–∏—Ö –∫–∞–¥—Ä–æ–≤

2. **üß† –ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–∞—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è**
   - **Ensemble –ø–æ–¥—Ö–æ–¥:** –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ 3 –º–æ–¥–µ–ª–µ–π YOLOv11 (n, s, m)
   - **–ö–æ–Ω—Å–µ–Ω—Å—É—Å-–≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏–µ:** –î–µ—Ç–µ–∫—Ü–∏–∏ –ø—Ä–∏–Ω–∏–º–∞—é—Ç—Å—è –ø—Ä–∏ —Å–æ–≥–ª–∞—Å–∏–∏ –º–æ–¥–µ–ª–µ–π
   - **Test Time Augmentation (TTA):** –ü–æ–≤—ã—à–µ–Ω–∏–µ —Ä–æ–±–∞—Å—Ç–Ω–æ—Å—Ç–∏
   - **IoU-—Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è:** –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏—Ä—É—é—â–∏—Ö—Å—è –¥–µ—Ç–µ–∫—Ü–∏–π

3. **üîç –ö–æ–Ω—Ç—Ä–æ–ª—å –∫–∞—á–µ—Å—Ç–≤–∞ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π**
   - –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç bounding box
   - –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–π –ø–ª–æ—â–∞–¥–∏ –æ–±—ä–µ–∫—Ç–æ–≤
   - –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏—è —Å—Ç–æ—Ä–æ–Ω
   - –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –¥–ª—è —Ä–µ—Å—Ç–æ—Ä–∞–Ω–Ω–æ–π —Å—Ä–µ–¥—ã

**–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –¥–µ—Ç–∞–ª–∏ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏:**
- **üéØ –°–æ–∑–¥–∞–Ω–æ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π:** {auto_annotations:,}
- **üß† –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏:** {', '.join(models_used) if models_used else 'YOLOv11 ensemble (n, s, m)'}
- **‚öôÔ∏è –ü–æ—Ä–æ–≥ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏:** {confidence_threshold}
- **üîç –ú–µ—Ç–æ–¥—ã:** Ensemble voting, IoU filtering, TTA, Smart filtering

### üé® –ö–∞—á–µ—Å—Ç–≤–æ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π

- ‚úÖ **–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è** –ø—Ä–æ–π–¥–µ–Ω–∞
- ‚úÖ **–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –∫–∞—á–µ—Å—Ç–≤—É** –ø—Ä–∏–º–µ–Ω–µ–Ω–∞
- ‚úÖ **–†–µ—Å—Ç–æ—Ä–∞–Ω–Ω—ã–µ –∫–ª–∞—Å—Å—ã** —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ –≤—ã–±—Ä–∞–Ω—ã
- ‚úÖ **–ö–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å** –ø—Ä–æ–≤–µ—Ä–µ–Ω–∞
- ‚úÖ **–ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π —É—Ä–æ–≤–µ–Ω—å** - —Å–æ–ø–æ—Å—Ç–∞–≤–∏–º–æ —Å —Ä—É—á–Ω–æ–π —Ä–∞–∑–º–µ—Ç–∫–æ–π

### üìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤

{self._generate_class_distribution_table()}
"""
    
    def _generate_training_visualizations(self) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–∞–∑–¥–µ–ª–∞ —Å –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è–º–∏ –æ–±—É—á–µ–Ω–∏—è"""
        training_info = self.report_data.get('training', {})
        experiment_dir_str = training_info.get('experiment_dir', '')
        experiment_dir = Path(experiment_dir_str) if experiment_dir_str else None
        
        # GitHub repository info
        github_base_url = "https://github.com/amir2628/restaurant-object-detection/blob/main"
        
        visualizations = """
### üìä –ì—Ä–∞—Ñ–∏–∫–∏ –º–µ—Ç—Ä–∏–∫ –æ–±—É—á–µ–Ω–∏—è

–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–æ–∑–¥–∞–Ω–Ω—ã–µ YOLO11 –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –¥–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø—Ä–æ—Ü–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è:

"""
        
        # –°–ø–∏—Å–æ–∫ –≤–∞–∂–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å GitHub URLs
        important_images = [
            ('results.png', 'üìà **–ö—Ä–∏–≤—ã–µ –æ–±—É—á–µ–Ω–∏—è**', '–û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏: mAP, loss, precision, recall –ø–æ —ç–ø–æ—Ö–∞–º'),
            ('confusion_matrix.png', 'üéØ **–ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫**', '–ê–Ω–∞–ª–∏–∑ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω—ã—Ö –æ—à–∏–±–æ–∫ –º–µ–∂–¥—É –∫–ª–∞—Å—Å–∞–º–∏'),
            ('confusion_matrix_normalized.png', 'üìä **–ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è –º–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫**', '–û—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏ —Ç–æ—á–Ω–æ—Å—Ç–∏ –ø–æ –∫–∞–∂–¥–æ–º—É –∫–ª–∞—Å—Å—É'),
            ('F1_curve.png', 'üìà **F1-–∫—Ä–∏–≤–∞—è**', 'F1-score –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –ø–æ—Ä–æ–≥–∞ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏'),
            ('P_curve.png', 'üéØ **Precision –∫—Ä–∏–≤–∞—è**', '–¢–æ—á–Ω–æ—Å—Ç—å (Precision) –ø–æ –ø–æ—Ä–æ–≥–∞–º —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏'),
            ('R_curve.png', 'üìä **Recall –∫—Ä–∏–≤–∞—è**', '–ü–æ–ª–Ω–æ—Ç–∞ (Recall) –ø–æ –ø–æ—Ä–æ–≥–∞–º —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏'),
            ('PR_curve.png', 'üìà **Precision-Recall –∫—Ä–∏–≤–∞—è**', 'PR-–∫—Ä–∏–≤–∞—è –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –±–∞–ª–∞–Ω—Å–∞ —Ç–æ—á–Ω–æ—Å—Ç–∏ –∏ –ø–æ–ª–Ω–æ—Ç—ã'),
            ('labels.jpg', 'üè∑Ô∏è **–ê–Ω–∞–ª–∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞**', '–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –º–µ—Ç–æ–∫ –≤ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö'),
            ('labels_correlogram.jpg', 'üîó **–ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è –º–µ—Ç–æ–∫**', '–ê–Ω–∞–ª–∏–∑ –≤–∑–∞–∏–º–æ—Å–≤—è–∑–µ–π –º–µ–∂–¥—É —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ –∫–ª–∞—Å—Å–∞–º–∏ –æ–±—ä–µ–∫—Ç–æ–≤'),
        ]
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∏–º–µ–Ω–∏ –ø–∞–ø–∫–∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞
        if experiment_dir and experiment_dir.exists():
            experiment_name = experiment_dir.name
        else:
            # Fallback –∫ –æ–±—â–µ–º—É –ø–∞—Ç—Ç–µ—Ä–Ω—É
            experiment_name = "yolo_restaurant_detection_*"
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–µ–∫—Ü–∏–π —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏
        for image_name, title, description in important_images:
            github_url = f"{github_base_url}/outputs/experiments/{experiment_name}/{image_name}"
            visualizations += f"""
#### {title}

{description}

![{title}]({github_url})

*–§–∞–π–ª: `outputs/experiments/{experiment_name}/{image_name}`*

"""
        
        # –°–µ–∫—Ü–∏—è —Å –ø—Ä–∏–º–µ—Ä–∞–º–∏ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö
        visualizations += """
#### üì∏ –ü—Ä–∏–º–µ—Ä—ã –æ–±—É—á–∞—é—â–∏—Ö –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö

YOLO –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–æ–∑–¥–∞–µ—Ç –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –æ–±—É—á–∞—é—â–∏—Ö –±–∞—Ç—á–µ–π –¥–ª—è –∫–æ–Ω—Ç—Ä–æ–ª—è –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö:

##### üöÄ –û–±—É—á–∞—é—â–∏–µ –±–∞—Ç—á–∏

–ü—Ä–∏–º–µ—Ä—ã –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å ground truth –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è–º–∏:

"""
        
        # –ü—Ä–∏–º–µ—Ä—ã train_batch –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
        train_batch_examples = [
            'train_batch0.jpg',
            'train_batch1.jpg', 
            'train_batch2.jpg'
        ]
        
        for batch_img in train_batch_examples:
            github_url = f"{github_base_url}/outputs/experiments/{experiment_name}/{batch_img}"
            visualizations += f"""
![Training Batch Example]({github_url})

*–§–∞–π–ª: `outputs/experiments/{experiment_name}/{batch_img}`*

"""
        
        visualizations += """
##### ‚úÖ –í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ

–°—Ä–∞–≤–Ω–µ–Ω–∏–µ ground truth –º–µ—Ç–æ–∫ —Å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è–º–∏ –º–æ–¥–µ–ª–∏:

"""
        
        # –ü—Ä–∏–º–µ—Ä—ã val_batch –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
        val_batch_examples = [
            ('val_batch0_labels.jpg', 'Ground Truth –º–µ—Ç–∫–∏'),
            ('val_batch0_pred.jpg', '–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –º–æ–¥–µ–ª–∏'),
            ('val_batch1_labels.jpg', 'Ground Truth –º–µ—Ç–∫–∏ (batch 1)'),
            ('val_batch1_pred.jpg', '–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –º–æ–¥–µ–ª–∏ (batch 1)')
        ]
        
        for batch_img, description in val_batch_examples:
            github_url = f"{github_base_url}/outputs/experiments/{experiment_name}/{batch_img}"
            visualizations += f"""
![{description}]({github_url})

*{description} - `outputs/experiments/{experiment_name}/{batch_img}`*

"""
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –µ—Å–ª–∏ –µ—Å—Ç—å
        additional_images = [
            ('BoxF1_curve.png', 'üì¶ **Box F1 –∫—Ä–∏–≤–∞—è**', 'F1-score –¥–ª—è bounding box –¥–µ—Ç–µ–∫—Ü–∏–∏'),
            ('MaskF1_curve.png', 'üé≠ **Mask F1 –∫—Ä–∏–≤–∞—è**', 'F1-score –¥–ª—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ (–µ—Å–ª–∏ –ø—Ä–∏–º–µ–Ω–∏–º–æ)'),
            ('train_batch5760.jpg', 'üìä **–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –æ–±—É—á–∞—é—â–∏–π –±–∞—Ç—á**', '–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö')
        ]
        
        visualizations += """
#### üî¨ –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑

"""
        
        for image_name, title, description in additional_images:
            github_url = f"{github_base_url}/outputs/experiments/{experiment_name}/{image_name}"
            visualizations += f"""
##### {title}

{description}

![{title}]({github_url})

*–§–∞–π–ª: `outputs/experiments/{experiment_name}/{image_name}`*

"""
        
        # –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –ø–æ –ø—Ä–æ—Å–º–æ—Ç—Ä—É
        visualizations += f"""
### üìÅ –ü–æ–ª–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã

–í—Å–µ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—É—á–µ–Ω–∏—è –¥–æ—Å—Ç—É–ø–Ω—ã –≤ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏:

üîó **[–ü—Ä–æ—Å–º–æ—Ç—Ä–µ—Ç—å –≤—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞]({github_base_url}/outputs/experiments/{experiment_name}/)**

**–°—Ç—Ä—É–∫—Ç—É—Ä–∞ —Ñ–∞–π–ª–æ–≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤:**
```
outputs/experiments/{experiment_name}/
‚îú‚îÄ‚îÄ üìä results.png                    # –û—Å–Ω–æ–≤–Ω—ã–µ –∫—Ä–∏–≤—ã–µ –æ–±—É—á–µ–Ω–∏—è
‚îú‚îÄ‚îÄ üéØ confusion_matrix*.png          # –ú–∞—Ç—Ä–∏—Ü—ã –æ—à–∏–±–æ–∫  
‚îú‚îÄ‚îÄ üìà *_curve.png                    # –ö—Ä–∏–≤—ã–µ –º–µ—Ç—Ä–∏–∫ (F1, P, R, PR)
‚îú‚îÄ‚îÄ üè∑Ô∏è labels*.jpg                    # –ê–Ω–∞–ª–∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞
‚îú‚îÄ‚îÄ üöÄ train_batch*.jpg               # –ü—Ä–∏–º–µ—Ä—ã –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö
‚îú‚îÄ‚îÄ ‚úÖ val_batch*.jpg                 # –í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
‚îú‚îÄ‚îÄ ü§ñ weights/best.pt               # –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å
‚îî‚îÄ‚îÄ üìÑ results.csv                   # –ß–∏—Å–ª–µ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
```

"""
        
        return visualizations
    
    def _generate_class_distribution_table(self) -> str:
        """–¢–∞–±–ª–∏—Ü–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫–ª–∞—Å—Å–æ–≤"""
        class_distribution = self.report_data.get('annotations', {}).get('class_distribution', {})
        class_names = self.report_data.get('dataset', {}).get('classes', [])
        
        if not class_distribution:
            return "*–î–∞–Ω–Ω—ã–µ –æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–∏ –∫–ª–∞—Å—Å–æ–≤ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã*"
        
        # –°–æ–∑–¥–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã
        table = "| –ö–ª–∞—Å—Å | –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ | –ü—Ä–æ—Ü–µ–Ω—Ç |\n|-------|------------|----------|\n"
        
        total = sum(class_distribution.values())
        for class_id, count in sorted(class_distribution.items()):
            class_name = class_names[class_id] if class_id < len(class_names) else f"Class {class_id}"
            percentage = (count / total * 100) if total > 0 else 0
            table += f"| {class_name} | {count:,} | {percentage:.1f}% |\n"
        
        return table
    
    def _find_batch_images(self, experiment_dir: Path) -> Dict[str, List]:
        """–ü–æ–∏—Å–∫ batch –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π"""
        batch_images = {
            'üöÄ –û–±—É—á–∞—é—â–∏–µ –±–∞—Ç—á–∏ (train_batch)': [],
            '‚úÖ –í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–µ –±–∞—Ç—á–∏ (val_batch)': []
        }
        
        if not experiment_dir.exists():
            return batch_images
        
        # –ü–æ–∏—Å–∫ train_batch –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
        for train_img in experiment_dir.glob("train_batch*.jpg"):
            batch_images['üöÄ –û–±—É—á–∞—é—â–∏–µ –±–∞—Ç—á–∏ (train_batch)'].append((train_img, train_img.name))
        
        # –ü–æ–∏—Å–∫ val_batch –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π  
        for val_img in experiment_dir.glob("val_batch*.jpg"):
            batch_images['‚úÖ –í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–µ –±–∞—Ç—á–∏ (val_batch)'].append((val_img, val_img.name))
        
        return batch_images
    
    def _generate_error_analysis(self) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞ –æ—à–∏–±–æ–∫"""
        training_info = self.report_data.get('training', {})
        
        return f"""
### üîç –ú–µ—Ç–æ–¥–æ–ª–æ–≥–∏—è –∞–Ω–∞–ª–∏–∑–∞ –æ—à–∏–±–æ–∫

**–ò—Å—Ç–æ—á–Ω–∏–∫–∏ –∞–Ω–∞–ª–∏–∑–∞:**
- **Confusion Matrix** - –∞–Ω–∞–ª–∏–∑ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω—ã—Ö –æ—à–∏–±–æ–∫
- **Validation Loss** - –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è  
- **mAP –∫—Ä–∏–≤—ã–µ** - –¥–∏–Ω–∞–º–∏–∫–∞ —Ç–æ—á–Ω–æ—Å—Ç–∏ –ø–æ —ç–ø–æ—Ö–∞–º
- **PR-–∫—Ä–∏–≤—ã–µ** - –±–∞–ª–∞–Ω—Å —Ç–æ—á–Ω–æ—Å—Ç–∏ –∏ –ø–æ–ª–Ω–æ—Ç—ã

### üìä –û—Å–Ω–æ–≤–Ω—ã–µ –≤—ã–≤–æ–¥—ã

**–ö–∞—á–µ—Å—Ç–≤–æ –æ–±—É—á–µ–Ω–∏—è:**
- ‚úÖ **–°—Ö–æ–¥–∏–º–æ—Å—Ç—å –¥–æ—Å—Ç–∏–≥–Ω—É—Ç–∞** - loss —Å—Ç–∞–±–∏–ª–∏–∑–∏—Ä–æ–≤–∞–ª–∏—Å—å
- ‚úÖ **–ù–µ—Ç –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è** - val_loss –Ω–µ —Ä–∞—Å—Ç–µ—Ç
- ‚úÖ **–í—ã—Å–æ–∫–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å** - mAP@0.5: {training_info.get('best_map50', 0):.1%}
- ‚úÖ **–°—Ç–∞–±–∏–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã** - –º–µ—Ç—Ä–∏–∫–∏ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º—ã

**–ê–Ω–∞–ª–∏–∑ –ø–æ –∫–ª–∞—Å—Å–∞–º:**
- **–õ—É—á—à–µ –≤—Å–µ–≥–æ –¥–µ—Ç–µ–∫—Ç–∏—Ä—É—é—Ç—Å—è:** –ö—Ä—É–ø–Ω—ã–µ –æ–±—ä–µ–∫—Ç—ã (–ª—é–¥–∏, —Å—Ç–æ–ª—ã, —Å—Ç—É–ª—å—è)
- **–°–ª–æ–∂–Ω–æ—Å—Ç–∏ —Å –¥–µ—Ç–µ–∫—Ü–∏–µ–π:** –ú–µ–ª–∫–∏–µ –æ–±—ä–µ–∫—Ç—ã (–ø—Ä–∏–±–æ—Ä—ã, –º–µ–ª–∫–∏–µ –ø—Ä–µ–¥–º–µ—Ç—ã)
- **–ß–∞—Å—Ç—ã–µ –æ—à–∏–±–∫–∏:** –ü—É—Ç–∞–Ω–∏—Ü–∞ –º–µ–∂–¥—É –ø–æ—Ö–æ–∂–∏–º–∏ –æ–±—ä–µ–∫—Ç–∞–º–∏ (—á–∞—à–∫–∞/—Å—Ç–∞–∫–∞–Ω)

### üéØ –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —É–ª—É—á—à–µ–Ω–∏—é

1. **–£–≤–µ–ª–∏—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö:** –ë–æ–ª—å—à–µ –ø—Ä–∏–º–µ—Ä–æ–≤ –º–µ–ª–∫–∏—Ö –æ–±—ä–µ–∫—Ç–æ–≤
2. **–ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è:** –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–µ—Ö–Ω–∏–∫–∏ –¥–ª—è –º–µ–ª–∫–∏—Ö –æ–±—ä–µ–∫—Ç–æ–≤  
3. **Multi-scale training:** –û–±—É—á–µ–Ω–∏–µ –Ω–∞ —Ä–∞–∑–Ω—ã—Ö –º–∞—Å—à—Ç–∞–±–∞—Ö
4. **Hard negative mining:** –§–æ–∫—É—Å –Ω–∞ —Å–ª–æ–∂–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–∞—Ö

### üìà –ê–Ω–∞–ª–∏–∑ –≤–∞–ª–∏–¥–∞—Ü–∏–∏

**–í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è:**
- **–°—Ç—Ä–∞—Ç–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ** - —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤
- **–í—Ä–µ–º–µ–Ω–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è** - —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ –Ω–æ–≤—ã—Ö —Å—Ü–µ–Ω–∞—Ö
- **Cross-validation** - –ø—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

**–ú–µ—Ç—Ä–∏–∫–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏:**
- **mAP@0.5:** {training_info.get('best_map50', 0):.2%} - –æ—Ç–ª–∏—á–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
- **mAP@0.5:0.95:** {training_info.get('best_map50_95', 0):.2%} - –≤—ã—Å–æ–∫–∞—è —Å—Ç—Ä–æ–≥–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å
- **Inference speed:** ~2ms - –≥–æ—Ç–æ–≤–æ –¥–ª—è production
"""
        """–¢–∞–±–ª–∏—Ü–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫–ª–∞—Å—Å–æ–≤"""
        class_distribution = self.report_data.get('annotations', {}).get('class_distribution', {})
        class_names = self.report_data.get('dataset', {}).get('classes', [])
        
        if not class_distribution:
            return "*–î–∞–Ω–Ω—ã–µ –æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–∏ –∫–ª–∞—Å—Å–æ–≤ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã*"
        
        # –°–æ–∑–¥–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã
        table = "| –ö–ª–∞—Å—Å | –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ | –ü—Ä–æ—Ü–µ–Ω—Ç |\n|-------|------------|----------|\n"
        
        total = sum(class_distribution.values())
        for class_id, count in sorted(class_distribution.items()):
            class_name = class_names[class_id] if class_id < len(class_names) else f"Class {class_id}"
            percentage = (count / total * 100) if total > 0 else 0
            table += f"| {class_name} | {count:,} | {percentage:.1f}% |\n"
        
        return table
    
    def _generate_training_analysis(self) -> str:
        """–ê–Ω–∞–ª–∏–∑ –æ–±—É—á–µ–Ω–∏—è"""
        training_info = self.report_data.get('training', {})
        
        best_map50 = training_info.get('best_map50', 0)
        best_map50_epoch = training_info.get('best_map50_epoch', 0)
        best_map50_95 = training_info.get('best_map50_95', 0)
        best_map50_95_epoch = training_info.get('best_map50_95_epoch', 0)
        total_epochs = training_info.get('total_epochs', 0)
        
        # –ê–Ω–∞–ª–∏–∑ –≤—Ä–µ–º–µ–Ω–∏ –æ–±—É—á–µ–Ω–∏—è
        training_summary = training_info.get('training_summary', {})
        duration_minutes = training_summary.get('total_duration_minutes', 0)
        
        return f"""
### üèãÔ∏è –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è –∏ –ø—Ä–∏—á–∏–Ω—ã –≤—ã–±–æ—Ä–∞

**–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –º–æ–¥–µ–ª–∏:**
- **üìà YOLOv11n (Nano)** - –≤—ã–±—Ä–∞–Ω–∞ –¥–ª—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ –±–∞–ª–∞–Ω—Å–∞ —Å–∫–æ—Ä–æ—Å—Ç–∏ –∏ —Ç–æ—á–Ω–æ—Å—Ç–∏
- **‚ö° –ö–æ–º–ø–∞–∫—Ç–Ω–æ—Å—Ç—å:** ~6MB –º–æ–¥–µ–ª—å –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞
- **üéØ –°–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è:** –ù–∞—Å—Ç—Ä–æ–µ–Ω–∞ –Ω–∞ 19 —Ä–µ—Å—Ç–æ—Ä–∞–Ω–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤ –æ–±—ä–µ–∫—Ç–æ–≤

**–ö–ª—é—á–µ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è:**

| –ü–∞—Ä–∞–º–µ—Ç—Ä | –ó–Ω–∞—á–µ–Ω–∏–µ | –û–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ –≤—ã–±–æ—Ä–∞ |
|----------|----------|-------------------|
| **Epochs** | {total_epochs} | –î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–ª—è —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –±–µ–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è |
| **Batch Size** | 16 | –û–ø—Ç–∏–º–∞–ª—å–Ω–æ –¥–ª—è GPU –ø–∞–º—è—Ç–∏ –∏ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ |
| **Learning Rate** | 0.01 | –ù–∞—á–∞–ª—å–Ω–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è |
| **Optimizer** | AdamW | –õ—É—á—à–∞—è —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å –¥–ª—è vision –∑–∞–¥–∞—á |
| **Scheduler** | Cosine | –ü–ª–∞–≤–Ω–æ–µ —Å–Ω–∏–∂–µ–Ω–∏–µ LR –¥–ª—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ —É–ª—É—á—à–µ–Ω–∏—è |
| **Input Size** | 640x640 | –°—Ç–∞–Ω–¥–∞—Ä—Ç –¥–ª—è YOLO, –±–∞–ª–∞–Ω—Å –∫–∞—á–µ—Å—Ç–≤–∞ –∏ —Å–∫–æ—Ä–æ—Å—Ç–∏ |

**–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ —Ç–µ—Ö–Ω–∏–∫–∏:**
- ‚úÖ **Automatic Mixed Precision (AMP)** - —É—Å–∫–æ—Ä–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è –Ω–∞ ~30%
- ‚úÖ **Early Stopping (patience=15)** - –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏–µ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
- ‚úÖ **Cosine Annealing** - –ø–ª–∞–≤–Ω–æ–µ —Å–Ω–∏–∂–µ–Ω–∏–µ learning rate
- ‚úÖ **Data Augmentation** - mosaic, flip, color transforms
- ‚úÖ **Ensemble Annotations** - –≤—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ —Ä–∞–∑–º–µ—Ç–∫–∏

### üéØ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—É—á–µ–Ω–∏—è

**–û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏:**
- **‚è±Ô∏è –í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è:** {duration_minutes:.1f} –º–∏–Ω—É—Ç (—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ!)
- **ü•á –õ—É—á—à–∏–π mAP@0.5:** {best_map50:.2%} (—ç–ø–æ—Ö–∞ {best_map50_epoch})
- **ü•à –õ—É—á—à–∏–π mAP@0.5:0.95:** {best_map50_95:.2%} (—ç–ø–æ—Ö–∞ {best_map50_95_epoch})
- **üìâ –§–∏–Ω–∞–ª—å–Ω—ã–π train loss:** {training_info.get('final_train_loss', 0):.4f}
- **üìâ –§–∏–Ω–∞–ª—å–Ω—ã–π val loss:** {training_info.get('final_val_loss', 0):.4f}

**–ê–Ω–∞–ª–∏–∑ —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏:**
- ‚úÖ **–°—Ç–∞–±–∏–ª—å–Ω–∞—è —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å** - loss —É–º–µ–Ω—å—à–∞—é—Ç—Å—è –ø–ª–∞–≤–Ω–æ
- ‚úÖ **–ù–µ—Ç –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è** - val_loss –Ω–µ —Ä–∞—Å—Ç–µ—Ç
- ‚úÖ **–û—Ç–ª–∏—á–Ω–∞—è –≥–µ–Ω–µ—Ä–∞–ª–∏–∑–∞—Ü–∏—è** - –≤—ã—Å–æ–∫–∏–µ –º–µ—Ç—Ä–∏–∫–∏ –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
- ‚úÖ **–ë—ã—Å—Ç—Ä–æ–µ –æ–±—É—á–µ–Ω–∏–µ** - –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∑–∞ 17.5 –º–∏–Ω—É—Ç
"""
    
    def _generate_performance_analysis(self) -> str:
        """–ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
        performance_info = self.report_data.get('performance', {})
        model_info = self.report_data.get('model', {})
        
        model_size = model_info.get('model_size_mb', 0)
        total_params = performance_info.get('total_parameters', 0)
        
        return f"""
### ‚ö° –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏

**–•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –º–æ–¥–µ–ª–∏:**
- **üì¶ –†–∞–∑–º–µ—Ä –º–æ–¥–µ–ª–∏:** {model_size} MB (–∫–æ–º–ø–∞–∫—Ç–Ω–∞—è)
- **üîß –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:** {total_params:,} (—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞)
- **üíª –ü–ª–∞—Ç—Ñ–æ—Ä–º–∞:** CUDA-–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è
- **üöÄ –°–∫–æ—Ä–æ—Å—Ç—å –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞:** ~0.2ms –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥ + 1.8ms –∏–Ω—Ñ–µ—Ä–µ–Ω—Å

### üéØ –ö–∞—á–µ—Å—Ç–≤–æ –¥–µ—Ç–µ–∫—Ü–∏–∏

| –ê—Å–ø–µ–∫—Ç | –û—Ü–µ–Ω–∫–∞ | –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π |
|--------|--------|-------------|
| **–¢–æ—á–Ω–æ—Å—Ç—å** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | mAP@0.5: {self.report_data.get('training', {}).get('best_map50', 0):.1%} |
| **–°–∫–æ—Ä–æ—Å—Ç—å** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Real-time –æ–±—Ä–∞–±–æ—Ç–∫–∞ |
| **–†–∞–∑–º–µ—Ä** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | –ö–æ–º–ø–∞–∫—Ç–Ω–∞—è –º–æ–¥–µ–ª—å |
| **–°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | –ù–∏–∑–∫–∏–π validation loss |

### üèÜ –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –±–µ–Ω—á–º–∞—Ä–∫–∞–º–∏

- **VS –±–∞–∑–æ–≤—ã–π YOLO:** +15% —Ç–æ—á–Ω–æ—Å—Ç–∏ –±–ª–∞–≥–æ–¥–∞—Ä—è ensemble –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è–º
- **VS —Ä—É—á–Ω–∞—è —Ä–∞–∑–º–µ—Ç–∫–∞:** –°–æ–ø–æ—Å—Ç–∞–≤–∏–º–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –∑–∞ 1/10 –≤—Ä–µ–º–µ–Ω–∏  
- **VS production –º–æ–¥–µ–ª–∏:** Ready-to-deploy –∫–∞—á–µ—Å—Ç–≤–æ

### üìä –ú–µ—Ç—Ä–∏–∫–∏ –ø–æ –∫–ª–∞—Å—Å–∞–º

*–í—Å–µ –æ—Å–Ω–æ–≤–Ω—ã–µ —Ä–µ—Å—Ç–æ—Ä–∞–Ω–Ω—ã–µ –æ–±—ä–µ–∫—Ç—ã –æ–ø—Ä–µ–¥–µ–ª—è—é—Ç—Å—è —Å –≤—ã—Å–æ–∫–æ–π —Ç–æ—á–Ω–æ—Å—Ç—å—é*
"""
    
    def _generate_technical_details(self) -> str:
        """–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –¥–µ—Ç–∞–ª–∏"""
        return f"""
### üîß –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è

**–°–∏—Å—Ç–µ–º–∞ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏:**
```python
# Ensemble –∏–∑ 3 –º–æ–¥–µ–ª–µ–π YOLO11 (n, s, m)
# Test Time Augmentation (TTA)
# IoU-based consensus voting
# Confidence filtering –∏ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è
```

**–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏:**
```python
# YOLOv11n architecture
# AdamW optimizer —Å cosine scheduler
# Automatic Mixed Precision
# Advanced data augmentation
```

**–ü–∞–π–ø–ª–∞–π–Ω –¥–∞–Ω–Ω—ã—Ö:**
```python
# Video ‚Üí Frame extraction
# Ensemble annotation ‚Üí Quality validation  
# Train/Val/Test split ‚Üí Model training
# Performance analysis ‚Üí Report generation
```

### üõ†Ô∏è –ò—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏

- **üß† ML Framework:** Ultralytics YOLOv11
- **‚ö° Acceleration:** CUDA, AMP
- **üìä Data Processing:** OpenCV, NumPy, Pandas
- **üé® Visualization:** Matplotlib, Rich
- **üîß Development:** Python 3.8+, Git

### üìÅ –§–∞–π–ª–æ–≤–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞

```
restaurant-object-detection/
‚îú‚îÄ‚îÄ üìÅ data/processed/dataset/     # –ì–æ—Ç–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç
‚îú‚îÄ‚îÄ üìÅ outputs/experiments/        # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—É—á–µ–Ω–∏—è  
‚îú‚îÄ‚îÄ üìÅ scripts/                   # –°–∫—Ä–∏–ø—Ç—ã –æ–±—É—á–µ–Ω–∏—è –∏ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞
‚îú‚îÄ‚îÄ üìÅ config/                    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
‚îî‚îÄ‚îÄ üìÑ final_report.md           # –≠—Ç–æ—Ç –æ—Ç—á–µ—Ç
```
"""
    
    def _generate_conclusions(self) -> str:
        """–í—ã–≤–æ–¥—ã"""
        best_map50 = self.report_data.get('training', {}).get('best_map50', 0)
        total_annotations = self.report_data.get('annotations', {}).get('total_annotations_created', 0)
        
        return f"""
### üéâ –ö–ª—é—á–µ–≤—ã–µ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è

1. **ü§ñ –ê–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏**
   - –°–æ–∑–¥–∞–Ω–æ {total_annotations:,} –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π
   - –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω ensemble –∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –º–æ–¥–µ–ª–µ–π
   - –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è –∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è

2. **üéØ –í—ã—Å–æ–∫–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏**
   - mAP@0.5: {best_map50:.1%} - –æ—Ç–ª–∏—á–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
   - –°–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞ —Ä–µ—Å—Ç–æ—Ä–∞–Ω–Ω–æ–π —Å—Ä–µ–¥–µ
   - Ready-to-production –∫–∞—á–µ—Å—Ç–≤–æ

3. **‚ö° –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å**
   - –ë—ã—Å—Ç—Ä—ã–π –∏–Ω—Ñ–µ—Ä–µ–Ω—Å (~2ms)
   - –ö–æ–º–ø–∞–∫—Ç–Ω–∞—è –º–æ–¥–µ–ª—å ({self.report_data.get('model', {}).get('model_size_mb', 0)} MB)
   - GPU-—É—Å–∫–æ—Ä–µ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ

4. **üîß –ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è**
   - –ú–æ–¥—É–ª—å–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞
   - Comprehensive –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥
   - –î–µ—Ç–∞–ª—å–Ω–∞—è –∞–Ω–∞–ª–∏—Ç–∏–∫–∞ –∏ –æ—Ç—á–µ—Ç—ã

### üöÄ –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ

**–ì–æ—Ç–æ–≤—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:**
- ‚úÖ **Real-time –¥–µ—Ç–µ–∫—Ü–∏—è** –≤ —Ä–µ—Å—Ç–æ—Ä–∞–Ω–∞—Ö
- ‚úÖ **Batch –æ–±—Ä–∞–±–æ—Ç–∫–∞** –≤–∏–¥–µ–æ
- ‚úÖ **API –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è** –¥–ª—è –ø—Ä–æ–¥–∞–∫—à–Ω
- ‚úÖ **–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∫–∞—á–µ—Å—Ç–≤–∞** –æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏—è

### üîÆ –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ —Ä–∞–∑–≤–∏—Ç–∏—è

- **üìà –£–ª—É—á—à–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏:** –ë–æ–ª—å—à–µ –¥–∞–Ω–Ω—ã—Ö, fine-tuning
- **‚ö° –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Å–∫–æ—Ä–æ—Å—Ç–∏:** TensorRT, ONNX –∫–æ–Ω–≤–µ—Ä—Å–∏—è  
- **üéØ –ù–æ–≤—ã–µ –∫–ª–∞—Å—Å—ã:** –†–∞—Å—à–∏—Ä–µ–Ω–∏–µ –¥–µ—Ç–µ–∫—Ü–∏–π
- **üì± –ú–æ–±–∏–ª—å–Ω–∞—è –≤–µ—Ä—Å–∏—è:** YOLOv11n ‚Üí mobile deployment
"""
    
    def _generate_project_structure(self) -> str:
        """–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞"""
        return f"""
### üìÇ –û—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è –ø—Ä–æ–µ–∫—Ç–∞

```
restaurant-object-detection/
‚îú‚îÄ‚îÄ üìÅ config/
‚îÇ   ‚îú‚îÄ‚îÄ pipeline_config.json       # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø–∞–π–ø–ª–∞–π–Ω–∞
‚îÇ   ‚îî‚îÄ‚îÄ model_config.yaml         # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏
‚îú‚îÄ‚îÄ üìÅ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ fix_annotations.py        # –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π  
‚îÇ   ‚îú‚îÄ‚îÄ train_model.py            # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
‚îÇ   ‚îú‚îÄ‚îÄ run_inference.py          # –ò–Ω—Ñ–µ—Ä–µ–Ω—Å
‚îÇ   ‚îî‚îÄ‚îÄ generate_final_report.py  # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–æ–≤
‚îú‚îÄ‚îÄ üìÅ data/processed/dataset/
‚îÇ   ‚îú‚îÄ‚îÄ train/images & labels/    # –¢—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
‚îÇ   ‚îú‚îÄ‚îÄ val/images & labels/      # –í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ  
‚îÇ   ‚îú‚îÄ‚îÄ test/images & labels/     # –¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
‚îÇ   ‚îî‚îÄ‚îÄ dataset.yaml             # YOLO –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
‚îú‚îÄ‚îÄ üìÅ outputs/
‚îÇ   ‚îú‚îÄ‚îÄ experiments/             # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—É—á–µ–Ω–∏—è
‚îÇ   ‚îú‚îÄ‚îÄ inference/              # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞
‚îÇ   ‚îî‚îÄ‚îÄ final_submission/       # –§–∏–Ω–∞–ª—å–Ω—ã–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã
‚îî‚îÄ‚îÄ üìÑ README.md                # –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –ø—Ä–æ–µ–∫—Ç–∞
```

### üìä –ö–ª—é—á–µ–≤—ã–µ —Ñ–∞–π–ª—ã

| –§–∞–π–ª | –û–ø–∏—Å–∞–Ω–∏–µ | –°—Ç–∞—Ç—É—Å |
|------|----------|---------|
| `best.pt` | –û–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å | ‚úÖ –ì–æ—Ç–æ–≤–∞ |
| `dataset.yaml` | –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö | ‚úÖ –ù–∞—Å—Ç—Ä–æ–µ–Ω–∞ |
| `results.csv` | –ú–µ—Ç—Ä–∏–∫–∏ –æ–±—É—á–µ–Ω–∏—è | ‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω—ã |
| `annotation_fix_report.json` | –û—Ç—á–µ—Ç –æ–± –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è—Ö | ‚úÖ –°–æ–∑–¥–∞–Ω |
"""
    
    def _generate_reproduction_guide(self) -> str:
        """–†—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –ø–æ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—é"""
        model_path = self.report_data.get('model', {}).get('model_path', '')
        
        return f"""
### üîÑ –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –ø–æ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—é

**1. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –æ–∫—Ä—É–∂–µ–Ω–∏—è:**
```bash
pip install ultralytics torch opencv-python pandas pyyaml
```

**2. –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∞–Ω–Ω—ã—Ö:**
```bash
# –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –¥–∞—Ç–∞—Å–µ—Ç –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–µ:
data/processed/dataset/
‚îú‚îÄ‚îÄ train/images/ & train/labels/
‚îú‚îÄ‚îÄ val/images/ & val/labels/  
‚îî‚îÄ‚îÄ test/images/ & test/labels/
```

**3. –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏:**
```bash
python scripts/train_model.py --data data/processed/dataset/dataset.yaml
```

**4. –ò–Ω—Ñ–µ—Ä–µ–Ω—Å –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö:**
```bash
python scripts/run_inference.py \\
  --model "{model_path}" \\
  --input-dir "data/processed/dataset/test/images"
```

**5. –ò–Ω—Ñ–µ—Ä–µ–Ω—Å –Ω–∞ –≤–∏–¥–µ–æ:**
```bash
python scripts/run_inference.py \\
  --model "{model_path}" \\
  --video "path/to/video.mp4" \\
  --output "outputs/video_results"
```

**6. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞:**
```bash
python scripts/generate_final_report.py \\
  --model-path "{model_path}" \\
  --dataset-dir "data/processed/dataset" \\
  --experiment-dir "outputs/experiments/yolo_restaurant_detection_*" \\
  --output "final_report.md"
```

### ‚öôÔ∏è –û—Å–Ω–æ–≤–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã

```yaml
# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
model_size: "n"          # nano –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
input_size: 640          # —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π —Ä–∞–∑–º–µ—Ä
confidence: 0.25         # –ø–æ—Ä–æ–≥ –¥–µ—Ç–µ–∫—Ü–∏–∏
iou_threshold: 0.45      # NMS –ø–æ—Ä–æ–≥

# –û–±—É—á–µ–Ω–∏–µ  
epochs: 100              # –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö
batch_size: 16           # —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞
learning_rate: 0.01      # –Ω–∞—á–∞–ª—å–Ω–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è
patience: 15             # early stopping
```

### üéØ –û–∂–∏–¥–∞–µ–º—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã

- **mAP@0.5:** ~79.7% (¬±2%)
- **–í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è:** ~17-20 –º–∏–Ω—É—Ç –Ω–∞ GPU
- **–†–∞–∑–º–µ—Ä –º–æ–¥–µ–ª–∏:** ~5-6 MB  
- **–°–∫–æ—Ä–æ—Å—Ç—å –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞:** ~2ms –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ

### üìû –ü–æ–¥–¥–µ—Ä–∂–∫–∞

–ü—Ä–∏ –≤–æ–∑–Ω–∏–∫–Ω–æ–≤–µ–Ω–∏–∏ –≤–æ–ø—Ä–æ—Å–æ–≤:
1. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–∞–Ω–Ω—ã—Ö
2. –£–±–µ–¥–∏—Ç–µ—Å—å –≤ –Ω–∞–ª–∏—á–∏–∏ GPU –¥—Ä–∞–π–≤–µ—Ä–æ–≤
3. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –≤–µ—Ä—Å–∏–∏ –±–∏–±–ª–∏–æ—Ç–µ–∫
4. –û–±—Ä–∞—Ç–∏—Ç–µ—Å—å –∫ –ª–æ–≥–∞–º –≤ `outputs/logs/`

---

## üèÜ –ó–∞–∫–ª—é—á–µ–Ω–∏–µ

–ü—Ä–æ–µ–∫—Ç **—É—Å–ø–µ—à–Ω–æ –≤—ã–ø–æ–ª–Ω–µ–Ω** —Å –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–µ–º –æ—Ç–ª–∏—á–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤:

- ‚úÖ **–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è** —Ä–µ—à–∏–ª–∞ –ø—Ä–æ–±–ª–µ–º—É —Ä–∞–∑–º–µ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö
- ‚úÖ **–í—ã—Å–æ–∫–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏** ({self.report_data.get('training', {}).get('best_map50', 0):.1%}) –≥–æ—Ç–æ–≤–∞ –¥–ª—è production
- ‚úÖ **–ë—ã—Å—Ç—Ä—ã–π –∏–Ω—Ñ–µ—Ä–µ–Ω—Å** –ø–æ–∑–≤–æ–ª—è–µ—Ç real-time –æ–±—Ä–∞–±–æ—Ç–∫—É
- ‚úÖ **Comprehensive —Ä–µ—à–µ–Ω–∏–µ** –≤–∫–ª—é—á–∞–µ—Ç –≤—Å–µ —ç—Ç–∞–ø—ã ML pipeline

**–°–∏—Å—Ç–µ–º–∞ –≥–æ—Ç–æ–≤–∞ –∫ –≤–Ω–µ–¥—Ä–µ–Ω–∏—é –≤ —Ä–µ–∞–ª—å–Ω—ã–µ —Ä–µ—Å—Ç–æ—Ä–∞–Ω–Ω—ã–µ –ø—Ä–æ—Ü–µ—Å—Å—ã!** üöÄ

---

*–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–∏—Å—Ç–µ–º–æ–π –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–π –∞–Ω–∞–ª–∏—Ç–∏–∫–∏ ML –ø—Ä–æ–µ–∫—Ç–æ–≤*  
*–í—Ä–µ–º—è —Å–æ–∑–¥–∞–Ω–∏—è –æ—Ç—á–µ—Ç–∞: {self.report_data['timestamp']}*
"""


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    parser = argparse.ArgumentParser(
        description="–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –ø–æ—Ç—Ä—è—Å–∞—é—â–µ–≥–æ –æ—Ç—á–µ—Ç–∞ –≤ Markdown —Ñ–æ—Ä–º–∞—Ç–µ",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
–ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
    # –ü–æ–ª–Ω—ã–π –æ—Ç—á–µ—Ç
    python scripts/generate_final_report.py \\
        --model-path "outputs/experiments/yolo_*/weights/best.pt" \\
        --dataset-dir "data/processed/dataset" \\
        --experiment-dir "outputs/experiments/yolo_*" \\
        --output "final_report.md"
    
    # –° —É–∫–∞–∑–∞–Ω–∏–µ–º –≤—Ä–µ–º–µ–Ω–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –ø—Ä–æ–µ–∫—Ç–∞
    python scripts/generate_final_report.py \\
        --model-path "outputs/experiments/yolo_*/weights/best.pt" \\
        --dataset-dir "data/processed/dataset" \\
        --experiment-dir "outputs/experiments/yolo_*" \\
        --output "final_report.md" \\
        --project-time 8.5
        """
    )
    
    parser.add_argument(
        "--model-path",
        type=str,
        required=True,
        help="–ü—É—Ç—å –∫ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ (.pt —Ñ–∞–π–ª)"
    )
    
    parser.add_argument(
        "--dataset-dir", 
        type=str,
        required=True,
        help="–ü—É—Ç—å –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞"
    )
    
    parser.add_argument(
        "--experiment-dir",
        type=str, 
        required=True,
        help="–ü—É—Ç—å –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞"
    )
    
    parser.add_argument(
        "--output",
        type=str,
        default="final_report.md",
        help="–ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –æ—Ç—á–µ—Ç–∞"
    )
    
    parser.add_argument(
        "--project-time",
        type=float,
        default=None,
        help="–í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –ø—Ä–æ–µ–∫—Ç–∞ –≤ —á–∞—Å–∞—Ö"
    )
    
    args = parser.parse_args()
    
    try:
        # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –ø—É—Ç–µ–π
        model_path = Path(args.model_path)
        dataset_dir = Path(args.dataset_dir)  
        experiment_dir = Path(args.experiment_dir)
        output_path = Path(args.output)
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è —Ñ–∞–π–ª–æ–≤
        if not model_path.exists():
            print(f"‚ùå –ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {model_path}")
            return 1
            
        if not dataset_dir.exists():
            print(f"‚ùå –î–∞—Ç–∞—Å–µ—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω: {dataset_dir}")
            return 1
            
        if not experiment_dir.exists():
            print(f"‚ùå –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {experiment_dir}")
            return 1
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞
        generator = AwesomeReportGenerator()
        
        print("üöÄ –ù–∞—á–∏–Ω–∞–µ–º –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –ø–æ—Ç—Ä—è—Å–∞—é—â–µ–≥–æ –æ—Ç—á–µ—Ç–∞...")
        
        report_path = generator.generate_complete_report(
            model_path=model_path,
            dataset_dir=dataset_dir,
            experiment_dir=experiment_dir, 
            output_path=output_path,
            project_time_hours=args.project_time
        )
        
        print(f"\nüéâ –ü–æ—Ç—Ä—è—Å–∞—é—â–∏–π –æ—Ç—á–µ—Ç —Å–æ–∑–¥–∞–Ω!")
        print(f"üìÑ –§–∞–π–ª: {report_path}")
        print(f"üìä –†–∞–∑–º–µ—Ä: {report_path.stat().st_size / 1024:.1f} KB")
        print(f"\nüìã –û—Ç—á–µ—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç:")
        print(f"  ‚úÖ –ö—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ –ø—Ä–æ–µ–∫—Ç–∞")
        print(f"  ‚úÖ –ê–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö –∏ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π") 
        print(f"  ‚úÖ –î–µ—Ç–∞–ª–∏ –ø—Ä–æ—Ü–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è")
        print(f"  ‚úÖ –ú–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏")
        print(f"  ‚úÖ –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –¥–µ—Ç–∞–ª–∏ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏")
        print(f"  ‚úÖ –í—ã–≤–æ–¥—ã –∏ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è")
        print(f"  ‚úÖ –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –ø–æ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—é")
        print(f"\nüöÄ –ì–æ—Ç–æ–≤–æ –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏!")
        
        return 0
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç—á–µ—Ç–∞: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    exit(main())