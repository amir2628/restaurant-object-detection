"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ YOLOv11
–í—ã—á–∏—Å–ª—è–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –∏ —Å–æ–∑–¥–∞–µ—Ç –¥–µ—Ç–∞–ª—å–Ω—ã–µ –æ—Ç—á–µ—Ç—ã
"""
import argparse
import sys
from pathlib import Path
from typing import Optional, List, Dict, Any

# –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∫–æ—Ä–Ω–µ–≤–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –≤ –ø—É—Ç—å
sys.path.append(str(Path(__file__).parent.parent))

from src.utils.logger import setup_logging, get_logger
from src.models.inference import YOLOInference
from src.utils.metrics import MetricsCalculator, COCOMetrics, evaluate_yolo_model
from src.utils.visualization import ReportGenerator
from ultralytics import YOLO
import json

def load_test_dataset_info(dataset_yaml: Path) -> Dict[str, Any]:
    """
    –ó–∞–≥—Ä—É–∑–∫–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Ç–µ—Å—Ç–æ–≤–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ
    
    Args:
        dataset_yaml: –ü—É—Ç—å –∫ YAML –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞
        
    Returns:
        –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –¥–∞—Ç–∞—Å–µ—Ç–µ
    """
    import yaml
    
    logger = get_logger(__name__)
    
    if not dataset_yaml.exists():
        raise FileNotFoundError(f"–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {dataset_yaml}")
    
    with open(dataset_yaml, 'r', encoding='utf-8') as f:
        dataset_config = yaml.safe_load(f)
    
    logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞: {dataset_yaml}")
    logger.info(f"–ö–ª–∞—Å—Å–æ–≤: {dataset_config.get('nc', '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')}")
    
    return dataset_config

def run_official_evaluation(model_path: Path, 
                          dataset_yaml: Path,
                          output_dir: Path,
                          confidence_threshold: float = 0.001,
                          iou_threshold: float = 0.6) -> Dict[str, Any]:
    """
    –ó–∞–ø—É—Å–∫ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ YOLO
    
    Args:
        model_path: –ü—É—Ç—å –∫ –º–æ–¥–µ–ª–∏
        dataset_yaml: –ü—É—Ç—å –∫ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞
        output_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        confidence_threshold: –ü–æ—Ä–æ–≥ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
        iou_threshold: –ü–æ—Ä–æ–≥ IoU
        
    Returns:
        –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ü–µ–Ω–∫–∏
    """
    logger = get_logger(__name__)
    
    logger.info("–ó–∞–ø—É—Å–∫ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ YOLO...")
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
    model = YOLO(str(model_path))
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    eval_dir = output_dir / "official_evaluation"
    eval_dir.mkdir(parents=True, exist_ok=True)
    
    # –ó–∞–ø—É—Å–∫ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
    results = model.val(
        data=str(dataset_yaml),
        conf=confidence_threshold,
        iou=iou_threshold,
        save_json=True,
        save_hybrid=False,
        plots=True,
        verbose=True,
        project=str(eval_dir),
        name="validation"
    )
    
    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
    metrics = {}
    if hasattr(results, 'results_dict'):
        metrics = results.results_dict
    
    # –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    evaluation_results = {
        "model_path": str(model_path),
        "dataset_config": str(dataset_yaml),
        "evaluation_config": {
            "confidence_threshold": confidence_threshold,
            "iou_threshold": iou_threshold
        },
        "metrics": metrics,
        "output_directory": str(eval_dir)
    }
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    results_file = eval_dir / "evaluation_results.json"
    with open(results_file, 'w', encoding='utf-8') as f:
        json.dump(evaluation_results, f, ensure_ascii=False, indent=2, default=str)
    
    logger.info(f"–û—Ñ–∏—Ü–∏–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤: {eval_dir}")
    
    return evaluation_results

def run_custom_evaluation(model_path: Path,
                        test_images_dir: Path,
                        test_annotations_dir: Path,
                        class_mapping: Dict[int, str],
                        output_dir: Path,
                        confidence_threshold: float = 0.25,
                        iou_threshold: float = 0.5) -> Dict[str, Any]:
    """
    –ó–∞–ø—É—Å–∫ –∫–∞—Å—Ç–æ–º–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ —Å –¥–µ—Ç–∞–ª—å–Ω—ã–º –∞–Ω–∞–ª–∏–∑–æ–º
    
    Args:
        model_path: –ü—É—Ç—å –∫ –º–æ–¥–µ–ª–∏
        test_images_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å —Ç–µ—Å—Ç–æ–≤—ã–º–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏
        test_annotations_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è–º–∏
        class_mapping: –ú–∞–ø–ø–∏–Ω–≥ –∫–ª–∞—Å—Å–æ–≤
        output_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        confidence_threshold: –ü–æ—Ä–æ–≥ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
        iou_threshold: –ü–æ—Ä–æ–≥ IoU
        
    Returns:
        –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∫–∞—Å—Ç–æ–º–Ω–æ–π –æ—Ü–µ–Ω–∫–∏
    """
    logger = get_logger(__name__)
    
    logger.info("–ó–∞–ø—É—Å–∫ –∫–∞—Å—Ç–æ–º–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ —Å –¥–µ—Ç–∞–ª—å–Ω—ã–º –∞–Ω–∞–ª–∏–∑–æ–º...")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    custom_eval_dir = output_dir / "custom_evaluation"
    custom_eval_dir.mkdir(parents=True, exist_ok=True)
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞
    inference = YOLOInference(model_path, confidence_threshold, iou_threshold)
    
    # –ü–æ–∏—Å–∫ —Ç–µ—Å—Ç–æ–≤—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
    image_files = []
    for ext in ['.jpg', '.jpeg', '.png', '.bmp']:
        image_files.extend(test_images_dir.glob(f"*{ext}"))
        image_files.extend(test_images_dir.glob(f"*{ext.upper()}"))
    
    if not image_files:
        raise ValueError(f"–¢–µ—Å—Ç–æ–≤—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤: {test_images_dir}")
    
    logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(image_files)} —Ç–µ—Å—Ç–æ–≤—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–∞–ª—å–∫—É–ª—è—Ç–æ—Ä–∞ –º–µ—Ç—Ä–∏–∫
    class_names = list(class_mapping.values())
    metrics_calc = MetricsCalculator(len(class_names), class_names)
    coco_metrics = COCOMetrics(len(class_names), class_names)
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ ground truth
    from src.utils.metrics import load_ground_truth_from_yolo
    ground_truths = load_ground_truth_from_yolo(test_annotations_dir, class_mapping)
    
    logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(ground_truths)} –∏—Å—Ç–∏–Ω–Ω—ã—Ö –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π")
    
    # –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –∏ —Å–±–æ—Ä —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    logger.info("–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö...")
    
    from src.utils.metrics import DetectionResult
    detections = []
    
    inference_results = inference.predict_batch(image_files)
    
    for result in inference_results:
        image_id = Path(result.image_path).stem
        
        for detection in result.detections:
            det_result = DetectionResult(
                class_id=detection.class_id,
                confidence=detection.confidence,
                bbox=detection.bbox,
                image_id=image_id
            )
            detections.append(det_result)
    
    logger.info(f"–ü–æ–ª—É—á–µ–Ω–æ {len(detections)} –¥–µ—Ç–µ–∫—Ü–∏–π")
    
    # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤ –∫–∞–ª—å–∫—É–ª—è—Ç–æ—Ä—ã –º–µ—Ç—Ä–∏–∫
    metrics_calc.add_detections(detections)
    metrics_calc.add_ground_truths(ground_truths)
    
    coco_metrics.add_detections(detections)
    coco_metrics.add_ground_truths(ground_truths)
    
    # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
    logger.info("–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫...")
    
    # –û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    overall_report = metrics_calc.generate_metrics_report(confidence_threshold, iou_threshold)
    
    # COCO –º–µ—Ç—Ä–∏–∫–∏
    coco_results = coco_metrics.calculate_coco_map()
    
    # –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤
    logger.info("–°–æ–∑–¥–∞–Ω–∏–µ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–π...")
    
    # PR –∫—Ä–∏–≤—ã–µ
    pr_curves_dir = custom_eval_dir / "pr_curves"
    metrics_calc.plot_pr_curves(pr_curves_dir, iou_threshold)
    
    # –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫
    from src.utils.visualization import MetricsVisualizer
    vis = MetricsVisualizer()
    
    confusion_matrix = metrics_calc.calculate_confusion_matrix(confidence_threshold, iou_threshold)
    class_names_with_bg = class_names + ["background"]
    
    vis.plot_confusion_matrix(
        confusion_matrix,
        class_names_with_bg,
        output_path=custom_eval_dir / "confusion_matrix.png"
    )
    
    # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤
    class_counts = overall_report["data_statistics"]["class_distribution"]
    vis.plot_class_distribution(
        class_counts,
        output_path=custom_eval_dir / "class_distribution.png"
    )
    
    # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    custom_results = {
        "model_path": str(model_path),
        "test_images_directory": str(test_images_dir),
        "test_annotations_directory": str(test_annotations_dir),
        "evaluation_config": {
            "confidence_threshold": confidence_threshold,
            "iou_threshold": iou_threshold,
            "class_mapping": class_mapping
        },
        "overall_metrics": overall_report,
        "coco_metrics": coco_results,
        "performance_stats": inference.get_performance_stats(),
        "visualizations": {
            "pr_curves": str(pr_curves_dir),
            "confusion_matrix": str(custom_eval_dir / "confusion_matrix.png"),
            "class_distribution": str(custom_eval_dir / "class_distribution.png")
        }
    }
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    results_file = custom_eval_dir / "custom_evaluation_results.json"
    metrics_calc.save_metrics_report(overall_report, results_file)
    
    detailed_results_file = custom_eval_dir / "detailed_results.json"
    with open(detailed_results_file, 'w', encoding='utf-8') as f:
        json.dump(custom_results, f, ensure_ascii=False, indent=2, default=str)
    
    logger.info(f"–ö–∞—Å—Ç–æ–º–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤: {custom_eval_dir}")
    
    return custom_results

def generate_comparison_report(evaluations: List[Dict[str, Any]], 
                             output_dir: Path) -> Path:
    """
    –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞
    
    Args:
        evaluations: –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ—Ü–µ–Ω–∫–∏
        output_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è –æ—Ç—á–µ—Ç–∞
        
    Returns:
        –ü—É—Ç—å –∫ —Å—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω–æ–º—É –æ—Ç—á–µ—Ç—É
    """
    logger = get_logger(__name__)
    
    logger.info("–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞...")
    
    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö –º–µ—Ç—Ä–∏–∫
    comparison_data = []
    
    for eval_result in evaluations:
        eval_type = "official" if "official" in str(eval_result.get("output_directory", "")) else "custom"
        
        if eval_type == "official":
            metrics = eval_result.get("metrics", {})
            data = {
                "type": "Official YOLO",
                "map50": metrics.get("metrics/mAP50(B)", 0),
                "map50_95": metrics.get("metrics/mAP50-95(B)", 0),
                "precision": metrics.get("metrics/precision(B)", 0),
                "recall": metrics.get("metrics/recall(B)", 0),
                "model_path": eval_result.get("model_path", "")
            }
        else:
            overall_metrics = eval_result.get("overall_metrics", {}).get("overall_metrics", {})
            map_metrics = eval_result.get("overall_metrics", {}).get("map_metrics", {})
            
            data = {
                "type": "Custom Analysis",
                "map50": map_metrics.get("mAP@0.50", 0),
                "map50_95": map_metrics.get("mAP@0.5:0.95", 0),
                "precision": overall_metrics.get("precision", 0),
                "recall": overall_metrics.get("recall", 0),
                "f1_score": overall_metrics.get("f1_score", 0),
                "model_path": eval_result.get("model_path", "")
            }
        
        comparison_data.append(data)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ HTML –æ—Ç—á–µ—Ç–∞
    report_content = f"""
    <!DOCTYPE html>
    <html>
    <head>
        <title>Model Evaluation Comparison Report</title>
        <style>
            body {{ font-family: Arial, sans-serif; margin: 40px; }}
            .header {{ text-align: center; margin-bottom: 30px; }}
            .section {{ margin-bottom: 30px; }}
            .metrics-table {{ border-collapse: collapse; width: 100%; }}
            .metrics-table th, .metrics-table td {{ border: 1px solid #ddd; padding: 12px; text-align: left; }}
            .metrics-table th {{ background-color: #f2f2f2; font-weight: bold; }}
            .best-score {{ background-color: #d4edda; font-weight: bold; }}
            .summary {{ background-color: #f8f9fa; padding: 20px; border-radius: 5px; }}
        </style>
    </head>
    <body>
        <div class="header">
            <h1>üéØ Model Evaluation Comparison Report</h1>
            <p>Generated: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
        </div>
        
        <div class="section">
            <h2>üìä Metrics Comparison</h2>
            <table class="metrics-table">
                <tr>
                    <th>Evaluation Type</th>
                    <th>mAP@0.5</th>
                    <th>mAP@0.5:0.95</th>
                    <th>Precision</th>
                    <th>Recall</th>
                    <th>F1-Score</th>
                    <th>Model</th>
                </tr>
    """
    
    # –ü–æ–∏—Å–∫ –ª—É—á—à–∏—Ö –∑–Ω–∞—á–µ–Ω–∏–π –¥–ª—è –≤—ã–¥–µ–ª–µ–Ω–∏—è
    if comparison_data:
        best_map50 = max(data.get("map50", 0) for data in comparison_data)
        best_map50_95 = max(data.get("map50_95", 0) for data in comparison_data)
        best_precision = max(data.get("precision", 0) for data in comparison_data)
        best_recall = max(data.get("recall", 0) for data in comparison_data)
        best_f1 = max(data.get("f1_score", 0) for data in comparison_data)
        
        for data in comparison_data:
            map50_class = "best-score" if data.get("map50", 0) == best_map50 else ""
            map50_95_class = "best-score" if data.get("map50_95", 0) == best_map50_95 else ""
            precision_class = "best-score" if data.get("precision", 0) == best_precision else ""
            recall_class = "best-score" if data.get("recall", 0) == best_recall else ""
            f1_class = "best-score" if data.get("f1_score", 0) == best_f1 else ""
            
            model_name = Path(data["model_path"]).name if data["model_path"] else "Unknown"
            
            report_content += f"""
                <tr>
                    <td>{data["type"]}</td>
                    <td class="{map50_class}">{data.get("map50", 0):.4f}</td>
                    <td class="{map50_95_class}">{data.get("map50_95", 0):.4f}</td>
                    <td class="{precision_class}">{data.get("precision", 0):.4f}</td>
                    <td class="{recall_class}">{data.get("recall", 0):.4f}</td>
                    <td class="{f1_class}">{data.get("f1_score", 0):.4f}</td>
                    <td>{model_name}</td>
                </tr>
            """
    
    # –†–µ–∑—é–º–µ
    summary_text = ""
    if comparison_data:
        official_data = [d for d in comparison_data if d["type"] == "Official YOLO"]
        custom_data = [d for d in comparison_data if d["type"] == "Custom Analysis"]
        
        if official_data and custom_data:
            official_map50 = official_data[0].get("map50", 0)
            custom_map50 = custom_data[0].get("map50", 0)
            
            if abs(official_map50 - custom_map50) < 0.01:
                summary_text = "‚úÖ –û—Ç–ª–∏—á–Ω–∞—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –º–µ–∂–¥—É –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ–π –∏ –∫–∞—Å—Ç–æ–º–Ω–æ–π –æ—Ü–µ–Ω–∫–æ–π"
            elif abs(official_map50 - custom_map50) < 0.05:
                summary_text = "‚ö†Ô∏è –ù–µ–±–æ–ª—å—à–∏–µ —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏—è –º–µ–∂–¥—É –º–µ—Ç–æ–¥–∞–º–∏ –æ—Ü–µ–Ω–∫–∏"
            else:
                summary_text = "‚ùå –ó–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏—è —Ç—Ä–µ–±—É—é—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞"
    
    report_content += f"""
            </table>
        </div>
        
        <div class="section">
            <h2>üìã Summary</h2>
            <div class="summary">
                <p><strong>Evaluation Summary:</strong> {summary_text}</p>
                <p><strong>Total Evaluations:</strong> {len(comparison_data)}</p>
                <p><strong>Best Overall mAP@0.5:</strong> {best_map50:.4f}</p>
                <p><strong>Best Overall mAP@0.5:0.95:</strong> {best_map50_95:.4f}</p>
            </div>
        </div>
        
        <div class="section">
            <h2>üîç Recommendations</h2>
            <ul>
    """
    
    # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    if comparison_data:
        avg_map50 = sum(d.get("map50", 0) for d in comparison_data) / len(comparison_data)
        
        if avg_map50 > 0.7:
            report_content += "<li>‚úÖ –û—Ç–ª–∏—á–Ω–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ (mAP@0.5 > 70%)</li>"
        elif avg_map50 > 0.5:
            report_content += "<li>‚ö†Ô∏è –•–æ—Ä–æ—à–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å, –Ω–æ –µ—Å—Ç—å –º–µ—Å—Ç–æ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è</li>"
        else:
            report_content += "<li>‚ùå –¢—Ä–µ–±—É–µ—Ç—Å—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –º–æ–¥–µ–ª–∏</li>"
        
        if best_precision > 0.8:
            report_content += "<li>‚úÖ –í—ã—Å–æ–∫–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –¥–µ—Ç–µ–∫—Ü–∏–π</li>"
        else:
            report_content += "<li>‚ö†Ô∏è –†–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ –ø–æ—Ä–æ–≥–∞ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —Ç–æ—á–Ω–æ—Å—Ç–∏</li>"
        
        if best_recall > 0.8:
            report_content += "<li>‚úÖ –•–æ—Ä–æ—à–µ–µ –ø–æ–∫—Ä—ã—Ç–∏–µ –æ–±—ä–µ–∫—Ç–æ–≤</li>"
        else:
            report_content += "<li>‚ö†Ô∏è –†–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ —Å–Ω–∏–∂–µ–Ω–∏–µ –ø–æ—Ä–æ–≥–∞ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –∏–ª–∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—É—é –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—é</li>"
    
    report_content += """
            </ul>
        </div>
    </body>
    </html>
    """
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–∞
    report_path = output_dir / "evaluation_comparison_report.html"
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write(report_content)
    
    logger.info(f"–°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç —Å–æ–∑–¥–∞–Ω: {report_path}")
    return report_path

def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Å–∫—Ä–∏–ø—Ç–∞"""
    parser = argparse.ArgumentParser(
        description="–û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ YOLOv11 –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏ –æ–±—ä–µ–∫—Ç–æ–≤",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
–ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
  # –ë–∞–∑–æ–≤–∞—è –æ—Ü–µ–Ω–∫–∞
  python scripts/evaluate_model.py --model models/trained/best.pt --dataset data/dataset/dataset.yaml

  # –ö–∞—Å—Ç–æ–º–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ —Å –¥–µ—Ç–∞–ª—å–Ω—ã–º –∞–Ω–∞–ª–∏–∑–æ–º
  python scripts/evaluate_model.py --model models/trained/best.pt --test-images data/test/images --test-annotations data/test/labels

  # –ü–æ–ª–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ (–æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–∞—è + –∫–∞—Å—Ç–æ–º–Ω–∞—è)
  python scripts/evaluate_model.py --model models/trained/best.pt --dataset data/dataset/dataset.yaml --test-images data/test/images --test-annotations data/test/labels --full-evaluation

  # –û—Ü–µ–Ω–∫–∞ —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–æ–π –ø–æ—Ä–æ–≥–æ–≤
  python scripts/evaluate_model.py --model models/trained/best.pt --dataset data/dataset/dataset.yaml --confidence 0.1 --iou 0.5
        """
    )
    
    # –û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –∞—Ä–≥—É–º–µ–Ω—Ç—ã
    parser.add_argument(
        '--model',
        type=str,
        required=True,
        help='–ü—É—Ç—å –∫ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ (.pt —Ñ–∞–π–ª)'
    )
    
    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞
    parser.add_argument(
        '--dataset',
        type=str,
        help='–ü—É—Ç—å –∫ YAML –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞'
    )
    
    # –ö–∞—Å—Ç–æ–º–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
    parser.add_argument(
        '--test-images',
        type=str,
        help='–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å —Ç–µ—Å—Ç–æ–≤—ã–º–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏ –¥–ª—è –∫–∞—Å—Ç–æ–º–Ω–æ–π –æ—Ü–µ–Ω–∫–∏'
    )
    
    parser.add_argument(
        '--test-annotations',
        type=str,
        help='–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å —Ç–µ—Å—Ç–æ–≤—ã–º–∏ –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è–º–∏ –¥–ª—è –∫–∞—Å—Ç–æ–º–Ω–æ–π –æ—Ü–µ–Ω–∫–∏'
    )
    
    parser.add_argument(
        '--class-mapping',
        type=str,
        help='–ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –º–∞–ø–ø–∏–Ω–≥–∞ –∫–ª–∞—Å—Å–æ–≤ (JSON)'
    )
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ—Ü–µ–Ω–∫–∏
    parser.add_argument(
        '--confidence',
        type=float,
        default=0.001,
        help='–ü–æ—Ä–æ–≥ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –¥–ª—è –æ—Ü–µ–Ω–∫–∏'
    )
    
    parser.add_argument(
        '--iou',
        type=float,
        default=0.6,
        help='–ü–æ—Ä–æ–≥ IoU –¥–ª—è NMS'
    )
    
    # –û–ø—Ü–∏–∏ –≤—ã–≤–æ–¥–∞
    parser.add_argument(
        '--output-dir',
        type=str,
        default='outputs/evaluation',
        help='–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤'
    )
    
    parser.add_argument(
        '--full-evaluation',
        action='store_true',
        help='–í—ã–ø–æ–ª–Ω–∏—Ç—å –ø–æ–ª–Ω—É—é –æ—Ü–µ–Ω–∫—É (–æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–∞—è + –∫–∞—Å—Ç–æ–º–Ω–∞—è)'
    )
    
    parser.add_argument(
        '--generate-report',
        action='store_true',
        default=True,
        help='–ì–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å HTML –æ—Ç—á–µ—Ç'
    )
    
    # –°–∏—Å—Ç–µ–º–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    parser.add_argument(
        '--log-level',
        type=str,
        default='INFO',
        choices=['DEBUG', 'INFO', 'WARNING', 'ERROR'],
        help='–£—Ä–æ–≤–µ–Ω—å –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è'
    )
    
    args = parser.parse_args()
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
    setup_logging(log_level=args.log_level)
    logger = get_logger(__name__)
    
    logger.info("üéØ –ó–∞–ø—É—Å–∫ —Å–∫—Ä–∏–ø—Ç–∞ –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–∏ YOLOv11")
    logger.info(f"–ê—Ä–≥—É–º–µ–Ω—Ç—ã: {vars(args)}")
    
    try:
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –º–æ–¥–µ–ª–∏
        model_path = Path(args.model)
        if not model_path.exists():
            logger.error(f"–ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {model_path}")
            sys.exit(1)
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –≤—ã—Ö–æ–¥–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        output_dir = Path(args.output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
        
        evaluations = []
        
        # –û—Ñ–∏—Ü–∏–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ YOLO
        if args.dataset:
            dataset_yaml = Path(args.dataset)
            if not dataset_yaml.exists():
                logger.error(f"–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {dataset_yaml}")
                sys.exit(1)
            
            logger.info("üîÑ –ó–∞–ø—É—Å–∫ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ YOLO...")
            
            official_results = run_official_evaluation(
                model_path=model_path,
                dataset_yaml=dataset_yaml,
                output_dir=output_dir,
                confidence_threshold=args.confidence,
                iou_threshold=args.iou
            )
            
            evaluations.append(official_results)
            
            # –í—ã–≤–æ–¥ –æ—Å–Ω–æ–≤–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫
            metrics = official_results.get("metrics", {})
            logger.info("üìä –û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ (–æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞):")
            logger.info(f"  mAP@0.5: {metrics.get('metrics/mAP50(B)', 0):.4f}")
            logger.info(f"  mAP@0.5:0.95: {metrics.get('metrics/mAP50-95(B)', 0):.4f}")
            logger.info(f"  Precision: {metrics.get('metrics/precision(B)', 0):.4f}")
            logger.info(f"  Recall: {metrics.get('metrics/recall(B)', 0):.4f}")
        
        # –ö–∞—Å—Ç–æ–º–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
        if args.test_images and args.test_annotations:
            logger.info("üîÑ –ó–∞–ø—É—Å–∫ –∫–∞—Å—Ç–æ–º–Ω–æ–π –æ—Ü–µ–Ω–∫–∏...")
            
            test_images_dir = Path(args.test_images)
            test_annotations_dir = Path(args.test_annotations)
            
            if not test_images_dir.exists():
                logger.error(f"–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Ç–µ—Å—Ç–æ–≤—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {test_images_dir}")
                sys.exit(1)
            
            if not test_annotations_dir.exists():
                logger.error(f"–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Ç–µ—Å—Ç–æ–≤—ã—Ö –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {test_annotations_dir}")
                sys.exit(1)
            
            # –ó–∞–≥—Ä—É–∑–∫–∞ –º–∞–ø–ø–∏–Ω–≥–∞ –∫–ª–∞—Å—Å–æ–≤
            class_mapping = {}
            if args.class_mapping:
                class_mapping_file = Path(args.class_mapping)
                if class_mapping_file.exists():
                    with open(class_mapping_file, 'r', encoding='utf-8') as f:
                        mapping_data = json.load(f)
                        class_mapping = {int(k): v for k, v in mapping_data.get('reverse_mapping', {}).items()}
                else:
                    logger.warning(f"–§–∞–π–ª –º–∞–ø–ø–∏–Ω–≥–∞ –∫–ª–∞—Å—Å–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω: {class_mapping_file}")
            
            # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ –º–∞–ø–ø–∏–Ω–≥–∞, –µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω
            if not class_mapping:
                logger.info("–°–æ–∑–¥–∞–Ω–∏–µ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –º–∞–ø–ø–∏–Ω–≥–∞ –∫–ª–∞—Å—Å–æ–≤...")
                from config.config import config
                class_mapping = {i: name for i, name in enumerate(config.annotation.target_classes)}
            
            custom_results = run_custom_evaluation(
                model_path=model_path,
                test_images_dir=test_images_dir,
                test_annotations_dir=test_annotations_dir,
                class_mapping=class_mapping,
                output_dir=output_dir,
                confidence_threshold=args.confidence,
                iou_threshold=args.iou
            )
            
            evaluations.append(custom_results)
            
            # –í—ã–≤–æ–¥ –æ—Å–Ω–æ–≤–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫
            overall_metrics = custom_results.get("overall_metrics", {}).get("overall_metrics", {})
            map_metrics = custom_results.get("overall_metrics", {}).get("map_metrics", {})
            
            logger.info("üìä –û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ (–∫–∞—Å—Ç–æ–º–Ω–∞—è –æ—Ü–µ–Ω–∫–∞):")
            logger.info(f"  mAP@0.5: {map_metrics.get('mAP@0.50', 0):.4f}")
            logger.info(f"  mAP@0.5:0.95: {map_metrics.get('mAP@0.5:0.95', 0):.4f}")
            logger.info(f"  Precision: {overall_metrics.get('precision', 0):.4f}")
            logger.info(f"  Recall: {overall_metrics.get('recall', 0):.4f}")
            logger.info(f"  F1-Score: {overall_metrics.get('f1_score', 0):.4f}")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞, —á—Ç–æ —Ö–æ—Ç—è –±—ã –æ–¥–Ω–∞ –æ—Ü–µ–Ω–∫–∞ –±—ã–ª–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∞
        if not evaluations:
            logger.error("–ù–µ —É–∫–∞–∑–∞–Ω—ã –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –Ω–∏ –¥–ª—è –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ–π, –Ω–∏ –¥–ª—è –∫–∞—Å—Ç–æ–º–Ω–æ–π –æ—Ü–µ–Ω–∫–∏")
            logger.error("–£–∫–∞–∂–∏—Ç–µ --dataset –¥–ª—è –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ –∏–ª–∏ --test-images –∏ --test-annotations –¥–ª—è –∫–∞—Å—Ç–æ–º–Ω–æ–π")
            sys.exit(1)
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞
        if args.generate_report and len(evaluations) > 0:
            logger.info("üìù –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞...")
            
            report_path = generate_comparison_report(evaluations, output_dir)
            
            logger.info(f"‚úÖ –°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç —Å–æ–∑–¥–∞–Ω: {report_path}")
        
        # –ò—Ç–æ–≥–æ–≤–æ–µ —Ä–µ–∑—é–º–µ
        logger.info("üéâ –û–¶–ï–ù–ö–ê –ú–û–î–ï–õ–ò –ó–ê–í–ï–†–®–ï–ù–ê!")
        logger.info(f"–í—ã–ø–æ–ª–Ω–µ–Ω–æ –æ—Ü–µ–Ω–æ–∫: {len(evaluations)}")
        logger.info(f"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {output_dir}")
        
        # –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –¥–ª—è —Å–ª–µ–¥—É—é—â–∏—Ö —à–∞–≥–æ–≤
        logger.info("üìã –°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏:")
        logger.info("1. –ü—Ä–æ—Å–º–æ—Ç—Ä–∏—Ç–µ HTML –æ—Ç—á–µ—Ç –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞")
        logger.info("2. –ò–∑—É—á–∏—Ç–µ PR-–∫—Ä–∏–≤—ã–µ –∏ –º–∞—Ç—Ä–∏—Ü—É –æ—à–∏–±–æ–∫")
        logger.info("3. –ü—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –Ω–∞—Å—Ç—Ä–æ–π—Ç–µ –ø–æ—Ä–æ–≥–∏ confidence/IoU")
        logger.info("4. –†–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—É—é –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—é –¥–ª—è —Å–ª–∞–±—ã—Ö –∫–ª–∞—Å—Å–æ–≤")
        
        if args.generate_report:
            logger.info(f"5. –û—Ç–∫—Ä–æ–π—Ç–µ –æ—Ç—á–µ—Ç: {output_dir / 'evaluation_comparison_report.html'}")
        
        sys.exit(0)
        
    except KeyboardInterrupt:
        logger.info("–û—Ü–µ–Ω–∫–∞ –ø—Ä–µ—Ä–≤–∞–Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
        sys.exit(1)
    except Exception as e:
        logger.error(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞: {e}")
        import traceback
        logger.error(traceback.format_exc())
        sys.exit(1)


if __name__ == "__main__":
    import pandas as pd
    main()