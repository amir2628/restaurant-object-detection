"""
–ë—ã—Å—Ç—Ä–æ–µ —Ä–µ—à–µ–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º—ã —Å –ø—É—Å—Ç—ã–º–∏ –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è–º–∏
–°–æ–∑–¥–∞–µ—Ç –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ –¥–ª—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
"""

import os
import sys
import logging
import argparse
from pathlib import Path
from typing import List, Dict, Any, Tuple
import cv2
import numpy as np
import json
import yaml
import time
from tqdm import tqdm

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–Ω–µ–≤—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –ø—Ä–æ–µ–∫—Ç–∞ –≤ sys.path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.utils.logger import setup_logger

def check_dataset_structure(dataset_dir: Path) -> bool:
    """
    –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞—Ç–∞—Å–µ—Ç–∞ YOLO
    
    Args:
        dataset_dir: –ü—É—Ç—å –∫ –¥–∞—Ç–∞—Å–µ—Ç—É
        
    Returns:
        True –µ—Å–ª–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–∞
    """
    logger = setup_logger(__name__)
    
    required_dirs = [
        'train/images',
        'train/labels', 
        'val/images',
        'val/labels',
        'test/images',
        'test/labels'
    ]
    
    missing_dirs = []
    for dir_path in required_dirs:
        full_path = dataset_dir / dir_path
        if not full_path.exists():
            missing_dirs.append(dir_path)
    
    if missing_dirs:
        logger.error(f"‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω—ã –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏ –≤ {dataset_dir}")
        logger.error("–û–∂–∏–¥–∞–µ–º–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞:")
        logger.error("  dataset/")
        for req_dir in required_dirs:
            status = "‚ùå" if req_dir in missing_dirs else "‚úÖ"
            logger.error(f"  {status} {req_dir}/")
        return False
    
    return True

def create_dataset_structure(dataset_dir: Path):
    """
    –°–æ–∑–¥–∞–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞—Ç–∞—Å–µ—Ç–∞ –µ—Å–ª–∏ –æ–Ω–∞ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
    
    Args:
        dataset_dir: –ü—É—Ç—å –∫ –¥–∞—Ç–∞—Å–µ—Ç—É
    """
    logger = setup_logger(__name__)
    
    logger.info(f"üèóÔ∏è –°–æ–∑–¥–∞–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞—Ç–∞—Å–µ—Ç–∞ –≤ {dataset_dir}")
    
    required_dirs = [
        'train/images',
        'train/labels', 
        'val/images',
        'val/labels',
        'test/images',
        'test/labels'
    ]
    
    for dir_path in required_dirs:
        full_path = dataset_dir / dir_path
        full_path.mkdir(parents=True, exist_ok=True)
        logger.info(f"‚úÖ –°–æ–∑–¥–∞–Ω–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {dir_path}")
    
    logger.info("üéØ –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ —Å–æ–∑–¥–∞–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")

def get_available_yolo_models() -> List[str]:
    """
    –ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö YOLO –º–æ–¥–µ–ª–µ–π
    
    Returns:
        –°–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
    """
    # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –º–æ–¥–µ–ª–∏ YOLOv8/v11
    standard_models = [
        'yolo11n.pt', 'yolo11s.pt', 'yolo11m.pt', 'yolo11l.pt', 'yolo11x.pt',
        'yolov8n.pt', 'yolov8s.pt', 'yolov8m.pt', 'yolov8l.pt', 'yolov8x.pt'
    ]
    
    available_models = []
    
    try:
        from ultralytics import YOLO
        
        for model_name in standard_models:
            try:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏
                model = YOLO(model_name)
                available_models.append(model_name)
            except Exception:
                continue
                
    except ImportError:
        # –ï—Å–ª–∏ ultralytics –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –±–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏
        available_models = ['yolo11n.pt', 'yolov8n.pt']
    
    return available_models

def detect_objects_with_yolo(image_path: Path, 
                           model_names: List[str],
                           restaurant_classes: List[str],
                           confidence_threshold: float = 0.25) -> List[Dict]:
    """
    –ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–∞—è –¥–µ—Ç–µ–∫—Ü–∏—è –æ–±—ä–µ–∫—Ç–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∞–Ω—Å–∞–º–±–ª—è YOLO –º–æ–¥–µ–ª–µ–π
    
    Args:
        image_path: –ü—É—Ç—å –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é
        model_names: –°–ø–∏—Å–æ–∫ –∏–º–µ–Ω –º–æ–¥–µ–ª–µ–π –¥–ª—è –∞–Ω—Å–∞–º–±–ª—è
        restaurant_classes: –¶–µ–ª–µ–≤—ã–µ –∫–ª–∞—Å—Å—ã –¥–ª—è —Ä–µ—Å—Ç–æ—Ä–∞–Ω–Ω–æ–π —Å—Ä–µ–¥—ã
        confidence_threshold: –ü–æ—Ä–æ–≥ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
        
    Returns:
        –°–ø–∏—Å–æ–∫ –¥–µ—Ç–µ–∫—Ü–∏–π –≤ —Ñ–æ—Ä–º–∞—Ç–µ YOLO
    """
    try:
        from ultralytics import YOLO
    except ImportError:
        return []
    
    logger = setup_logger(__name__)
    
    image = cv2.imread(str(image_path))
    if image is None:
        return []
    
    height, width = image.shape[:2]
    all_detections = []
    
    # –î–µ—Ç–µ–∫—Ü–∏—è –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª—å—é –∏–∑ –∞–Ω—Å–∞–º–±–ª—è
    for model_name in model_names:
        try:
            model = YOLO(model_name)
            results = model(image, conf=confidence_threshold, verbose=False)
            
            if results and len(results) > 0:
                result = results[0]
                
                if result.boxes is not None and len(result.boxes) > 0:
                    boxes = result.boxes.xyxy.cpu().numpy()
                    confidences = result.boxes.conf.cpu().numpy()
                    class_ids = result.boxes.cls.cpu().numpy().astype(int)
                    
                    for box, conf, class_id in zip(boxes, confidences, class_ids):
                        if class_id < len(result.names):
                            class_name = result.names[class_id]
                            
                            # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ —Ä–µ—Å—Ç–æ—Ä–∞–Ω–Ω—ã–º –∫–ª–∞—Å—Å–∞–º
                            if class_name in restaurant_classes:
                                x1, y1, x2, y2 = box
                                
                                # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ —Ñ–æ—Ä–º–∞—Ç YOLO (–Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã)
                                x_center = (x1 + x2) / 2 / width
                                y_center = (y1 + y2) / 2 / height
                                w = (x2 - x1) / width
                                h = (y2 - y1) / height
                                
                                # –í–∞–ª–∏–¥–∞—Ü–∏—è –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç
                                if (0 <= x_center <= 1 and 0 <= y_center <= 1 and
                                    0 < w <= 1 and 0 < h <= 1):
                                    
                                    detection = {
                                        'class_name': class_name,
                                        'class_id': restaurant_classes.index(class_name) if class_name in restaurant_classes else 0,
                                        'confidence': float(conf),
                                        'bbox': [x_center, y_center, w, h]
                                    }
                                    all_detections.append(detection)
        
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –¥–µ—Ç–µ–∫—Ü–∏–∏ —Å –º–æ–¥–µ–ª—å—é {model_name}: {e}")
            continue
    
    # –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
    final_detections = remove_duplicate_detections(all_detections)
    
    return final_detections

def remove_duplicate_detections(detections: List[Dict], iou_threshold: float = 0.6) -> List[Dict]:
    """
    –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏—Ä—É—é—â–∏—Ö—Å—è –¥–µ—Ç–µ–∫—Ü–∏–π –ø–æ IoU
    
    Args:
        detections: –°–ø–∏—Å–æ–∫ –¥–µ—Ç–µ–∫—Ü–∏–π
        iou_threshold: –ü–æ—Ä–æ–≥ IoU –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
        
    Returns:
        –§–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ –¥–µ—Ç–µ–∫—Ü–∏–π
    """
    if not detections:
        return []
    
    # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ (—É–±—ã–≤–∞–Ω–∏–µ)
    detections = sorted(detections, key=lambda x: x['confidence'], reverse=True)
    
    filtered = []
    for detection in detections:
        is_duplicate = False
        
        for existing in filtered:
            if (detection['class_name'] == existing['class_name'] and
                calculate_iou(detection['bbox'], existing['bbox']) > iou_threshold):
                is_duplicate = True
                break
        
        if not is_duplicate:
            filtered.append(detection)
    
    return filtered

def calculate_iou(bbox1: List[float], bbox2: List[float]) -> float:
    """
    –í—ã—á–∏—Å–ª–µ–Ω–∏–µ IoU –¥–ª—è bbox –≤ —Ñ–æ—Ä–º–∞—Ç–µ YOLO (x_center, y_center, width, height)
    
    Args:
        bbox1, bbox2: Bbox –≤ —Ñ–æ—Ä–º–∞—Ç–µ [x_center, y_center, width, height]
        
    Returns:
        IoU –∑–Ω–∞—á–µ–Ω–∏–µ
    """
    # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ —Ñ–æ—Ä–º–∞—Ç (x1, y1, x2, y2)
    def yolo_to_xyxy(bbox):
        x_center, y_center, width, height = bbox
        x1 = x_center - width / 2
        y1 = y_center - height / 2
        x2 = x_center + width / 2
        y2 = y_center + height / 2
        return [x1, y1, x2, y2]
    
    box1 = yolo_to_xyxy(bbox1)
    box2 = yolo_to_xyxy(bbox2)
    
    # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏—è
    x1 = max(box1[0], box2[0])
    y1 = max(box1[1], box2[1])
    x2 = min(box1[2], box2[2])
    y2 = min(box1[3], box2[3])
    
    if x2 <= x1 or y2 <= y1:
        return 0.0
    
    intersection = (x2 - x1) * (y2 - y1)
    
    # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –ø–ª–æ—â–∞–¥–µ–π
    area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])
    area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])
    
    union = area1 + area2 - intersection
    
    return intersection / union if union > 0 else 0.0

def save_yolo_annotation(detections: List[Dict], output_path: Path):
    """
    –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π –≤ —Ñ–æ—Ä–º–∞—Ç–µ YOLO
    
    Args:
        detections: –°–ø–∏—Å–æ–∫ –¥–µ—Ç–µ–∫—Ü–∏–π
        output_path: –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ñ–∞–π–ª–∞ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏
    """
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_path, 'w', encoding='utf-8') as f:
        for detection in detections:
            class_id = detection['class_id']
            x_center, y_center, width, height = detection['bbox']
            
            # –§–æ—Ä–º–∞—Ç YOLO: class_id x_center y_center width height
            f.write(f"{class_id} {x_center:.6f} {y_center:.6f} {width:.6f} {height:.6f}\n")

def process_dataset_split(split_dir: Path, 
                         restaurant_classes: List[str],
                         model_names: List[str],
                         confidence_threshold: float = 0.25,
                         use_auto_annotation: bool = False) -> Dict[str, int]:
    """
    –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–≥–æ split'–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ (train/val/test)
    
    Args:
        split_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è split'–∞
        restaurant_classes: –°–ø–∏—Å–æ–∫ —Ü–µ–ª–µ–≤—ã—Ö –∫–ª–∞—Å—Å–æ–≤
        model_names: –ú–æ–¥–µ–ª–∏ –¥–ª—è –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏
        confidence_threshold: –ü–æ—Ä–æ–≥ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
        use_auto_annotation: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫—É—é –∞–Ω–Ω–æ—Ç–∞—Ü–∏—é
        
    Returns:
        –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏
    """
    logger = setup_logger(__name__)
    
    images_dir = split_dir / "images"
    labels_dir = split_dir / "labels"
    
    if not images_dir.exists():
        logger.warning(f"‚ö†Ô∏è –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {images_dir}")
        return {'processed': 0, 'annotated': 0, 'errors': 0}
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π
    labels_dir.mkdir(parents=True, exist_ok=True)
    
    # –ü–æ–∏—Å–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
    image_extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.tiff']
    images = []
    for ext in image_extensions:
        images.extend(list(images_dir.glob(f"*{ext}")))
        images.extend(list(images_dir.glob(f"*{ext.upper()}")))
    
    if not images:
        logger.warning(f"‚ö†Ô∏è –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ {images_dir}")
        return {'processed': 0, 'annotated': 0, 'errors': 0}
    
    logger.info(f"üì∑ –ù–∞–π–¥–µ–Ω–æ {len(images)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ {split_dir.name}")
    
    stats = {'processed': 0, 'annotated': 0, 'errors': 0}
    
    for image_path in tqdm(images, desc=f"–û–±—Ä–∞–±–æ—Ç–∫–∞ {split_dir.name}"):
        try:
            label_filename = image_path.stem + ".txt"
            label_path = labels_dir / label_filename
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏
            if label_path.exists():
                # –ü—Ä–æ–≤–µ—Ä–∫–∞, –ø—É—Å—Ç–∞—è –ª–∏ –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è
                with open(label_path, 'r', encoding='utf-8') as f:
                    content = f.read().strip()
                
                if content and not use_auto_annotation:
                    # –ê–Ω–Ω–æ—Ç–∞—Ü–∏—è —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –∏ –Ω–µ –ø—É—Å—Ç–∞—è
                    stats['processed'] += 1
                    continue
            
            # –°–æ–∑–¥–∞–Ω–∏–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏
            detections = []
            
            if use_auto_annotation and model_names:
                # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è —Å –ø–æ–º–æ—â—å—é YOLO
                detections = detect_objects_with_yolo(
                    image_path=image_path,
                    model_names=model_names,
                    restaurant_classes=restaurant_classes,
                    confidence_threshold=confidence_threshold
                )
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ (–ø—É—Å—Ç–æ–π –∏–ª–∏ —Å –¥–µ—Ç–µ–∫—Ü–∏—è–º–∏)
            save_yolo_annotation(detections, label_path)
            
            stats['processed'] += 1
            if detections:
                stats['annotated'] += 1
                
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {image_path}: {e}")
            stats['errors'] += 1
            
            # –°–æ–∑–¥–∞–Ω–∏–µ –ø—É—Å—Ç–æ–π –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
            try:
                label_filename = image_path.stem + ".txt"
                label_path = labels_dir / label_filename
                with open(label_path, 'w', encoding='utf-8') as f:
                    pass
            except Exception:
                pass
    
    return stats

def create_or_update_dataset_yaml(dataset_dir: Path, restaurant_classes: List[str]):
    """
    –°–æ–∑–¥–∞–Ω–∏–µ –∏–ª–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ dataset.yaml —Ñ–∞–π–ª–∞
    
    Args:
        dataset_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞
        restaurant_classes: –°–ø–∏—Å–æ–∫ –∫–ª–∞—Å—Å–æ–≤ –¥–ª—è —Ä–µ—Å—Ç–æ—Ä–∞–Ω–Ω–æ–π —Å—Ä–µ–¥—ã
    """
    logger = setup_logger(__name__)
    
    yaml_config = {
        'path': str(dataset_dir.absolute()),
        'train': 'train/images',
        'val': 'val/images',
        'test': 'test/images',
        'nc': len(restaurant_classes),
        'names': restaurant_classes
    }
    
    yaml_path = dataset_dir / "dataset.yaml"
    
    try:
        with open(yaml_path, 'w', encoding='utf-8') as f:
            yaml.dump(yaml_config, f, default_flow_style=False, allow_unicode=True)
        
        logger.info(f"‚úÖ –°–æ–∑–¥–∞–Ω/–æ–±–Ω–æ–≤–ª–µ–Ω dataset.yaml: {yaml_path}")
        logger.info(f"üìã –ö–ª–∞—Å—Å—ã ({len(restaurant_classes)}): {', '.join(restaurant_classes)}")
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è dataset.yaml: {e}")

class AnnotationFixer:
    """–û—Å–Ω–æ–≤–Ω–æ–π –∫–ª–∞—Å—Å –¥–ª—è –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π"""
    
    def __init__(self, dataset_dir: Path, config: Dict[str, Any] = None):
        self.dataset_dir = Path(dataset_dir)
        self.config = config or self._get_default_config()
        self.logger = setup_logger(self.__class__.__name__)
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.stats = {
            'start_time': time.time(),
            'total_processed': 0,
            'total_annotated': 0,
            'total_errors': 0,
            'splits_processed': []
        }
    
    def _get_default_config(self) -> Dict[str, Any]:
        """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é"""
        return {
            'restaurant_classes': [
                'person', 'chair', 'dining_table', 'cup', 'bowl',
                'bottle', 'wine_glass', 'fork', 'knife', 'spoon',
                'plate', 'food', 'phone', 'book', 'laptop'
            ],
            'auto_annotation': {
                'enabled': True,
                'confidence_threshold': 0.25,
                'models': ['yolo11n.pt', 'yolov8n.pt'],
                'max_models': 2
            },
            'processing': {
                'create_structure_if_missing': True,
                'overwrite_existing': False,
                'splits_to_process': ['train', 'val', 'test']
            }
        }
    
    def run_fix_process(self):
        """–ó–∞–ø—É—Å–∫ –ø—Ä–æ—Ü–µ—Å—Å–∞ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π"""
        self.logger.info("üîß –ó–∞–ø—É—Å–∫ –ø—Ä–æ—Ü–µ—Å—Å–∞ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π")
        
        try:
            # 1. –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏ —Å–æ–∑–¥–∞–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
            if not check_dataset_structure(self.dataset_dir):
                if self.config['processing']['create_structure_if_missing']:
                    create_dataset_structure(self.dataset_dir)
                else:
                    raise ValueError("–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–∞")
            
            # 2. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –º–æ–¥–µ–ª–µ–π –¥–ª—è –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏
            available_models = []
            if self.config['auto_annotation']['enabled']:
                available_models = self._prepare_annotation_models()
            
            # 3. –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞–∂–¥–æ–≥–æ split'–∞
            splits_to_process = self.config['processing']['splits_to_process']
            
            for split_name in splits_to_process:
                split_dir = self.dataset_dir / split_name
                
                if not split_dir.exists():
                    self.logger.warning(f"‚ö†Ô∏è Split '{split_name}' –Ω–µ –Ω–∞–π–¥–µ–Ω, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
                    continue
                
                self.logger.info(f"üîÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ {split_name} split...")
                
                split_stats = process_dataset_split(
                    split_dir=split_dir,
                    restaurant_classes=self.config['restaurant_classes'],
                    model_names=available_models,
                    confidence_threshold=self.config['auto_annotation']['confidence_threshold'],
                    use_auto_annotation=self.config['auto_annotation']['enabled']
                )
                
                # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –æ–±—â–µ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
                self.stats['total_processed'] += split_stats['processed']
                self.stats['total_annotated'] += split_stats['annotated']
                self.stats['total_errors'] += split_stats['errors']
                self.stats['splits_processed'].append({
                    'split': split_name,
                    'stats': split_stats
                })
                
                self.logger.info(f"‚úÖ {split_name}: –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ {split_stats['processed']}, "
                               f"–∞–Ω–Ω–æ—Ç–∏—Ä–æ–≤–∞–Ω–æ {split_stats['annotated']}, "
                               f"–æ—à–∏–±–æ–∫ {split_stats['errors']}")
            
            # 4. –°–æ–∑–¥–∞–Ω–∏–µ/–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ dataset.yaml
            create_or_update_dataset_yaml(
                dataset_dir=self.dataset_dir,
                restaurant_classes=self.config['restaurant_classes']
            )
            
            # 5. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞
            self._generate_completion_report()
            
            self.logger.info("üéâ –ü—Ä–æ—Ü–µ—Å—Å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ!")
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è: {e}")
            self._generate_error_report(e)
            raise
    
    def _prepare_annotation_models(self) -> List[str]:
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –º–æ–¥–µ–ª–µ–π –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏"""
        self.logger.info("ü§ñ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –º–æ–¥–µ–ª–µ–π –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏...")
        
        try:
            available_models = get_available_yolo_models()
            
            if not available_models:
                self.logger.warning("‚ö†Ô∏è YOLO –º–æ–¥–µ–ª–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã, –±—É–¥—É—Ç —Å–æ–∑–¥–∞–Ω—ã –ø—É—Å—Ç—ã–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏")
                return []
            
            # –í—ã–±–æ—Ä –º–æ–¥–µ–ª–µ–π –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
            target_models = self.config['auto_annotation']['models']
            max_models = self.config['auto_annotation']['max_models']
            
            selected_models = []
            for model_name in target_models:
                if model_name in available_models and len(selected_models) < max_models:
                    selected_models.append(model_name)
            
            # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ —Ü–µ–ª–µ–≤—ã–µ –º–æ–¥–µ–ª–∏, –±–µ—Ä–µ–º –ª—é–±—ã–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ
            if not selected_models:
                selected_models = available_models[:max_models]
            
            self.logger.info(f"üéØ –í—ã–±—Ä–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏: {selected_models}")
            return selected_models
            
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –º–æ–¥–µ–ª–µ–π: {e}")
            return []
    
    def _generate_completion_report(self):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞ –æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏"""
        total_time = time.time() - self.stats['start_time']
        
        report = {
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
            'status': 'completed',
            'execution_time_seconds': round(total_time, 2),
            'dataset_directory': str(self.dataset_dir),
            'configuration': self.config,
            'statistics': {
                'total_processed': self.stats['total_processed'],
                'total_annotated': self.stats['total_annotated'],
                'total_errors': self.stats['total_errors'],
                'splits_processed': self.stats['splits_processed']
            },
            'output_files': {
                'dataset_yaml': str(self.dataset_dir / 'dataset.yaml'),
                'annotation_files_created': self.stats['total_processed']
            },
            'next_steps': [
                "–î–∞—Ç–∞—Å–µ—Ç –≥–æ—Ç–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è YOLO –º–æ–¥–µ–ª–∏",
                "–ó–∞–ø—É—Å—Ç–∏—Ç–µ train_model.py –¥–ª—è –Ω–∞—á–∞–ª–∞ –æ–±—É—á–µ–Ω–∏—è",
                "–ü—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –ø—Ä–æ–≤–µ—Ä—å—Ç–µ –∫–∞—á–µ—Å—Ç–≤–æ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π –≤—Ä—É—á–Ω—É—é"
            ]
        }
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–∞
        report_path = self.dataset_dir / "annotation_fix_report.json"
        try:
            with open(report_path, 'w', encoding='utf-8') as f:
                json.dump(report, f, ensure_ascii=False, indent=2)
            
            self.logger.info(f"üìã –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {report_path}")
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –æ—Ç—á–µ—Ç–∞: {e}")
        
        # –í—ã–≤–æ–¥ –∫—Ä–∞—Ç–∫–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        self.logger.info(f"‚è±Ô∏è –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {total_time:.2f} —Å–µ–∫—É–Ω–¥")
        self.logger.info(f"üìä –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Ñ–∞–π–ª–æ–≤: {self.stats['total_processed']}")
        self.logger.info(f"üéØ –°–æ–∑–¥–∞–Ω–æ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π: {self.stats['total_annotated']}")
    
    def _generate_error_report(self, error: Exception):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞ –æ–± –æ—à–∏–±–∫–µ"""
        total_time = time.time() - self.stats['start_time']
        
        error_report = {
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
            'status': 'failed',
            'execution_time_seconds': round(total_time, 2),
            'error': {
                'type': type(error).__name__,
                'message': str(error)
            },
            'statistics': self.stats,
            'troubleshooting': [
                "–ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–∞—Ç–∞—Å–µ—Ç–∞",
                "–£–±–µ–¥–∏—Ç–µ—Å—å –≤ –Ω–∞–ª–∏—á–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ –ø–∞–ø–∫–∞—Ö",
                "–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å YOLO –º–æ–¥–µ–ª–µ–π",
                "–£–±–µ–¥–∏—Ç–µ—Å—å –≤ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç–∏ –ø—É—Ç–µ–π"
            ]
        }
        
        error_report_path = self.dataset_dir / "annotation_fix_error.json"
        try:
            with open(error_report_path, 'w', encoding='utf-8') as f:
                json.dump(error_report, f, ensure_ascii=False, indent=2)
        except Exception:
            pass

def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    parser = argparse.ArgumentParser(
        description="–°–∫—Ä–∏–ø—Ç –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π –¥–ª—è YOLO –¥–∞—Ç–∞—Å–µ—Ç–∞",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
–ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:

1. –ë–∞–∑–æ–≤–æ–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ (—Å–æ–∑–¥–∞–Ω–∏–µ –ø—É—Å—Ç—ã—Ö –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π):
   python scripts/fix_annotations.py --dataset "data/processed/dataset"

2. –° –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –∞–Ω–Ω–æ—Ç–∞—Ü–∏–µ–π:
   python scripts/fix_annotations.py --dataset "data/processed/dataset" --auto-annotate

3. –°–æ–∑–¥–∞–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –∏ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π:
   python scripts/fix_annotations.py --dataset "data/processed/dataset" --create-structure --auto-annotate

4. –¢–æ–ª—å–∫–æ –¥–ª—è train split:
   python scripts/fix_annotations.py --dataset "data/processed/dataset" --splits train

–ß—Ç–æ –¥–µ–ª–∞–µ—Ç —Å–∫—Ä–∏–ø—Ç:
- –ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–∞—Ç–∞—Å–µ—Ç–∞ YOLO
- –°–æ–∑–¥–∞–µ—Ç –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
- –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ –¥–ª—è –≤—Å–µ—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
- –°–æ–∑–¥–∞–µ—Ç dataset.yaml –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
- –ü—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –¥–µ—Ç–∞–ª—å–Ω—ã–µ –æ—Ç—á–µ—Ç—ã
        """
    )
    
    parser.add_argument(
        '--dataset',
        type=str,
        required=True,
        help='–ü—É—Ç—å –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞'
    )
    
    parser.add_argument(
        '--auto-annotate',
        action='store_true',
        help='–í–∫–ª—é—á–∏—Ç—å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫—É—é –∞–Ω–Ω–æ—Ç–∞—Ü–∏—é —Å –ø–æ–º–æ—â—å—é YOLO –º–æ–¥–µ–ª–µ–π'
    )
    
    parser.add_argument(
        '--create-structure',
        action='store_true',
        help='–°–æ–∑–¥–∞—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–∞—Ç–∞—Å–µ—Ç–∞ –µ—Å–ª–∏ –æ–Ω–∞ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç'
    )
    
    parser.add_argument(
        '--confidence',
        type=float,
        default=0.25,
        help='–ü–æ—Ä–æ–≥ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 0.25)'
    )
    
    parser.add_argument(
        '--splits',
        nargs='+',
        default=['train', 'val', 'test'],
        help='–°–ø–∏—Å–æ–∫ splits –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: train val test)'
    )
    
    parser.add_argument(
        '--models',
        nargs='+',
        default=['yolo11n.pt', 'yolov8n.pt'],
        help='YOLO –º–æ–¥–µ–ª–∏ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏'
    )
    
    args = parser.parse_args()
    
    try:
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        config = {
            'restaurant_classes': [
                'person', 'chair', 'dining_table', 'cup', 'bowl',
                'bottle', 'wine_glass', 'fork', 'knife', 'spoon',
                'plate', 'food', 'phone', 'book', 'laptop'
            ],
            'auto_annotation': {
                'enabled': args.auto_annotate,
                'confidence_threshold': args.confidence,
                'models': args.models,
                'max_models': 2
            },
            'processing': {
                'create_structure_if_missing': args.create_structure,
                'overwrite_existing': False,
                'splits_to_process': args.splits
            }
        }
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –∏ –∑–∞–ø—É—Å–∫ —Ñ–∏–∫—Å–µ—Ä–∞
        fixer = AnnotationFixer(
            dataset_dir=Path(args.dataset),
            config=config
        )
        
        fixer.run_fix_process()
        
        print("\n" + "="*50)
        print("üéâ –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï –ê–ù–ù–û–¢–ê–¶–ò–ô –ó–ê–í–ï–†–®–ï–ù–û!")
        print("="*50)
        print(f"üìÅ –î–∞—Ç–∞—Å–µ—Ç: {args.dataset}")
        print(f"‚öôÔ∏è –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è: {args.dataset}/dataset.yaml")
        print(f"üìã –û—Ç—á–µ—Ç: {args.dataset}/annotation_fix_report.json")
        print("\nüöÄ –î–∞—Ç–∞—Å–µ—Ç –≥–æ—Ç–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏!")
        print("="*50)
        
    except Exception as e:
        print(f"\n‚ùå –û–®–ò–ë–ö–ê: {e}")
        print("üìã –ü—Ä–æ–≤–µ—Ä—å—Ç–µ error_report.json –¥–ª—è –¥–µ—Ç–∞–ª–µ–π")
        sys.exit(1)

if __name__ == "__main__":
    main()