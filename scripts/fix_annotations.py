"""
–ë—ã—Å—Ç—Ä–æ–µ —Ä–µ—à–µ–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º—ã —Å –ø—É—Å—Ç—ã–º–∏ –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è–º–∏
–°–æ–∑–¥–∞–µ—Ç –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ –¥–ª—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
"""

import os
import sys
import logging
import argparse
from pathlib import Path
from typing import List, Dict, Any
import cv2
import numpy as np
import torch
from ultralytics import YOLO
from tqdm import tqdm
import json
import yaml
import time

def setup_logger():
    """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–≥–µ—Ä–∞"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(),
            logging.FileHandler('annotation_fix.log', encoding='utf-8')
        ]
    )
    return logging.getLogger(__name__)

def detect_objects_professional(image_path: Path, models: List[YOLO], 
                               restaurant_classes: List[str],
                               confidence_threshold: float = 0.25) -> List[Dict]:
    """
    –ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–∞—è –¥–µ—Ç–µ–∫—Ü–∏—è –æ–±—ä–µ–∫—Ç–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∞–Ω—Å–∞–º–±–ª—è –º–æ–¥–µ–ª–µ–π
    """
    image = cv2.imread(str(image_path))
    if image is None:
        return []
    
    height, width = image.shape[:2]
    all_detections = []
    
    # –î–µ—Ç–µ–∫—Ü–∏—è –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª—å—é
    for model in models:
        try:
            results = model(image, conf=confidence_threshold, verbose=False)
            
            if results and len(results) > 0:
                result = results[0]
                
                if result.boxes is not None and len(result.boxes) > 0:
                    boxes = result.boxes.xyxy.cpu().numpy()
                    confidences = result.boxes.conf.cpu().numpy()
                    class_ids = result.boxes.cls.cpu().numpy().astype(int)
                    
                    for box, conf, class_id in zip(boxes, confidences, class_ids):
                        if class_id < len(result.names):
                            class_name = result.names[class_id]
                            
                            # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ —Ä–µ—Å—Ç–æ—Ä–∞–Ω–Ω—ã–º –∫–ª–∞—Å—Å–∞–º
                            if class_name in restaurant_classes:
                                x1, y1, x2, y2 = box
                                
                                # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ —Ñ–æ—Ä–º–∞—Ç YOLO (–Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã)
                                x_center = (x1 + x2) / 2 / width
                                y_center = (y1 + y2) / 2 / height
                                w = (x2 - x1) / width
                                h = (y2 - y1) / height
                                
                                # –í–∞–ª–∏–¥–∞—Ü–∏—è –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç
                                if (0 <= x_center <= 1 and 0 <= y_center <= 1 and
                                    0 < w <= 1 and 0 < h <= 1):
                                    
                                    detection = {
                                        'class_name': class_name,
                                        'confidence': float(conf),
                                        'bbox': [x_center, y_center, w, h]
                                    }
                                    all_detections.append(detection)
        
        except Exception as e:
            logging.warning(f"–û—à–∏–±–∫–∞ –¥–µ—Ç–µ–∫—Ü–∏–∏ —Å –º–æ–¥–µ–ª—å—é: {e}")
            continue
    
    # –ü—Ä–æ—Å—Ç–æ–µ —É–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –ø–æ IoU
    final_detections = remove_duplicate_detections(all_detections)
    
    return final_detections

def remove_duplicate_detections(detections: List[Dict], iou_threshold: float = 0.6) -> List[Dict]:
    """–£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏—Ä—É—é—â–∏—Ö—Å—è –¥–µ—Ç–µ–∫—Ü–∏–π"""
    if not detections:
        return []
    
    # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
    detections = sorted(detections, key=lambda x: x['confidence'], reverse=True)
    
    filtered = []
    for detection in detections:
        is_duplicate = False
        
        for existing in filtered:
            if (detection['class_name'] == existing['class_name'] and
                calculate_iou(detection['bbox'], existing['bbox']) > iou_threshold):
                is_duplicate = True
                break
        
        if not is_duplicate:
            filtered.append(detection)
    
    return filtered

def calculate_iou(bbox1: List[float], bbox2: List[float]) -> float:
    """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ IoU –º–µ–∂–¥—É –¥–≤—É–º—è bbox –≤ —Ñ–æ—Ä–º–∞—Ç–µ YOLO"""
    def yolo_to_corners(bbox):
        x_center, y_center, width, height = bbox
        x1 = x_center - width / 2
        y1 = y_center - height / 2
        x2 = x_center + width / 2
        y2 = y_center + height / 2
        return x1, y1, x2, y2
    
    x1_1, y1_1, x2_1, y2_1 = yolo_to_corners(bbox1)
    x1_2, y1_2, x2_2, y2_2 = yolo_to_corners(bbox2)
    
    # –ü–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ
    x1_inter = max(x1_1, x1_2)
    y1_inter = max(y1_1, y1_2)
    x2_inter = min(x2_1, x2_2)
    y2_inter = min(y2_1, y2_2)
    
    if x2_inter <= x1_inter or y2_inter <= y1_inter:
        return 0.0
    
    intersection = (x2_inter - x1_inter) * (y2_inter - y1_inter)
    
    # –ü–ª–æ—â–∞–¥–∏
    area1 = (x2_1 - x1_1) * (y2_1 - y1_1)
    area2 = (x2_2 - x1_2) * (y2_2 - y1_2)
    
    union = area1 + area2 - intersection
    
    return intersection / union if union > 0 else 0.0

def create_class_mapping(restaurant_classes: List[str]) -> Dict[str, int]:
    """–°–æ–∑–¥–∞–Ω–∏–µ –º–∞–ø–ø–∏–Ω–≥–∞ –∫–ª–∞—Å—Å–æ–≤"""
    return {class_name: idx for idx, class_name in enumerate(restaurant_classes)}

def save_yolo_annotation(detections: List[Dict], output_path: Path, class_mapping: Dict[str, int]):
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ –≤ —Ñ–æ—Ä–º–∞—Ç–µ YOLO"""
    with open(output_path, 'w', encoding='utf-8') as f:
        for detection in detections:
            class_name = detection['class_name']
            if class_name in class_mapping:
                class_id = class_mapping[class_name]
                bbox = detection['bbox']
                
                # –§–æ—Ä–º–∞—Ç YOLO: class_id x_center y_center width height
                line = f"{class_id} {bbox[0]:.6f} {bbox[1]:.6f} {bbox[2]:.6f} {bbox[3]:.6f}\n"
                f.write(line)

def create_dataset_yaml(dataset_path: Path, class_mapping: Dict[str, int]):
    """–°–æ–∑–¥–∞–Ω–∏–µ dataset.yaml –¥–ª—è YOLO"""
    yaml_content = {
        'path': str(dataset_path.absolute()),
        'train': 'train/images',
        'val': 'val/images',
        'test': 'test/images',
        'nc': len(class_mapping),
        'names': list(class_mapping.keys())
    }
    
    yaml_path = dataset_path / 'dataset.yaml'
    with open(yaml_path, 'w', encoding='utf-8') as f:
        yaml.dump(yaml_content, f, default_flow_style=False, allow_unicode=True)
    
    return yaml_path

def fix_annotations(dataset_dir: Path, confidence_threshold: float = 0.25):
    """
    –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π
    """
    logger = setup_logger()
    logger.info("üöÄ –ó–∞–ø—É—Å–∫ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–≥–æ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π")
    
    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Ä–µ—Å—Ç–æ—Ä–∞–Ω–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤
    restaurant_classes = [
        'person', 'chair', 'dining table', 'cup', 'fork', 'knife',
        'spoon', 'bowl', 'bottle', 'wine glass', 'sandwich', 'pizza',
        'cake', 'apple', 'banana', 'orange', 'cell phone', 'laptop', 'book'
    ]
    
    class_mapping = create_class_mapping(restaurant_classes)
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π
    logger.info("üß† –ó–∞–≥—Ä—É–∑–∫–∞ –∞–Ω—Å–∞–º–±–ª—è –º–æ–¥–µ–ª–µ–π YOLO...")
    models = []
    
    model_configs = [
        ('yolo11n.pt', 0.15),
        ('yolo11s.pt', 0.18),
        ('yolo11m.pt', 0.22)
    ]
    
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    logger.info(f"üíª –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")
    
    for model_name, conf in model_configs:
        try:
            logger.info(f"üì• –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏: {model_name}")
            model = YOLO(model_name)
            model.to(device)
            models.append(model)
            logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å {model_name} —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å {model_name}: {e}")
    
    if not models:
        logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –Ω–∏ –æ–¥–Ω–æ–π –º–æ–¥–µ–ª–∏!")
        return False
    
    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞–∂–¥–æ–≥–æ split'–∞
    splits = ['train', 'val', 'test']
    total_processed = 0
    total_annotations_created = 0
    
    for split in splits:
        split_images_dir = dataset_dir / split / 'images'
        split_labels_dir = dataset_dir / split / 'labels'
        
        if not split_images_dir.exists():
            logger.warning(f"‚ö†Ô∏è –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è {split_images_dir} –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
            continue
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π
        split_labels_dir.mkdir(parents=True, exist_ok=True)
        
        # –ü–æ–∏—Å–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
        image_extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.tiff']
        image_files = []
        for ext in image_extensions:
            image_files.extend(list(split_images_dir.glob(f"*{ext}")))
            image_files.extend(list(split_images_dir.glob(f"*{ext.upper()}")))
        
        if not image_files:
            logger.warning(f"‚ö†Ô∏è –ù–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ {split_images_dir}")
            continue
        
        logger.info(f"üñºÔ∏è –û–±—Ä–∞–±–æ—Ç–∫–∞ {split}: –Ω–∞–π–¥–µ–Ω–æ {len(image_files)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
        
        split_annotations = 0
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–æ–º
        for image_path in tqdm(image_files, desc=f"–ê–Ω–Ω–æ—Ç–∞—Ü–∏—è {split}"):
            try:
                # –î–µ—Ç–µ–∫—Ü–∏—è –æ–±—ä–µ–∫—Ç–æ–≤
                detections = detect_objects_professional(
                    image_path, models, restaurant_classes, confidence_threshold
                )
                
                # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏
                annotation_path = split_labels_dir / f"{image_path.stem}.txt"
                save_yolo_annotation(detections, annotation_path, class_mapping)
                
                total_processed += 1
                split_annotations += len(detections)
                
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {image_path}: {e}")
                
                # –°–æ–∑–¥–∞–Ω–∏–µ –ø—É—Å—Ç–æ–≥–æ —Ñ–∞–π–ª–∞ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
                annotation_path = split_labels_dir / f"{image_path.stem}.txt"
                annotation_path.touch()
        
        logger.info(f"‚úÖ {split} –∑–∞–≤–µ—Ä—à–µ–Ω: {len(image_files)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, {split_annotations} –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π")
        total_annotations_created += split_annotations
    
    # –°–æ–∑–¥–∞–Ω–∏–µ dataset.yaml
    logger.info("üìÑ –°–æ–∑–¥–∞–Ω–∏–µ dataset.yaml...")
    yaml_path = create_dataset_yaml(dataset_dir, class_mapping)
    logger.info(f"‚úÖ –°–æ–∑–¥–∞–Ω —Ñ–∞–π–ª: {yaml_path}")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –æ—Ç—á–µ—Ç–∞
    report = {
        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
        'total_images_processed': total_processed,
        'total_annotations_created': total_annotations_created,
        'models_used': [config[0] for config in model_configs if config[0] in [str(m.ckpt_path) for m in models]],
        'confidence_threshold': confidence_threshold,
        'restaurant_classes': restaurant_classes,
        'class_mapping': class_mapping,
        'dataset_yaml': str(yaml_path)
    }
    
    report_path = dataset_dir / 'annotation_fix_report.json'
    with open(report_path, 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    
    # –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    logger.info("\n" + "="*60)
    logger.info("üìã –ò–¢–û–ì–ò –ò–°–ü–†–ê–í–õ–ï–ù–ò–Ø –ê–ù–ù–û–¢–ê–¶–ò–ô")
    logger.info("="*60)
    logger.info(f"üñºÔ∏è –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {total_processed}")
    logger.info(f"üéØ –°–æ–∑–¥–∞–Ω–æ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π: {total_annotations_created}")
    logger.info(f"üìÑ Dataset YAML: {yaml_path}")
    logger.info(f"üìä –û—Ç—á–µ—Ç: {report_path}")
    logger.info("="*60)
    logger.info("‚úÖ –ü–†–û–ë–õ–ï–ú–ê –° –ü–£–°–¢–´–ú–ò –ê–ù–ù–û–¢–ê–¶–ò–Ø–ú–ò –†–ï–®–ï–ù–ê!")
    logger.info("üöÄ –¢–µ–ø–µ—Ä—å –º–æ–∂–Ω–æ –∑–∞–ø—É—Å–∫–∞—Ç—å –æ–±—É—á–µ–Ω–∏–µ:")
    logger.info(f"   python scripts/train_model.py --data {yaml_path}")
    logger.info("="*60)
    
    return True

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏"""
    parser = argparse.ArgumentParser(
        description="–ë—ã—Å—Ç—Ä–æ–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º—ã —Å –ø—É—Å—Ç—ã–º–∏ –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è–º–∏ YOLO",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
–ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
    # –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π –≤ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–º –¥–∞—Ç–∞—Å–µ—Ç–µ
    python fix_annotations.py --dataset data/processed/dataset
    
    # –° –∫–∞—Å—Ç–æ–º–Ω—ã–º –ø–æ—Ä–æ–≥–æ–º —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
    python fix_annotations.py --dataset data/processed/dataset --confidence 0.3
    
    # –î–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ split'–∞
    python fix_annotations.py --dataset data/processed/dataset --split train
        """
    )
    
    parser.add_argument(
        "--dataset",
        type=str,
        required=True,
        help="–ü—É—Ç—å –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞"
    )
    
    parser.add_argument(
        "--confidence",
        type=float,
        default=0.25,
        help="–ü–æ—Ä–æ–≥ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 0.25)"
    )
    
    parser.add_argument(
        "--split",
        type=str,
        choices=['train', 'val', 'test', 'all'],
        default='all',
        help="–ö–∞–∫–æ–π split –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: all)"
    )
    
    parser.add_argument(
        "--force",
        action="store_true",
        help="–ü–µ—Ä–µ–∑–∞–ø–∏—Å–∞—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏"
    )
    
    args = parser.parse_args()
    
    try:
        dataset_path = Path(args.dataset)
        
        if not dataset_path.exists():
            print(f"‚ùå –ü—É—Ç—å –∫ –¥–∞—Ç–∞—Å–µ—Ç—É –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {dataset_path}")
            sys.exit(1)
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞—Ç–∞—Å–µ—Ç–∞
        required_dirs = []
        if args.split == 'all':
            for split in ['train', 'val', 'test']:
                split_dir = dataset_path / split / 'images'
                if split_dir.exists():
                    required_dirs.append(split_dir)
        else:
            split_dir = dataset_path / args.split / 'images'
            if split_dir.exists():
                required_dirs.append(split_dir)
        
        if not required_dirs:
            print(f"‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω—ã –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏ –≤ {dataset_path}")
            print("–û–∂–∏–¥–∞–µ–º–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞:")
            print("  dataset/")
            print("  ‚îú‚îÄ‚îÄ train/images/")
            print("  ‚îú‚îÄ‚îÄ val/images/")
            print("  ‚îî‚îÄ‚îÄ test/images/")
            sys.exit(1)
        
        # –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –æ –ø–µ—Ä–µ–∑–∞–ø–∏—Å–∏
        if not args.force:
            labels_exist = any((dataset_path / split / 'labels').exists() and 
                             list((dataset_path / split / 'labels').glob('*.txt'))
                             for split in ['train', 'val', 'test'])
            
            if labels_exist:
                response = input("‚ö†Ô∏è –ù–∞–π–¥–µ–Ω—ã —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏. –ü–µ—Ä–µ–∑–∞–ø–∏—Å–∞—Ç—å? (y/N): ")
                if response.lower() not in ['y', 'yes', '–¥–∞']:
                    print("‚ùå –û–ø–µ—Ä–∞—Ü–∏—è –æ—Ç–º–µ–Ω–µ–Ω–∞")
                    sys.exit(0)
        
        # –ó–∞–ø—É—Å–∫ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è
        print("üöÄ –ù–∞—á–∏–Ω–∞–µ–º –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π...")
        success = fix_annotations(dataset_path, args.confidence)
        
        if success:
            print("\nüéâ –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
            print("üöÄ –ü—Ä–æ–±–ª–µ–º–∞ —Å WARNING Labels are missing —Ä–µ—à–µ–Ω–∞!")
            print("\n–¢–µ–ø–µ—Ä—å –º–æ–∂–Ω–æ –∑–∞–ø—É—Å–∫–∞—Ç—å –æ–±—É—á–µ–Ω–∏–µ –±–µ–∑ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–π:")
            print(f"   python scripts/train_model.py --data {dataset_path}/dataset.yaml")
            sys.exit(0)
        else:
            print("\n‚ùå –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π –∑–∞–≤–µ—Ä—à–∏–ª–æ—Å—å —Å –æ—à–∏–±–∫–∞–º–∏!")
            sys.exit(1)
            
    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è –û–ø–µ—Ä–∞—Ü–∏—è –ø—Ä–µ—Ä–≤–∞–Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
        sys.exit(1)
    except Exception as e:
        print(f"\n‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main()