# """
# –ë—ã—Å—Ç—Ä–æ–µ —Ä–µ—à–µ–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º—ã —Å –ø—É—Å—Ç—ã–º–∏ –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è–º–∏
# –°–æ–∑–¥–∞–µ—Ç –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ –¥–ª—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
# """

# import os
# import sys
# import logging
# import argparse
# from pathlib import Path
# from typing import List, Dict, Any
# import cv2
# import numpy as np
# import torch
# from ultralytics import YOLO
# from tqdm import tqdm
# import json
# import yaml
# import time

# def setup_logger():
#     """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–≥–µ—Ä–∞"""
#     logging.basicConfig(
#         level=logging.INFO,
#         format='%(asctime)s - %(levelname)s - %(message)s',
#         handlers=[
#             logging.StreamHandler(),
#             logging.FileHandler('annotation_fix.log', encoding='utf-8')
#         ]
#     )
#     return logging.getLogger(__name__)

# def detect_objects_professional(image_path: Path, models: List[YOLO], 
#                                restaurant_classes: List[str],
#                                confidence_threshold: float = 0.25) -> List[Dict]:
#     """
#     –ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–∞—è –¥–µ—Ç–µ–∫—Ü–∏—è –æ–±—ä–µ–∫—Ç–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∞–Ω—Å–∞–º–±–ª—è –º–æ–¥–µ–ª–µ–π
#     """
#     image = cv2.imread(str(image_path))
#     if image is None:
#         return []
    
#     height, width = image.shape[:2]
#     all_detections = []
    
#     # –î–µ—Ç–µ–∫—Ü–∏—è –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª—å—é
#     for model in models:
#         try:
#             results = model(image, conf=confidence_threshold, verbose=False)
            
#             if results and len(results) > 0:
#                 result = results[0]
                
#                 if result.boxes is not None and len(result.boxes) > 0:
#                     boxes = result.boxes.xyxy.cpu().numpy()
#                     confidences = result.boxes.conf.cpu().numpy()
#                     class_ids = result.boxes.cls.cpu().numpy().astype(int)
                    
#                     for box, conf, class_id in zip(boxes, confidences, class_ids):
#                         if class_id < len(result.names):
#                             class_name = result.names[class_id]
                            
#                             # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ —Ä–µ—Å—Ç–æ—Ä–∞–Ω–Ω—ã–º –∫–ª–∞—Å—Å–∞–º
#                             if class_name in restaurant_classes:
#                                 x1, y1, x2, y2 = box
                                
#                                 # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ —Ñ–æ—Ä–º–∞—Ç YOLO (–Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã)
#                                 x_center = (x1 + x2) / 2 / width
#                                 y_center = (y1 + y2) / 2 / height
#                                 w = (x2 - x1) / width
#                                 h = (y2 - y1) / height
                                
#                                 # –í–∞–ª–∏–¥–∞—Ü–∏—è –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç
#                                 if (0 <= x_center <= 1 and 0 <= y_center <= 1 and
#                                     0 < w <= 1 and 0 < h <= 1):
                                    
#                                     detection = {
#                                         'class_name': class_name,
#                                         'confidence': float(conf),
#                                         'bbox': [x_center, y_center, w, h]
#                                     }
#                                     all_detections.append(detection)
        
#         except Exception as e:
#             logging.warning(f"–û—à–∏–±–∫–∞ –¥–µ—Ç–µ–∫—Ü–∏–∏ —Å –º–æ–¥–µ–ª—å—é: {e}")
#             continue
    
#     # –ü—Ä–æ—Å—Ç–æ–µ —É–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –ø–æ IoU
#     final_detections = remove_duplicate_detections(all_detections)
    
#     return final_detections

# def remove_duplicate_detections(detections: List[Dict], iou_threshold: float = 0.6) -> List[Dict]:
#     """–£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏—Ä—É—é—â–∏—Ö—Å—è –¥–µ—Ç–µ–∫—Ü–∏–π"""
#     if not detections:
#         return []
    
#     # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
#     detections = sorted(detections, key=lambda x: x['confidence'], reverse=True)
    
#     filtered = []
#     for detection in detections:
#         is_duplicate = False
        
#         for existing in filtered:
#             if (detection['class_name'] == existing['class_name'] and
#                 calculate_iou(detection['bbox'], existing['bbox']) > iou_threshold):
#                 is_duplicate = True
#                 break
        
#         if not is_duplicate:
#             filtered.append(detection)
    
#     return filtered

# def calculate_iou(bbox1: List[float], bbox2: List[float]) -> float:
#     """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ IoU –º–µ–∂–¥—É –¥–≤—É–º—è bbox –≤ —Ñ–æ—Ä–º–∞—Ç–µ YOLO"""
#     def yolo_to_corners(bbox):
#         x_center, y_center, width, height = bbox
#         x1 = x_center - width / 2
#         y1 = y_center - height / 2
#         x2 = x_center + width / 2
#         y2 = y_center + height / 2
#         return x1, y1, x2, y2
    
#     x1_1, y1_1, x2_1, y2_1 = yolo_to_corners(bbox1)
#     x1_2, y1_2, x2_2, y2_2 = yolo_to_corners(bbox2)
    
#     # –ü–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ
#     x1_inter = max(x1_1, x1_2)
#     y1_inter = max(y1_1, y1_2)
#     x2_inter = min(x2_1, x2_2)
#     y2_inter = min(y2_1, y2_2)
    
#     if x2_inter <= x1_inter or y2_inter <= y1_inter:
#         return 0.0
    
#     intersection = (x2_inter - x1_inter) * (y2_inter - y1_inter)
    
#     # –ü–ª–æ—â–∞–¥–∏
#     area1 = (x2_1 - x1_1) * (y2_1 - y1_1)
#     area2 = (x2_2 - x1_2) * (y2_2 - y1_2)
    
#     union = area1 + area2 - intersection
    
#     return intersection / union if union > 0 else 0.0

# def create_class_mapping(restaurant_classes: List[str]) -> Dict[str, int]:
#     """–°–æ–∑–¥–∞–Ω–∏–µ –º–∞–ø–ø–∏–Ω–≥–∞ –∫–ª–∞—Å—Å–æ–≤"""
#     return {class_name: idx for idx, class_name in enumerate(restaurant_classes)}

# def save_yolo_annotation(detections: List[Dict], output_path: Path, class_mapping: Dict[str, int]):
#     """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ –≤ —Ñ–æ—Ä–º–∞—Ç–µ YOLO"""
#     with open(output_path, 'w', encoding='utf-8') as f:
#         for detection in detections:
#             class_name = detection['class_name']
#             if class_name in class_mapping:
#                 class_id = class_mapping[class_name]
#                 bbox = detection['bbox']
                
#                 # –§–æ—Ä–º–∞—Ç YOLO: class_id x_center y_center width height
#                 line = f"{class_id} {bbox[0]:.6f} {bbox[1]:.6f} {bbox[2]:.6f} {bbox[3]:.6f}\n"
#                 f.write(line)

# def create_dataset_yaml(dataset_path: Path, class_mapping: Dict[str, int]):
#     """–°–æ–∑–¥–∞–Ω–∏–µ dataset.yaml –¥–ª—è YOLO"""
#     yaml_content = {
#         'path': str(dataset_path.absolute()),
#         'train': 'train/images',
#         'val': 'val/images',
#         'test': 'test/images',
#         'nc': len(class_mapping),
#         'names': list(class_mapping.keys())
#     }
    
#     yaml_path = dataset_path / 'dataset.yaml'
#     with open(yaml_path, 'w', encoding='utf-8') as f:
#         yaml.dump(yaml_content, f, default_flow_style=False, allow_unicode=True)
    
#     return yaml_path

# def fix_annotations(dataset_dir: Path, confidence_threshold: float = 0.25):
#     """
#     –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π
#     """
#     logger = setup_logger()
#     logger.info("üöÄ –ó–∞–ø—É—Å–∫ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–≥–æ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π")
    
#     # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Ä–µ—Å—Ç–æ—Ä–∞–Ω–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤
#     restaurant_classes = [
#         'person', 'chair', 'dining table', 'cup', 'fork', 'knife',
#         'spoon', 'bowl', 'bottle', 'wine glass', 'sandwich', 'pizza',
#         'cake', 'apple', 'banana', 'orange', 'cell phone', 'laptop', 'book'
#     ]
    
#     class_mapping = create_class_mapping(restaurant_classes)
    
#     # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π
#     logger.info("üß† –ó–∞–≥—Ä—É–∑–∫–∞ –∞–Ω—Å–∞–º–±–ª—è –º–æ–¥–µ–ª–µ–π YOLO...")
#     models = []
    
#     model_configs = [
#         ('yolo11n.pt', 0.15),
#         ('yolo11s.pt', 0.18),
#         ('yolo11m.pt', 0.22)
#     ]
    
#     device = 'cuda' if torch.cuda.is_available() else 'cpu'
#     logger.info(f"üíª –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")
    
#     for model_name, conf in model_configs:
#         try:
#             logger.info(f"üì• –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏: {model_name}")
#             model = YOLO(model_name)
#             model.to(device)
#             models.append(model)
#             logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å {model_name} —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
#         except Exception as e:
#             logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å {model_name}: {e}")
    
#     if not models:
#         logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –Ω–∏ –æ–¥–Ω–æ–π –º–æ–¥–µ–ª–∏!")
#         return False
    
#     # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞–∂–¥–æ–≥–æ split'–∞
#     splits = ['train', 'val', 'test']
#     total_processed = 0
#     total_annotations_created = 0
    
#     for split in splits:
#         split_images_dir = dataset_dir / split / 'images'
#         split_labels_dir = dataset_dir / split / 'labels'
        
#         if not split_images_dir.exists():
#             logger.warning(f"‚ö†Ô∏è –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è {split_images_dir} –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
#             continue
        
#         # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π
#         split_labels_dir.mkdir(parents=True, exist_ok=True)
        
#         # –ü–æ–∏—Å–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
#         image_extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.tiff']
#         image_files = []
#         for ext in image_extensions:
#             image_files.extend(list(split_images_dir.glob(f"*{ext}")))
#             image_files.extend(list(split_images_dir.glob(f"*{ext.upper()}")))
        
#         if not image_files:
#             logger.warning(f"‚ö†Ô∏è –ù–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ {split_images_dir}")
#             continue
        
#         logger.info(f"üñºÔ∏è –û–±—Ä–∞–±–æ—Ç–∫–∞ {split}: –Ω–∞–π–¥–µ–Ω–æ {len(image_files)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
        
#         split_annotations = 0
        
#         # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–æ–º
#         for image_path in tqdm(image_files, desc=f"–ê–Ω–Ω–æ—Ç–∞—Ü–∏—è {split}"):
#             try:
#                 # –î–µ—Ç–µ–∫—Ü–∏—è –æ–±—ä–µ–∫—Ç–æ–≤
#                 detections = detect_objects_professional(
#                     image_path, models, restaurant_classes, confidence_threshold
#                 )
                
#                 # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏
#                 annotation_path = split_labels_dir / f"{image_path.stem}.txt"
#                 save_yolo_annotation(detections, annotation_path, class_mapping)
                
#                 total_processed += 1
#                 split_annotations += len(detections)
                
#             except Exception as e:
#                 logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {image_path}: {e}")
                
#                 # –°–æ–∑–¥–∞–Ω–∏–µ –ø—É—Å—Ç–æ–≥–æ —Ñ–∞–π–ª–∞ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
#                 annotation_path = split_labels_dir / f"{image_path.stem}.txt"
#                 annotation_path.touch()
        
#         logger.info(f"‚úÖ {split} –∑–∞–≤–µ—Ä—à–µ–Ω: {len(image_files)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, {split_annotations} –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π")
#         total_annotations_created += split_annotations
    
#     # –°–æ–∑–¥–∞–Ω–∏–µ dataset.yaml
#     logger.info("üìÑ –°–æ–∑–¥–∞–Ω–∏–µ dataset.yaml...")
#     yaml_path = create_dataset_yaml(dataset_dir, class_mapping)
#     logger.info(f"‚úÖ –°–æ–∑–¥–∞–Ω —Ñ–∞–π–ª: {yaml_path}")
    
#     # –°–æ–∑–¥–∞–Ω–∏–µ –æ—Ç—á–µ—Ç–∞
#     report = {
#         'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
#         'total_images_processed': total_processed,
#         'total_annotations_created': total_annotations_created,
#         'models_used': [config[0] for config in model_configs if config[0] in [str(m.ckpt_path) for m in models]],
#         'confidence_threshold': confidence_threshold,
#         'restaurant_classes': restaurant_classes,
#         'class_mapping': class_mapping,
#         'dataset_yaml': str(yaml_path)
#     }
    
#     report_path = dataset_dir / 'annotation_fix_report.json'
#     with open(report_path, 'w', encoding='utf-8') as f:
#         json.dump(report, f, ensure_ascii=False, indent=2)
    
#     # –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
#     logger.info("\n" + "="*60)
#     logger.info("üìã –ò–¢–û–ì–ò –ò–°–ü–†–ê–í–õ–ï–ù–ò–Ø –ê–ù–ù–û–¢–ê–¶–ò–ô")
#     logger.info("="*60)
#     logger.info(f"üñºÔ∏è –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {total_processed}")
#     logger.info(f"üéØ –°–æ–∑–¥–∞–Ω–æ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π: {total_annotations_created}")
#     logger.info(f"üìÑ Dataset YAML: {yaml_path}")
#     logger.info(f"üìä –û—Ç—á–µ—Ç: {report_path}")
#     logger.info("="*60)
#     logger.info("‚úÖ –ü–†–û–ë–õ–ï–ú–ê –° –ü–£–°–¢–´–ú–ò –ê–ù–ù–û–¢–ê–¶–ò–Ø–ú–ò –†–ï–®–ï–ù–ê!")
#     logger.info("üöÄ –¢–µ–ø–µ—Ä—å –º–æ–∂–Ω–æ –∑–∞–ø—É—Å–∫–∞—Ç—å –æ–±—É—á–µ–Ω–∏–µ:")
#     logger.info(f"   python scripts/train_model.py --data {yaml_path}")
#     logger.info("="*60)
    
#     return True

# def main():
#     """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏"""
#     parser = argparse.ArgumentParser(
#         description="–ë—ã—Å—Ç—Ä–æ–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º—ã —Å –ø—É—Å—Ç—ã–º–∏ –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è–º–∏ YOLO",
#         formatter_class=argparse.RawDescriptionHelpFormatter,
#         epilog="""
# –ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
#     # –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π –≤ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–º –¥–∞—Ç–∞—Å–µ—Ç–µ
#     python fix_annotations.py --dataset data/processed/dataset
    
#     # –° –∫–∞—Å—Ç–æ–º–Ω—ã–º –ø–æ—Ä–æ–≥–æ–º —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
#     python fix_annotations.py --dataset data/processed/dataset --confidence 0.3
    
#     # –î–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ split'–∞
#     python fix_annotations.py --dataset data/processed/dataset --split train
#         """
#     )
    
#     parser.add_argument(
#         "--dataset",
#         type=str,
#         required=True,
#         help="–ü—É—Ç—å –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞"
#     )
    
#     parser.add_argument(
#         "--confidence",
#         type=float,
#         default=0.25,
#         help="–ü–æ—Ä–æ–≥ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 0.25)"
#     )
    
#     parser.add_argument(
#         "--split",
#         type=str,
#         choices=['train', 'val', 'test', 'all'],
#         default='all',
#         help="–ö–∞–∫–æ–π split –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: all)"
#     )
    
#     parser.add_argument(
#         "--force",
#         action="store_true",
#         help="–ü–µ—Ä–µ–∑–∞–ø–∏—Å–∞—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏"
#     )
    
#     args = parser.parse_args()
    
#     try:
#         dataset_path = Path(args.dataset)
        
#         if not dataset_path.exists():
#             print(f"‚ùå –ü—É—Ç—å –∫ –¥–∞—Ç–∞—Å–µ—Ç—É –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {dataset_path}")
#             sys.exit(1)
        
#         # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞—Ç–∞—Å–µ—Ç–∞
#         required_dirs = []
#         if args.split == 'all':
#             for split in ['train', 'val', 'test']:
#                 split_dir = dataset_path / split / 'images'
#                 if split_dir.exists():
#                     required_dirs.append(split_dir)
#         else:
#             split_dir = dataset_path / args.split / 'images'
#             if split_dir.exists():
#                 required_dirs.append(split_dir)
        
#         if not required_dirs:
#             print(f"‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω—ã –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏ –≤ {dataset_path}")
#             print("–û–∂–∏–¥–∞–µ–º–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞:")
#             print("  dataset/")
#             print("  ‚îú‚îÄ‚îÄ train/images/")
#             print("  ‚îú‚îÄ‚îÄ val/images/")
#             print("  ‚îî‚îÄ‚îÄ test/images/")
#             sys.exit(1)
        
#         # –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –æ –ø–µ—Ä–µ–∑–∞–ø–∏—Å–∏
#         if not args.force:
#             labels_exist = any((dataset_path / split / 'labels').exists() and 
#                              list((dataset_path / split / 'labels').glob('*.txt'))
#                              for split in ['train', 'val', 'test'])
            
#             if labels_exist:
#                 response = input("‚ö†Ô∏è –ù–∞–π–¥–µ–Ω—ã —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏. –ü–µ—Ä–µ–∑–∞–ø–∏—Å–∞—Ç—å? (y/N): ")
#                 if response.lower() not in ['y', 'yes', '–¥–∞']:
#                     print("‚ùå –û–ø–µ—Ä–∞—Ü–∏—è –æ—Ç–º–µ–Ω–µ–Ω–∞")
#                     sys.exit(0)
        
#         # –ó–∞–ø—É—Å–∫ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è
#         print("üöÄ –ù–∞—á–∏–Ω–∞–µ–º –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π...")
#         success = fix_annotations(dataset_path, args.confidence)
        
#         if success:
#             print("\nüéâ –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
#             print("üöÄ –ü—Ä–æ–±–ª–µ–º–∞ —Å WARNING Labels are missing —Ä–µ—à–µ–Ω–∞!")
#             print("\n–¢–µ–ø–µ—Ä—å –º–æ–∂–Ω–æ –∑–∞–ø—É—Å–∫–∞—Ç—å –æ–±—É—á–µ–Ω–∏–µ –±–µ–∑ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–π:")
#             print(f"   python scripts/train_model.py --data {dataset_path}/dataset.yaml")
#             sys.exit(0)
#         else:
#             print("\n‚ùå –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π –∑–∞–≤–µ—Ä—à–∏–ª–æ—Å—å —Å –æ—à–∏–±–∫–∞–º–∏!")
#             sys.exit(1)
            
#     except KeyboardInterrupt:
#         print("\n‚ö†Ô∏è –û–ø–µ—Ä–∞—Ü–∏—è –ø—Ä–µ—Ä–≤–∞–Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
#         sys.exit(1)
#     except Exception as e:
#         print(f"\n‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
#         import traceback
#         traceback.print_exc()
#         sys.exit(1)

# if __name__ == "__main__":
#     main()



"""
–ë—ã—Å—Ç—Ä–æ–µ —Ä–µ—à–µ–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º—ã —Å –ø—É—Å—Ç—ã–º–∏ –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è–º–∏
–°–æ–∑–¥–∞–µ—Ç –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ –¥–ª—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
"""

import os
import sys
import logging
import argparse
from pathlib import Path
from typing import List, Dict, Any, Tuple
import cv2
import numpy as np
import json
import yaml
import time
from tqdm import tqdm

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–Ω–µ–≤—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –ø—Ä–æ–µ–∫—Ç–∞ –≤ sys.path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.utils.logger import setup_logger

def check_dataset_structure(dataset_dir: Path) -> bool:
    """
    –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞—Ç–∞—Å–µ—Ç–∞ YOLO
    
    Args:
        dataset_dir: –ü—É—Ç—å –∫ –¥–∞—Ç–∞—Å–µ—Ç—É
        
    Returns:
        True –µ—Å–ª–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–∞
    """
    logger = setup_logger(__name__)
    
    required_dirs = [
        'train/images',
        'train/labels', 
        'val/images',
        'val/labels',
        'test/images',
        'test/labels'
    ]
    
    missing_dirs = []
    for dir_path in required_dirs:
        full_path = dataset_dir / dir_path
        if not full_path.exists():
            missing_dirs.append(dir_path)
    
    if missing_dirs:
        logger.error(f"‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω—ã –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏ –≤ {dataset_dir}")
        logger.error("–û–∂–∏–¥–∞–µ–º–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞:")
        logger.error("  dataset/")
        for req_dir in required_dirs:
            status = "‚ùå" if req_dir in missing_dirs else "‚úÖ"
            logger.error(f"  {status} {req_dir}/")
        return False
    
    return True

def create_dataset_structure(dataset_dir: Path):
    """
    –°–æ–∑–¥–∞–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞—Ç–∞—Å–µ—Ç–∞ –µ—Å–ª–∏ –æ–Ω–∞ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
    
    Args:
        dataset_dir: –ü—É—Ç—å –∫ –¥–∞—Ç–∞—Å–µ—Ç—É
    """
    logger = setup_logger(__name__)
    
    logger.info(f"üèóÔ∏è –°–æ–∑–¥–∞–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞—Ç–∞—Å–µ—Ç–∞ –≤ {dataset_dir}")
    
    required_dirs = [
        'train/images',
        'train/labels', 
        'val/images',
        'val/labels',
        'test/images',
        'test/labels'
    ]
    
    for dir_path in required_dirs:
        full_path = dataset_dir / dir_path
        full_path.mkdir(parents=True, exist_ok=True)
        logger.info(f"‚úÖ –°–æ–∑–¥–∞–Ω–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {dir_path}")
    
    logger.info("üéØ –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ —Å–æ–∑–¥–∞–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")

def get_available_yolo_models() -> List[str]:
    """
    –ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö YOLO –º–æ–¥–µ–ª–µ–π
    
    Returns:
        –°–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
    """
    # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –º–æ–¥–µ–ª–∏ YOLOv8/v11
    standard_models = [
        'yolo11n.pt', 'yolo11s.pt', 'yolo11m.pt', 'yolo11l.pt', 'yolo11x.pt',
        'yolov8n.pt', 'yolov8s.pt', 'yolov8m.pt', 'yolov8l.pt', 'yolov8x.pt'
    ]
    
    available_models = []
    
    try:
        from ultralytics import YOLO
        
        for model_name in standard_models:
            try:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏
                model = YOLO(model_name)
                available_models.append(model_name)
            except Exception:
                continue
                
    except ImportError:
        # –ï—Å–ª–∏ ultralytics –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –±–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏
        available_models = ['yolo11n.pt', 'yolov8n.pt']
    
    return available_models

def detect_objects_with_yolo(image_path: Path, 
                           model_names: List[str],
                           restaurant_classes: List[str],
                           confidence_threshold: float = 0.25) -> List[Dict]:
    """
    –ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–∞—è –¥–µ—Ç–µ–∫—Ü–∏—è –æ–±—ä–µ–∫—Ç–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∞–Ω—Å–∞–º–±–ª—è YOLO –º–æ–¥–µ–ª–µ–π
    
    Args:
        image_path: –ü—É—Ç—å –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é
        model_names: –°–ø–∏—Å–æ–∫ –∏–º–µ–Ω –º–æ–¥–µ–ª–µ–π –¥–ª—è –∞–Ω—Å–∞–º–±–ª—è
        restaurant_classes: –¶–µ–ª–µ–≤—ã–µ –∫–ª–∞—Å—Å—ã –¥–ª—è —Ä–µ—Å—Ç–æ—Ä–∞–Ω–Ω–æ–π —Å—Ä–µ–¥—ã
        confidence_threshold: –ü–æ—Ä–æ–≥ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
        
    Returns:
        –°–ø–∏—Å–æ–∫ –¥–µ—Ç–µ–∫—Ü–∏–π –≤ —Ñ–æ—Ä–º–∞—Ç–µ YOLO
    """
    try:
        from ultralytics import YOLO
    except ImportError:
        return []
    
    logger = setup_logger(__name__)
    
    image = cv2.imread(str(image_path))
    if image is None:
        return []
    
    height, width = image.shape[:2]
    all_detections = []
    
    # –î–µ—Ç–µ–∫—Ü–∏—è –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª—å—é –∏–∑ –∞–Ω—Å–∞–º–±–ª—è
    for model_name in model_names:
        try:
            model = YOLO(model_name)
            results = model(image, conf=confidence_threshold, verbose=False)
            
            if results and len(results) > 0:
                result = results[0]
                
                if result.boxes is not None and len(result.boxes) > 0:
                    boxes = result.boxes.xyxy.cpu().numpy()
                    confidences = result.boxes.conf.cpu().numpy()
                    class_ids = result.boxes.cls.cpu().numpy().astype(int)
                    
                    for box, conf, class_id in zip(boxes, confidences, class_ids):
                        if class_id < len(result.names):
                            class_name = result.names[class_id]
                            
                            # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ —Ä–µ—Å—Ç–æ—Ä–∞–Ω–Ω—ã–º –∫–ª–∞—Å—Å–∞–º
                            if class_name in restaurant_classes:
                                x1, y1, x2, y2 = box
                                
                                # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ —Ñ–æ—Ä–º–∞—Ç YOLO (–Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã)
                                x_center = (x1 + x2) / 2 / width
                                y_center = (y1 + y2) / 2 / height
                                w = (x2 - x1) / width
                                h = (y2 - y1) / height
                                
                                # –í–∞–ª–∏–¥–∞—Ü–∏—è –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç
                                if (0 <= x_center <= 1 and 0 <= y_center <= 1 and
                                    0 < w <= 1 and 0 < h <= 1):
                                    
                                    detection = {
                                        'class_name': class_name,
                                        'class_id': restaurant_classes.index(class_name) if class_name in restaurant_classes else 0,
                                        'confidence': float(conf),
                                        'bbox': [x_center, y_center, w, h]
                                    }
                                    all_detections.append(detection)
        
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –¥–µ—Ç–µ–∫—Ü–∏–∏ —Å –º–æ–¥–µ–ª—å—é {model_name}: {e}")
            continue
    
    # –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
    final_detections = remove_duplicate_detections(all_detections)
    
    return final_detections

def remove_duplicate_detections(detections: List[Dict], iou_threshold: float = 0.6) -> List[Dict]:
    """
    –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏—Ä—É—é—â–∏—Ö—Å—è –¥–µ—Ç–µ–∫—Ü–∏–π –ø–æ IoU
    
    Args:
        detections: –°–ø–∏—Å–æ–∫ –¥–µ—Ç–µ–∫—Ü–∏–π
        iou_threshold: –ü–æ—Ä–æ–≥ IoU –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
        
    Returns:
        –§–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ –¥–µ—Ç–µ–∫—Ü–∏–π
    """
    if not detections:
        return []
    
    # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ (—É–±—ã–≤–∞–Ω–∏–µ)
    detections = sorted(detections, key=lambda x: x['confidence'], reverse=True)
    
    filtered = []
    for detection in detections:
        is_duplicate = False
        
        for existing in filtered:
            if (detection['class_name'] == existing['class_name'] and
                calculate_iou(detection['bbox'], existing['bbox']) > iou_threshold):
                is_duplicate = True
                break
        
        if not is_duplicate:
            filtered.append(detection)
    
    return filtered

def calculate_iou(bbox1: List[float], bbox2: List[float]) -> float:
    """
    –í—ã—á–∏—Å–ª–µ–Ω–∏–µ IoU –¥–ª—è bbox –≤ —Ñ–æ—Ä–º–∞—Ç–µ YOLO (x_center, y_center, width, height)
    
    Args:
        bbox1, bbox2: Bbox –≤ —Ñ–æ—Ä–º–∞—Ç–µ [x_center, y_center, width, height]
        
    Returns:
        IoU –∑–Ω–∞—á–µ–Ω–∏–µ
    """
    # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ —Ñ–æ—Ä–º–∞—Ç (x1, y1, x2, y2)
    def yolo_to_xyxy(bbox):
        x_center, y_center, width, height = bbox
        x1 = x_center - width / 2
        y1 = y_center - height / 2
        x2 = x_center + width / 2
        y2 = y_center + height / 2
        return [x1, y1, x2, y2]
    
    box1 = yolo_to_xyxy(bbox1)
    box2 = yolo_to_xyxy(bbox2)
    
    # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏—è
    x1 = max(box1[0], box2[0])
    y1 = max(box1[1], box2[1])
    x2 = min(box1[2], box2[2])
    y2 = min(box1[3], box2[3])
    
    if x2 <= x1 or y2 <= y1:
        return 0.0
    
    intersection = (x2 - x1) * (y2 - y1)
    
    # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –ø–ª–æ—â–∞–¥–µ–π
    area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])
    area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])
    
    union = area1 + area2 - intersection
    
    return intersection / union if union > 0 else 0.0

def save_yolo_annotation(detections: List[Dict], output_path: Path):
    """
    –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π –≤ —Ñ–æ—Ä–º–∞—Ç–µ YOLO
    
    Args:
        detections: –°–ø–∏—Å–æ–∫ –¥–µ—Ç–µ–∫—Ü–∏–π
        output_path: –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ñ–∞–π–ª–∞ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏
    """
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_path, 'w', encoding='utf-8') as f:
        for detection in detections:
            class_id = detection['class_id']
            x_center, y_center, width, height = detection['bbox']
            
            # –§–æ—Ä–º–∞—Ç YOLO: class_id x_center y_center width height
            f.write(f"{class_id} {x_center:.6f} {y_center:.6f} {width:.6f} {height:.6f}\n")

def process_dataset_split(split_dir: Path, 
                         restaurant_classes: List[str],
                         model_names: List[str],
                         confidence_threshold: float = 0.25,
                         use_auto_annotation: bool = False) -> Dict[str, int]:
    """
    –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–≥–æ split'–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ (train/val/test)
    
    Args:
        split_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è split'–∞
        restaurant_classes: –°–ø–∏—Å–æ–∫ —Ü–µ–ª–µ–≤—ã—Ö –∫–ª–∞—Å—Å–æ–≤
        model_names: –ú–æ–¥–µ–ª–∏ –¥–ª—è –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏
        confidence_threshold: –ü–æ—Ä–æ–≥ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
        use_auto_annotation: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫—É—é –∞–Ω–Ω–æ—Ç–∞—Ü–∏—é
        
    Returns:
        –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏
    """
    logger = setup_logger(__name__)
    
    images_dir = split_dir / "images"
    labels_dir = split_dir / "labels"
    
    if not images_dir.exists():
        logger.warning(f"‚ö†Ô∏è –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {images_dir}")
        return {'processed': 0, 'annotated': 0, 'errors': 0}
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π
    labels_dir.mkdir(parents=True, exist_ok=True)
    
    # –ü–æ–∏—Å–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
    image_extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.tiff']
    images = []
    for ext in image_extensions:
        images.extend(list(images_dir.glob(f"*{ext}")))
        images.extend(list(images_dir.glob(f"*{ext.upper()}")))
    
    if not images:
        logger.warning(f"‚ö†Ô∏è –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ {images_dir}")
        return {'processed': 0, 'annotated': 0, 'errors': 0}
    
    logger.info(f"üì∑ –ù–∞–π–¥–µ–Ω–æ {len(images)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ {split_dir.name}")
    
    stats = {'processed': 0, 'annotated': 0, 'errors': 0}
    
    for image_path in tqdm(images, desc=f"–û–±—Ä–∞–±–æ—Ç–∫–∞ {split_dir.name}"):
        try:
            label_filename = image_path.stem + ".txt"
            label_path = labels_dir / label_filename
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏
            if label_path.exists():
                # –ü—Ä–æ–≤–µ—Ä–∫–∞, –ø—É—Å—Ç–∞—è –ª–∏ –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è
                with open(label_path, 'r', encoding='utf-8') as f:
                    content = f.read().strip()
                
                if content and not use_auto_annotation:
                    # –ê–Ω–Ω–æ—Ç–∞—Ü–∏—è —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –∏ –Ω–µ –ø—É—Å—Ç–∞—è
                    stats['processed'] += 1
                    continue
            
            # –°–æ–∑–¥–∞–Ω–∏–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏
            detections = []
            
            if use_auto_annotation and model_names:
                # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è —Å –ø–æ–º–æ—â—å—é YOLO
                detections = detect_objects_with_yolo(
                    image_path=image_path,
                    model_names=model_names,
                    restaurant_classes=restaurant_classes,
                    confidence_threshold=confidence_threshold
                )
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ (–ø—É—Å—Ç–æ–π –∏–ª–∏ —Å –¥–µ—Ç–µ–∫—Ü–∏—è–º–∏)
            save_yolo_annotation(detections, label_path)
            
            stats['processed'] += 1
            if detections:
                stats['annotated'] += 1
                
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {image_path}: {e}")
            stats['errors'] += 1
            
            # –°–æ–∑–¥–∞–Ω–∏–µ –ø—É—Å—Ç–æ–π –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
            try:
                label_filename = image_path.stem + ".txt"
                label_path = labels_dir / label_filename
                with open(label_path, 'w', encoding='utf-8') as f:
                    pass
            except Exception:
                pass
    
    return stats

def create_or_update_dataset_yaml(dataset_dir: Path, restaurant_classes: List[str]):
    """
    –°–æ–∑–¥–∞–Ω–∏–µ –∏–ª–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ dataset.yaml —Ñ–∞–π–ª–∞
    
    Args:
        dataset_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞
        restaurant_classes: –°–ø–∏—Å–æ–∫ –∫–ª–∞—Å—Å–æ–≤ –¥–ª—è —Ä–µ—Å—Ç–æ—Ä–∞–Ω–Ω–æ–π —Å—Ä–µ–¥—ã
    """
    logger = setup_logger(__name__)
    
    yaml_config = {
        'path': str(dataset_dir.absolute()),
        'train': 'train/images',
        'val': 'val/images',
        'test': 'test/images',
        'nc': len(restaurant_classes),
        'names': restaurant_classes
    }
    
    yaml_path = dataset_dir / "dataset.yaml"
    
    try:
        with open(yaml_path, 'w', encoding='utf-8') as f:
            yaml.dump(yaml_config, f, default_flow_style=False, allow_unicode=True)
        
        logger.info(f"‚úÖ –°–æ–∑–¥–∞–Ω/–æ–±–Ω–æ–≤–ª–µ–Ω dataset.yaml: {yaml_path}")
        logger.info(f"üìã –ö–ª–∞—Å—Å—ã ({len(restaurant_classes)}): {', '.join(restaurant_classes)}")
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è dataset.yaml: {e}")

class AnnotationFixer:
    """–û—Å–Ω–æ–≤–Ω–æ–π –∫–ª–∞—Å—Å –¥–ª—è –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π"""
    
    def __init__(self, dataset_dir: Path, config: Dict[str, Any] = None):
        self.dataset_dir = Path(dataset_dir)
        self.config = config or self._get_default_config()
        self.logger = setup_logger(self.__class__.__name__)
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.stats = {
            'start_time': time.time(),
            'total_processed': 0,
            'total_annotated': 0,
            'total_errors': 0,
            'splits_processed': []
        }
    
    def _get_default_config(self) -> Dict[str, Any]:
        """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é"""
        return {
            'restaurant_classes': [
                'person', 'chair', 'dining_table', 'cup', 'bowl',
                'bottle', 'wine_glass', 'fork', 'knife', 'spoon',
                'plate', 'food', 'phone', 'book', 'laptop'
            ],
            'auto_annotation': {
                'enabled': True,
                'confidence_threshold': 0.25,
                'models': ['yolo11n.pt', 'yolov8n.pt'],
                'max_models': 2
            },
            'processing': {
                'create_structure_if_missing': True,
                'overwrite_existing': False,
                'splits_to_process': ['train', 'val', 'test']
            }
        }
    
    def run_fix_process(self):
        """–ó–∞–ø—É—Å–∫ –ø—Ä–æ—Ü–µ—Å—Å–∞ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π"""
        self.logger.info("üîß –ó–∞–ø—É—Å–∫ –ø—Ä–æ—Ü–µ—Å—Å–∞ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π")
        
        try:
            # 1. –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏ —Å–æ–∑–¥–∞–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
            if not check_dataset_structure(self.dataset_dir):
                if self.config['processing']['create_structure_if_missing']:
                    create_dataset_structure(self.dataset_dir)
                else:
                    raise ValueError("–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–∞")
            
            # 2. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –º–æ–¥–µ–ª–µ–π –¥–ª—è –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏
            available_models = []
            if self.config['auto_annotation']['enabled']:
                available_models = self._prepare_annotation_models()
            
            # 3. –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞–∂–¥–æ–≥–æ split'–∞
            splits_to_process = self.config['processing']['splits_to_process']
            
            for split_name in splits_to_process:
                split_dir = self.dataset_dir / split_name
                
                if not split_dir.exists():
                    self.logger.warning(f"‚ö†Ô∏è Split '{split_name}' –Ω–µ –Ω–∞–π–¥–µ–Ω, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
                    continue
                
                self.logger.info(f"üîÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ {split_name} split...")
                
                split_stats = process_dataset_split(
                    split_dir=split_dir,
                    restaurant_classes=self.config['restaurant_classes'],
                    model_names=available_models,
                    confidence_threshold=self.config['auto_annotation']['confidence_threshold'],
                    use_auto_annotation=self.config['auto_annotation']['enabled']
                )
                
                # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –æ–±—â–µ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
                self.stats['total_processed'] += split_stats['processed']
                self.stats['total_annotated'] += split_stats['annotated']
                self.stats['total_errors'] += split_stats['errors']
                self.stats['splits_processed'].append({
                    'split': split_name,
                    'stats': split_stats
                })
                
                self.logger.info(f"‚úÖ {split_name}: –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ {split_stats['processed']}, "
                               f"–∞–Ω–Ω–æ—Ç–∏—Ä–æ–≤–∞–Ω–æ {split_stats['annotated']}, "
                               f"–æ—à–∏–±–æ–∫ {split_stats['errors']}")
            
            # 4. –°–æ–∑–¥–∞–Ω–∏–µ/–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ dataset.yaml
            create_or_update_dataset_yaml(
                dataset_dir=self.dataset_dir,
                restaurant_classes=self.config['restaurant_classes']
            )
            
            # 5. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞
            self._generate_completion_report()
            
            self.logger.info("üéâ –ü—Ä–æ—Ü–µ—Å—Å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ!")
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è: {e}")
            self._generate_error_report(e)
            raise
    
    def _prepare_annotation_models(self) -> List[str]:
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –º–æ–¥–µ–ª–µ–π –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏"""
        self.logger.info("ü§ñ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –º–æ–¥–µ–ª–µ–π –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏...")
        
        try:
            available_models = get_available_yolo_models()
            
            if not available_models:
                self.logger.warning("‚ö†Ô∏è YOLO –º–æ–¥–µ–ª–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã, –±—É–¥—É—Ç —Å–æ–∑–¥–∞–Ω—ã –ø—É—Å—Ç—ã–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏")
                return []
            
            # –í—ã–±–æ—Ä –º–æ–¥–µ–ª–µ–π –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
            target_models = self.config['auto_annotation']['models']
            max_models = self.config['auto_annotation']['max_models']
            
            selected_models = []
            for model_name in target_models:
                if model_name in available_models and len(selected_models) < max_models:
                    selected_models.append(model_name)
            
            # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ —Ü–µ–ª–µ–≤—ã–µ –º–æ–¥–µ–ª–∏, –±–µ—Ä–µ–º –ª—é–±—ã–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ
            if not selected_models:
                selected_models = available_models[:max_models]
            
            self.logger.info(f"üéØ –í—ã–±—Ä–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏: {selected_models}")
            return selected_models
            
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –º–æ–¥–µ–ª–µ–π: {e}")
            return []
    
    def _generate_completion_report(self):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞ –æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏"""
        total_time = time.time() - self.stats['start_time']
        
        report = {
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
            'status': 'completed',
            'execution_time_seconds': round(total_time, 2),
            'dataset_directory': str(self.dataset_dir),
            'configuration': self.config,
            'statistics': {
                'total_processed': self.stats['total_processed'],
                'total_annotated': self.stats['total_annotated'],
                'total_errors': self.stats['total_errors'],
                'splits_processed': self.stats['splits_processed']
            },
            'output_files': {
                'dataset_yaml': str(self.dataset_dir / 'dataset.yaml'),
                'annotation_files_created': self.stats['total_processed']
            },
            'next_steps': [
                "–î–∞—Ç–∞—Å–µ—Ç –≥–æ—Ç–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è YOLO –º–æ–¥–µ–ª–∏",
                "–ó–∞–ø—É—Å—Ç–∏—Ç–µ train_model.py –¥–ª—è –Ω–∞—á–∞–ª–∞ –æ–±—É—á–µ–Ω–∏—è",
                "–ü—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –ø—Ä–æ–≤–µ—Ä—å—Ç–µ –∫–∞—á–µ—Å—Ç–≤–æ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π –≤—Ä—É—á–Ω—É—é"
            ]
        }
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–∞
        report_path = self.dataset_dir / "annotation_fix_report.json"
        try:
            with open(report_path, 'w', encoding='utf-8') as f:
                json.dump(report, f, ensure_ascii=False, indent=2)
            
            self.logger.info(f"üìã –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {report_path}")
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –æ—Ç—á–µ—Ç–∞: {e}")
        
        # –í—ã–≤–æ–¥ –∫—Ä–∞—Ç–∫–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        self.logger.info(f"‚è±Ô∏è –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {total_time:.2f} —Å–µ–∫—É–Ω–¥")
        self.logger.info(f"üìä –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Ñ–∞–π–ª–æ–≤: {self.stats['total_processed']}")
        self.logger.info(f"üéØ –°–æ–∑–¥–∞–Ω–æ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π: {self.stats['total_annotated']}")
    
    def _generate_error_report(self, error: Exception):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞ –æ–± –æ—à–∏–±–∫–µ"""
        total_time = time.time() - self.stats['start_time']
        
        error_report = {
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
            'status': 'failed',
            'execution_time_seconds': round(total_time, 2),
            'error': {
                'type': type(error).__name__,
                'message': str(error)
            },
            'statistics': self.stats,
            'troubleshooting': [
                "–ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–∞—Ç–∞—Å–µ—Ç–∞",
                "–£–±–µ–¥–∏—Ç–µ—Å—å –≤ –Ω–∞–ª–∏—á–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ –ø–∞–ø–∫–∞—Ö",
                "–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å YOLO –º–æ–¥–µ–ª–µ–π",
                "–£–±–µ–¥–∏—Ç–µ—Å—å –≤ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç–∏ –ø—É—Ç–µ–π"
            ]
        }
        
        error_report_path = self.dataset_dir / "annotation_fix_error.json"
        try:
            with open(error_report_path, 'w', encoding='utf-8') as f:
                json.dump(error_report, f, ensure_ascii=False, indent=2)
        except Exception:
            pass

def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    parser = argparse.ArgumentParser(
        description="–°–∫—Ä–∏–ø—Ç –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π –¥–ª—è YOLO –¥–∞—Ç–∞—Å–µ—Ç–∞",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
–ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:

1. –ë–∞–∑–æ–≤–æ–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ (—Å–æ–∑–¥–∞–Ω–∏–µ –ø—É—Å—Ç—ã—Ö –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π):
   python scripts/fix_annotations.py --dataset "data/processed/dataset"

2. –° –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –∞–Ω–Ω–æ—Ç–∞—Ü–∏–µ–π:
   python scripts/fix_annotations.py --dataset "data/processed/dataset" --auto-annotate

3. –°–æ–∑–¥–∞–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –∏ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π:
   python scripts/fix_annotations.py --dataset "data/processed/dataset" --create-structure --auto-annotate

4. –¢–æ–ª—å–∫–æ –¥–ª—è train split:
   python scripts/fix_annotations.py --dataset "data/processed/dataset" --splits train

–ß—Ç–æ –¥–µ–ª–∞–µ—Ç —Å–∫—Ä–∏–ø—Ç:
- –ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–∞—Ç–∞—Å–µ—Ç–∞ YOLO
- –°–æ–∑–¥–∞–µ—Ç –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
- –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ –¥–ª—è –≤—Å–µ—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
- –°–æ–∑–¥–∞–µ—Ç dataset.yaml –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
- –ü—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –¥–µ—Ç–∞–ª—å–Ω—ã–µ –æ—Ç—á–µ—Ç—ã
        """
    )
    
    parser.add_argument(
        '--dataset',
        type=str,
        required=True,
        help='–ü—É—Ç—å –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞'
    )
    
    parser.add_argument(
        '--auto-annotate',
        action='store_true',
        help='–í–∫–ª—é—á–∏—Ç—å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫—É—é –∞–Ω–Ω–æ—Ç–∞—Ü–∏—é —Å –ø–æ–º–æ—â—å—é YOLO –º–æ–¥–µ–ª–µ–π'
    )
    
    parser.add_argument(
        '--create-structure',
        action='store_true',
        help='–°–æ–∑–¥–∞—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–∞—Ç–∞—Å–µ—Ç–∞ –µ—Å–ª–∏ –æ–Ω–∞ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç'
    )
    
    parser.add_argument(
        '--confidence',
        type=float,
        default=0.25,
        help='–ü–æ—Ä–æ–≥ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 0.25)'
    )
    
    parser.add_argument(
        '--splits',
        nargs='+',
        default=['train', 'val', 'test'],
        help='–°–ø–∏—Å–æ–∫ splits –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: train val test)'
    )
    
    parser.add_argument(
        '--models',
        nargs='+',
        default=['yolo11n.pt', 'yolov8n.pt'],
        help='YOLO –º–æ–¥–µ–ª–∏ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏'
    )
    
    args = parser.parse_args()
    
    try:
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        config = {
            'restaurant_classes': [
                'person', 'chair', 'dining_table', 'cup', 'bowl',
                'bottle', 'wine_glass', 'fork', 'knife', 'spoon',
                'plate', 'food', 'phone', 'book', 'laptop'
            ],
            'auto_annotation': {
                'enabled': args.auto_annotate,
                'confidence_threshold': args.confidence,
                'models': args.models,
                'max_models': 2
            },
            'processing': {
                'create_structure_if_missing': args.create_structure,
                'overwrite_existing': False,
                'splits_to_process': args.splits
            }
        }
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –∏ –∑–∞–ø—É—Å–∫ —Ñ–∏–∫—Å–µ—Ä–∞
        fixer = AnnotationFixer(
            dataset_dir=Path(args.dataset),
            config=config
        )
        
        fixer.run_fix_process()
        
        print("\n" + "="*50)
        print("üéâ –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï –ê–ù–ù–û–¢–ê–¶–ò–ô –ó–ê–í–ï–†–®–ï–ù–û!")
        print("="*50)
        print(f"üìÅ –î–∞—Ç–∞—Å–µ—Ç: {args.dataset}")
        print(f"‚öôÔ∏è –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è: {args.dataset}/dataset.yaml")
        print(f"üìã –û—Ç—á–µ—Ç: {args.dataset}/annotation_fix_report.json")
        print("\nüöÄ –î–∞—Ç–∞—Å–µ—Ç –≥–æ—Ç–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏!")
        print("="*50)
        
    except Exception as e:
        print(f"\n‚ùå –û–®–ò–ë–ö–ê: {e}")
        print("üìã –ü—Ä–æ–≤–µ—Ä—å—Ç–µ error_report.json –¥–ª—è –¥–µ—Ç–∞–ª–µ–π")
        sys.exit(1)

if __name__ == "__main__":
    main()