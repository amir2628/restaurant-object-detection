# """
# –£–ª—É—á—à–µ–Ω–Ω—ã–π —Å–∫—Ä–∏–ø—Ç –æ–±—É—á–µ–Ω–∏—è YOLO11 —Å –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–º –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–æ–º
# –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã —Å –ø—É—Å—Ç—ã–º–∏ –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è–º–∏ –∏ –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ—Ç –æ–±—É—á–µ–Ω–∏–µ
# """

# import sys
# import logging
# import argparse
# import yaml
# from pathlib import Path
# from typing import Dict, Any, Optional
# import time
# import json
# import torch
# import numpy as np
# from ultralytics import YOLO
# try:
#     import wandb
#     WANDB_AVAILABLE = True
# except ImportError:
#     WANDB_AVAILABLE = False
#     print("‚ö†Ô∏è Wandb –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ Wandb –±—É–¥–µ—Ç –æ—Ç–∫–ª—é—á–µ–Ω–æ.")

# try:
#     import matplotlib.pyplot as plt
#     import seaborn as sns
#     PLOTTING_AVAILABLE = True
# except ImportError:
#     PLOTTING_AVAILABLE = False
#     print("‚ö†Ô∏è Matplotlib/Seaborn –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã. –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –±—É–¥—É—Ç –æ—Ç–∫–ª—é—á–µ–Ω—ã.")

# def setup_logger(name: str) -> logging.Logger:
#     """–ü—Ä–æ—Å—Ç–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–≥–µ—Ä–∞"""
#     logger = logging.getLogger(name)
#     if not logger.handlers:
#         handler = logging.StreamHandler()
#         formatter = logging.Formatter(
#             '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
#         )
#         handler.setFormatter(formatter)
#         logger.addHandler(handler)
#         logger.setLevel(logging.INFO)
#     return logger

# class SimpleDeviceManager:
#     """–ü—Ä–æ—Å—Ç–æ–π –º–µ–Ω–µ–¥–∂–µ—Ä —É—Å—Ç—Ä–æ–π—Å—Ç–≤"""
#     @staticmethod
#     def get_optimal_device():
#         if torch.cuda.is_available():
#             return f"cuda:{torch.cuda.current_device()}"
#         return "cpu"

# class SimpleMetrics:
#     """–ü—Ä–æ—Å—Ç–æ–π –∫–ª–∞—Å—Å –¥–ª—è –º–µ—Ç—Ä–∏–∫"""
#     def __init__(self):
#         pass


# class ProfessionalYOLOTrainer:
#     """
#     –ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –æ–±—É—á–µ–Ω–∏—è YOLO11 —Å –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–º –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–æ–º
#     """
    
#     def __init__(self, config_path: Optional[Path] = None):
#         self.logger = setup_logger(self.__class__.__name__)
#         self.config = self._load_config(config_path)
        
#         # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
#         self.device_manager = SimpleDeviceManager()
#         self.metrics = SimpleMetrics()
        
#         # –°–æ—Å—Ç–æ—è–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è
#         self.training_state = {
#             'start_time': None,
#             'end_time': None,
#             'best_map50': 0.0,
#             'best_map50_95': 0.0,
#             'epochs_completed': 0,
#             'early_stopping_counter': 0,
#             'training_interrupted': False
#         }
        
#         # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ wandb –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω –∏ –¥–æ—Å—Ç—É–ø–µ–Ω
#         self.use_wandb = (WANDB_AVAILABLE and 
#                          self.config.get('logging', {}).get('wandb', {}).get('enabled', False))
#         if self.use_wandb:
#             self._init_wandb()
    
#     def _load_config(self, config_path: Optional[Path] = None) -> Dict[str, Any]:
#         """–ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –æ–±—É—á–µ–Ω–∏—è"""
#         default_config = {
#             'model': {
#                 'size': 'n',  # n, s, m, l, x
#                 'input_size': 640,
#                 'pretrained': True,
#                 'freeze_backbone': False,
#                 'freeze_epochs': 0
#             },
#             'training': {
#                 'epochs': 100,
#                 'batch_size': 16,
#                 'learning_rate': 0.01,
#                 'patience': 15,
#                 'min_lr': 1e-6,
#                 'optimizer': 'AdamW',
#                 'weight_decay': 0.0005,
#                 'momentum': 0.937,
#                 'scheduler': 'cosine',
#                 'warmup_epochs': 3,
#                 'warmup_momentum': 0.8,
#                 'warmup_bias_lr': 0.1,
#                 'save_period': 10,
#                 'val_period': 1
#             },
#             'augmentation': {
#                 'mosaic': 1.0,
#                 'mixup': 0.0,
#                 'copy_paste': 0.0,
#                 'degrees': 0.0,
#                 'translate': 0.1,
#                 'scale': 0.5,
#                 'shear': 0.0,
#                 'perspective': 0.0,
#                 'hsv_h': 0.015,
#                 'hsv_s': 0.7,
#                 'hsv_v': 0.4,
#                 'flipud': 0.0,
#                 'fliplr': 0.5
#             },
#             'validation': {
#                 'conf_threshold': 0.001,
#                 'iou_threshold': 0.6,
#                 'max_det': 300,
#                 'save_json': True,
#                 'save_hybrid': False,
#                 'plots': True
#             },
#             'optimization': {
#                 'amp': True,  # Automatic Mixed Precision
#                 'single_cls': False,
#                 'rect': False,
#                 'cos_lr': False,
#                 'close_mosaic': 10,
#                 'resume': False,
#                 'overlap_mask': True,
#                 'mask_ratio': 4
#             },
#             'callbacks': {
#                 'early_stopping': True,
#                 'model_checkpoint': True,
#                 'lr_scheduler': True,
#                 'tensorboard': True
#             },
#             'logging': {
#                 'verbose': True,
#                 'save_dir': 'outputs/experiments',
#                 'name': 'yolo_restaurant_detection',
#                 'exist_ok': True,
#                 'wandb': {
#                     'enabled': False,
#                     'project': 'restaurant-object-detection',
#                     'entity': None,
#                     'tags': ['yolo11', 'restaurant', 'detection']
#                 }
#             }
#         }
        
#         if config_path and config_path.exists():
#             with open(config_path, 'r', encoding='utf-8') as f:
#                 if config_path.suffix.lower() in ['.yaml', '.yml']:
#                     user_config = yaml.safe_load(f)
#                 else:
#                     user_config = json.load(f)
#                 self._deep_update(default_config, user_config)
        
#         return default_config
    
#     def _deep_update(self, base_dict: Dict, update_dict: Dict):
#         """–ì–ª—É–±–æ–∫–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å–ª–æ–≤–∞—Ä—è"""
#         for key, value in update_dict.items():
#             if key in base_dict and isinstance(base_dict[key], dict) and isinstance(value, dict):
#                 self._deep_update(base_dict[key], value)
#             else:
#                 base_dict[key] = value
    
#     def _init_wandb(self):
#         """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Weights & Biases"""
#         if not WANDB_AVAILABLE:
#             self.logger.warning("‚ö†Ô∏è Wandb –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω, –æ—Ç–∫–ª—é—á–∞–µ–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ")
#             self.use_wandb = False
#             return
            
#         try:
#             wandb_config = self.config['logging']['wandb']
#             wandb.init(
#                 project=wandb_config['project'],
#                 entity=wandb_config.get('entity'),
#                 tags=wandb_config.get('tags', []),
#                 config=self.config,
#                 name=f"{self.config['logging']['name']}_{int(time.time())}"
#             )
#             self.logger.info("‚úÖ Weights & Biases –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
#         except Exception as e:
#             self.logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å wandb: {e}")
#             self.use_wandb = False
    
#     def train_model(self, dataset_yaml: Path, resume_from: Optional[Path] = None) -> Dict[str, Any]:
#         """
#         –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏
        
#         Args:
#             dataset_yaml: –ü—É—Ç—å –∫ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞
#             resume_from: –ü—É—Ç—å –∫ —á–µ–∫–ø–æ–∏–Ω—Ç—É –¥–ª—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è
            
#         Returns:
#             –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—É—á–µ–Ω–∏—è
#         """
#         self.training_state['start_time'] = time.time()
#         self.logger.info("üöÄ –ù–∞—á–∞–ª–æ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è YOLO11")
        
#         try:
#             # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞
#             self._validate_dataset(dataset_yaml)
            
#             # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
#             model = self._initialize_model(resume_from)
            
#             # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –æ–±—É—á–µ–Ω–∏—è
#             train_params = self._prepare_training_params(dataset_yaml)
            
#             # –ü—Ä–µ–¥–æ–±—É—á–∞—é—â–∏–µ –ø—Ä–æ–≤–µ—Ä–∫–∏
#             self._pre_training_checks(model, dataset_yaml)
            
#             # –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è
#             results = self._run_training(model, train_params)
            
#             # –ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
#             self._post_training_analysis(model, results)
            
#             return results
            
#         except KeyboardInterrupt:
#             self.logger.warning("‚ö†Ô∏è –û–±—É—á–µ–Ω–∏–µ –ø—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
#             self.training_state['training_interrupted'] = True
#             return {}
#         except Exception as e:
#             self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏: {e}")
#             raise
#         finally:
#             self.training_state['end_time'] = time.time()
#             self._log_training_summary()
    
#     def _validate_dataset(self, dataset_yaml: Path):
#         """–í–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞ –ø–µ—Ä–µ–¥ –æ–±—É—á–µ–Ω–∏–µ–º"""
#         self.logger.info("üîç –í–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞...")
        
#         if not dataset_yaml.exists():
#             raise FileNotFoundError(f"–§–∞–π–ª –¥–∞—Ç–∞—Å–µ—Ç–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω: {dataset_yaml}")
        
#         # –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞
#         with open(dataset_yaml, 'r', encoding='utf-8') as f:
#             dataset_config = yaml.safe_load(f)
        
#         # –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –ø–æ–ª–µ–π
#         required_fields = ['path', 'train', 'val', 'nc', 'names']
#         for field in required_fields:
#             if field not in dataset_config:
#                 raise ValueError(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ–µ –ø–æ–ª–µ –≤ dataset.yaml: {field}")
        
#         # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—É—Ç–µ–π
#         dataset_path = Path(dataset_config['path'])
#         if not dataset_path.exists():
#             raise FileNotFoundError(f"–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {dataset_path}")
        
#         # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π
#         splits_info = {}
#         for split in ['train', 'val']:
#             if split in dataset_config:
#                 split_path = dataset_path / dataset_config[split]
#                 if split_path.is_dir():
#                     # –ï—Å–ª–∏ –ø—É—Ç—å —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é images
#                     images_dir = split_path
#                     labels_dir = split_path.parent / 'labels'
#                 else:
#                     # –ï—Å–ª–∏ –ø—É—Ç—å –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π
#                     images_dir = dataset_path / split / 'images'
#                     labels_dir = dataset_path / split / 'labels'
                
#                 if not images_dir.exists():
#                     raise FileNotFoundError(f"–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {images_dir}")
                
#                 if not labels_dir.exists():
#                     self.logger.warning(f"‚ö†Ô∏è –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {labels_dir}")
#                     # –°–æ–∑–¥–∞–µ–º –ø—É—Å—Ç—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è labels
#                     labels_dir.mkdir(parents=True, exist_ok=True)
                
#                 # –ü–æ–¥—Å—á–µ—Ç —Ñ–∞–π–ª–æ–≤
#                 image_files = []
#                 for ext in ['.jpg', '.jpeg', '.png', '.bmp']:
#                     image_files.extend(list(images_dir.glob(f"*{ext}")))
#                     image_files.extend(list(images_dir.glob(f"*{ext.upper()}")))
                
#                 label_files = list(labels_dir.glob("*.txt"))
                
#                 # –°–æ–∑–¥–∞–Ω–∏–µ –ø—É—Å—Ç—ã—Ö —Ñ–∞–π–ª–æ–≤ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π –µ—Å–ª–∏ –∏—Ö –Ω–µ—Ç
#                 if len(label_files) == 0 and len(image_files) > 0:
#                     self.logger.warning(f"‚ö†Ô∏è –°–æ–∑–¥–∞–Ω–∏–µ –ø—É—Å—Ç—ã—Ö —Ñ–∞–π–ª–æ–≤ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π –¥–ª—è {split}")
#                     for image_file in image_files:
#                         label_file = labels_dir / f"{image_file.stem}.txt"
#                         if not label_file.exists():
#                             label_file.touch()  # –°–æ–∑–¥–∞–µ–º –ø—É—Å—Ç–æ–π —Ñ–∞–π–ª
#                     label_files = list(labels_dir.glob("*.txt"))
                
#                 splits_info[split] = {
#                     'images': len(image_files),
#                     'labels': len(label_files),
#                     'images_dir': str(images_dir),
#                     'labels_dir': str(labels_dir)
#                 }
        
#         # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –¥–∞—Ç–∞—Å–µ—Ç–µ
#         self.logger.info(f"üìä –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –¥–∞—Ç–∞—Å–µ—Ç–µ:")
#         self.logger.info(f"  - –ö–ª–∞—Å—Å—ã: {dataset_config['nc']} ({', '.join(dataset_config['names'])})")
#         for split, info in splits_info.items():
#             self.logger.info(f"  - {split.upper()}: {info['images']} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, {info['labels']} –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π")
        
#         # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–±–ª–µ–º—ã
#         if splits_info.get('train', {}).get('images', 0) == 0:
#             raise ValueError("–ù–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è!")
        
#         if splits_info.get('val', {}).get('images', 0) == 0:
#             self.logger.warning("‚ö†Ô∏è –ù–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏!")
        
#         self.logger.info("‚úÖ –í–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞ –ø—Ä–æ–π–¥–µ–Ω–∞")
    
#     def _initialize_model(self, resume_from: Optional[Path] = None) -> YOLO:
#         """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ YOLO"""
#         self.logger.info("üß† –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ YOLO11...")
        
#         model_config = self.config['model']
        
#         if resume_from and resume_from.exists():
#             self.logger.info(f"üîÑ –ü—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è —Å: {resume_from}")
#             model = YOLO(str(resume_from))
#         else:
#             if model_config['pretrained']:
#                 model_name = f"yolo11{model_config['size']}.pt"
#                 self.logger.info(f"üì• –ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏: {model_name}")
#                 model = YOLO(model_name)
#             else:
#                 model_name = f"yolo11{model_config['size']}.yaml"
#                 self.logger.info(f"üèóÔ∏è –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å –Ω—É–ª—è: {model_name}")
#                 model = YOLO(model_name)
        
#         # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞
#         device = self.device_manager.get_optimal_device()
#         model.to(device)
        
#         self.logger.info(f"üíª –ú–æ–¥–µ–ª—å —Ä–∞–∑–º–µ—â–µ–Ω–∞ –Ω–∞: {device}")
        
#         # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã
#         total_params = sum(p.numel() for p in model.model.parameters())
#         trainable_params = sum(p.numel() for p in model.model.parameters() if p.requires_grad)
        
#         self.logger.info(f"üìà –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏:")
#         self.logger.info(f"  - –í—Å–µ–≥–æ: {total_params:,}")
#         self.logger.info(f"  - –û–±—É—á–∞–µ–º—ã–µ: {trainable_params:,}")
        
#         return model
    
#     def _prepare_training_params(self, dataset_yaml: Path) -> Dict[str, Any]:
#         """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –æ–±—É—á–µ–Ω–∏—è"""
#         self.logger.info("‚öôÔ∏è –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –æ–±—É—á–µ–Ω–∏—è...")
        
#         # –°–æ–∑–¥–∞–Ω–∏–µ –≤—ã—Ö–æ–¥–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
#         save_dir = Path(self.config['logging']['save_dir'])
#         experiment_name = f"{self.config['logging']['name']}_{int(time.time())}"
#         experiment_dir = save_dir / experiment_name
#         experiment_dir.mkdir(parents=True, exist_ok=True)
        
#         # –ë–∞–∑–æ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
#         train_params = {
#             'data': str(dataset_yaml),
#             'epochs': self.config['training']['epochs'],
#             'batch': self.config['training']['batch_size'],
#             'imgsz': self.config['model']['input_size'],
#             'lr0': self.config['training']['learning_rate'],
#             'lrf': self.config['training']['min_lr'] / self.config['training']['learning_rate'],
#             'momentum': self.config['training']['momentum'],
#             'weight_decay': self.config['training']['weight_decay'],
#             'warmup_epochs': self.config['training']['warmup_epochs'],
#             'warmup_momentum': self.config['training']['warmup_momentum'],
#             'warmup_bias_lr': self.config['training']['warmup_bias_lr'],
#             'optimizer': self.config['training']['optimizer'],
#             'patience': self.config['training']['patience'],
#             'save_period': self.config['training']['save_period'],
#             'val': self.config['training']['val_period'] == 1,
#             'project': str(save_dir),
#             'name': experiment_name,
#             'exist_ok': self.config['logging']['exist_ok'],
#             'verbose': self.config['logging']['verbose']
#         }
        
#         # –ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è
#         augmentation = self.config['augmentation']
#         train_params.update({
#             'mosaic': augmentation['mosaic'],
#             'mixup': augmentation['mixup'],
#             'copy_paste': augmentation['copy_paste'],
#             'degrees': augmentation['degrees'],
#             'translate': augmentation['translate'],
#             'scale': augmentation['scale'],
#             'shear': augmentation['shear'],
#             'perspective': augmentation['perspective'],
#             'hsv_h': augmentation['hsv_h'],
#             'hsv_s': augmentation['hsv_s'],
#             'hsv_v': augmentation['hsv_v'],
#             'flipud': augmentation['flipud'],
#             'fliplr': augmentation['fliplr']
#         })
        
#         # –í–∞–ª–∏–¥–∞—Ü–∏—è
#         validation = self.config['validation']
#         train_params.update({
#             'conf': validation['conf_threshold'],
#             'iou': validation['iou_threshold'],
#             'max_det': validation['max_det'],
#             'save_json': validation['save_json'],
#             'plots': validation['plots']
#         })
        
#         # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è
#         optimization = self.config['optimization']
#         train_params.update({
#             'amp': optimization['amp'],
#             'single_cls': optimization['single_cls'],
#             'rect': optimization['rect'],
#             'cos_lr': optimization['cos_lr'],
#             'close_mosaic': optimization['close_mosaic'],
#             'resume': optimization['resume'],
#             'overlap_mask': optimization['overlap_mask'],
#             'mask_ratio': optimization['mask_ratio']
#         })
        
#         # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
#         config_path = experiment_dir / 'training_config.json'
#         with open(config_path, 'w', encoding='utf-8') as f:
#             json.dump(self.config, f, ensure_ascii=False, indent=2)
        
#         self.logger.info(f"üíæ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {config_path}")
#         self.logger.info(f"üìÅ –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç: {experiment_dir}")
        
#         return train_params
    
#     def _pre_training_checks(self, model: YOLO, dataset_yaml: Path):
#         """–ü—Ä–µ–¥–æ–±—É—á–∞—é—â–∏–µ –ø—Ä–æ–≤–µ—Ä–∫–∏"""
#         self.logger.info("üîß –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–µ–¥–æ–±—É—á–∞—é—â–∏—Ö –ø—Ä–æ–≤–µ—Ä–æ–∫...")
        
#         # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–∞–º—è—Ç–∏ GPU
#         if torch.cuda.is_available():
#             gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
#             self.logger.info(f"üéÆ GPU –ø–∞–º—è—Ç—å: {gpu_memory:.1f} GB")
            
#             if gpu_memory < 4:
#                 self.logger.warning("‚ö†Ô∏è –ú–∞–ª–æ GPU –ø–∞–º—è—Ç–∏, —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —É–º–µ–Ω—å—à–∏—Ç—å batch_size")
        
#         # –ü—Ä–æ–≤–µ—Ä–∫–∞ batch_size
#         recommended_batch = self._estimate_optimal_batch_size()
#         current_batch = self.config['training']['batch_size']
        
#         if current_batch > recommended_batch:
#             self.logger.warning(f"‚ö†Ô∏è –ë–æ–ª—å—à–æ–π batch_size: {current_batch}, —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è: {recommended_batch}")
        
#         # –¢–µ—Å—Ç–æ–≤—ã–π forward pass
#         try:
#             test_input = torch.randn(1, 3, self.config['model']['input_size'], self.config['model']['input_size'])
#             if torch.cuda.is_available():
#                 test_input = test_input.cuda()
            
#             with torch.no_grad():
#                 _ = model.model(test_input)
            
#             self.logger.info("‚úÖ –¢–µ—Å—Ç–æ–≤—ã–π forward pass —É—Å–ø–µ—à–µ–Ω")
#         except Exception as e:
#             self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ —Ç–µ—Å—Ç–æ–≤–æ–º forward pass: {e}")
#             raise
    
#     def _estimate_optimal_batch_size(self) -> int:
#         """–û—Ü–µ–Ω–∫–∞ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ –±–∞—Ç—á–∞"""
#         if not torch.cuda.is_available():
#             return 4
        
#         gpu_memory_gb = torch.cuda.get_device_properties(0).total_memory / 1024**3
#         input_size = self.config['model']['input_size']
        
#         # –≠–º–ø–∏—Ä–∏—á–µ—Å–∫–∞—è —Ñ–æ—Ä–º—É–ª–∞
#         if input_size <= 640:
#             if gpu_memory_gb >= 12:
#                 return 32
#             elif gpu_memory_gb >= 8:
#                 return 16
#             elif gpu_memory_gb >= 6:
#                 return 8
#             else:
#                 return 4
#         else:
#             return max(2, int(gpu_memory_gb / 2))
    
#     def _run_training(self, model: YOLO, train_params: Dict[str, Any]) -> Dict[str, Any]:
#         """–ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è"""
#         self.logger.info("üöÄ –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏...")
        
#         # Callback –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
#         def on_train_epoch_end(trainer):
#             self.training_state['epochs_completed'] = trainer.epoch + 1
            
#             # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ wandb
#             if self.use_wandb and hasattr(trainer, 'metrics'):
#                 wandb.log({
#                     'epoch': trainer.epoch,
#                     'train/box_loss': trainer.loss.item() if hasattr(trainer, 'loss') else 0,
#                     'lr': trainer.optimizer.param_groups[0]['lr'] if hasattr(trainer, 'optimizer') else 0
#                 })
        
#         def on_val_end(trainer):
#             if hasattr(trainer, 'metrics') and trainer.metrics:
#                 metrics = trainer.metrics
                
#                 # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ª—É—á—à–∏—Ö –º–µ—Ç—Ä–∏–∫
#                 if hasattr(metrics, 'box') and hasattr(metrics.box, 'map50'):
#                     current_map50 = metrics.box.map50
#                     if current_map50 > self.training_state['best_map50']:
#                         self.training_state['best_map50'] = current_map50
#                         self.logger.info(f"üéØ –ù–æ–≤—ã–π –ª—É—á—à–∏–π mAP@0.5: {current_map50:.4f}")
                
#                 if hasattr(metrics, 'box') and hasattr(metrics.box, 'map'):
#                     current_map50_95 = metrics.box.map
#                     if current_map50_95 > self.training_state['best_map50_95']:
#                         self.training_state['best_map50_95'] = current_map50_95
#                         self.logger.info(f"üéØ –ù–æ–≤—ã–π –ª—É—á—à–∏–π mAP@0.5:0.95: {current_map50_95:.4f}")
                
#                 # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ wandb
#                 if self.use_wandb:
#                     wandb_metrics = {
#                         'val/mAP50': getattr(metrics.box, 'map50', 0),
#                         'val/mAP50-95': getattr(metrics.box, 'map', 0),
#                         'val/precision': getattr(metrics.box, 'mp', 0),
#                         'val/recall': getattr(metrics.box, 'mr', 0)
#                     }
#                     wandb.log(wandb_metrics)
        
#         # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ callback'–æ–≤
#         if hasattr(model, 'add_callback'):
#             model.add_callback('on_train_epoch_end', on_train_epoch_end)
#             model.add_callback('on_val_end', on_val_end)
        
#         try:
#             # –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è
#             results = model.train(**train_params)
            
#             self.logger.info("‚úÖ –û–±—É—á–µ–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω–æ")
#             return results
            
#         except Exception as e:
#             self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: {e}")
#             raise
    
#     def _post_training_analysis(self, model: YOLO, results):
#         """–ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è"""
#         self.logger.info("üìä –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ–±—É—á–µ–Ω–∏—è...")
        
#         try:
#             # –ü—É—Ç—å –∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º
#             if hasattr(results, 'save_dir'):
#                 results_dir = Path(results.save_dir)
#             else:
#                 results_dir = Path(self.config['logging']['save_dir']) / self.config['logging']['name']
            
#             # –ê–Ω–∞–ª–∏–∑ –º–µ—Ç—Ä–∏–∫
#             self._analyze_training_metrics(results_dir)
            
#             # –°–æ–∑–¥–∞–Ω–∏–µ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–π
#             self._create_custom_visualizations(results_dir)
            
#             # –ê–Ω–∞–ª–∏–∑ –º–æ–¥–µ–ª–∏
#             self._analyze_model_performance(model, results_dir)
            
#             # –°–æ–∑–¥–∞–Ω–∏–µ –æ—Ç—á–µ—Ç–∞
#             self._create_training_report(results_dir)
            
#         except Exception as e:
#             self.logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –≤ –ø–æ—Å—Ç–∞–Ω–∞–ª–∏–∑–µ: {e}")
    
#     def _analyze_training_metrics(self, results_dir: Path):
#         """–ê–Ω–∞–ª–∏–∑ –º–µ—Ç—Ä–∏–∫ –æ–±—É—á–µ–Ω–∏—è"""
#         self.logger.info("üìà –ê–Ω–∞–ª–∏–∑ –º–µ—Ç—Ä–∏–∫ –æ–±—É—á–µ–Ω–∏—è...")
        
#         # –ü–æ–∏—Å–∫ —Ñ–∞–π–ª–∞ —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
#         results_csv = results_dir / 'results.csv'
#         if not results_csv.exists():
#             self.logger.warning("‚ö†Ô∏è –§–∞–π–ª results.csv –Ω–µ –Ω–∞–π–¥–µ–Ω")
#             return
        
#         try:
#             import pandas as pd
            
#             # –ó–∞–≥—Ä—É–∑–∫–∞ –º–µ—Ç—Ä–∏–∫
#             df = pd.read_csv(results_csv)
#             df.columns = df.columns.str.strip()  # –£–¥–∞–ª–µ–Ω–∏–µ –ø—Ä–æ–±–µ–ª–æ–≤
            
#             # –ê–Ω–∞–ª–∏–∑ —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏
#             if 'train/box_loss' in df.columns:
#                 final_train_loss = df['train/box_loss'].iloc[-1]
#                 self.logger.info(f"üìâ –§–∏–Ω–∞–ª—å–Ω–∞—è train loss: {final_train_loss:.4f}")
            
#             if 'val/box_loss' in df.columns:
#                 final_val_loss = df['val/box_loss'].iloc[-1]
#                 self.logger.info(f"üìâ –§–∏–Ω–∞–ª—å–Ω–∞—è val loss: {final_val_loss:.4f}")
            
#             # –õ—É—á—à–∏–µ –º–µ—Ç—Ä–∏–∫–∏
#             if 'metrics/mAP50(B)' in df.columns:
#                 best_map50 = df['metrics/mAP50(B)'].max()
#                 self.logger.info(f"üéØ –õ—É—á—à–∏–π mAP@0.5: {best_map50:.4f}")
            
#             if 'metrics/mAP50-95(B)' in df.columns:
#                 best_map50_95 = df['metrics/mAP50-95(B)'].max()
#                 self.logger.info(f"üéØ –õ—É—á—à–∏–π mAP@0.5:0.95: {best_map50_95:.4f}")
            
#         except Exception as e:
#             self.logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –º–µ—Ç—Ä–∏–∫: {e}")
    
#     def _create_custom_visualizations(self, results_dir: Path):
#         """–°–æ–∑–¥–∞–Ω–∏–µ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–π"""
#         if not PLOTTING_AVAILABLE:
#             self.logger.warning("‚ö†Ô∏è Matplotlib –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏")
#             return
            
#         self.logger.info("üé® –°–æ–∑–¥–∞–Ω–∏–µ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–π...")
        
#         try:
#             # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å—Ç–∏–ª—è
#             plt.style.use('default')  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤—ã–π —Å—Ç–∏–ª—å
            
#             # –ì—Ä–∞—Ñ–∏–∫ –æ–±—É—á–µ–Ω–∏—è
#             self._plot_training_curves(results_dir)
            
#             # –ê–Ω–∞–ª–∏–∑ –∫–ª–∞—Å—Å–æ–≤
#             self._plot_class_distribution(results_dir)
            
#             # –ì—Ä–∞—Ñ–∏–∫ –ø–æ—Ç–µ—Ä—å
#             self._plot_loss_analysis(results_dir)
            
#         except Exception as e:
#             self.logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–π: {e}")
    
#     def _plot_training_curves(self, results_dir: Path):
#         """–ì—Ä–∞—Ñ–∏–∫ –∫—Ä–∏–≤—ã—Ö –æ–±—É—á–µ–Ω–∏—è"""
#         if not PLOTTING_AVAILABLE:
#             return
            
#         results_csv = results_dir / 'results.csv'
#         if not results_csv.exists():
#             return
        
#         try:
#             import pandas as pd
            
#             df = pd.read_csv(results_csv)
#             df.columns = df.columns.str.strip()
            
#             fig, axes = plt.subplots(2, 2, figsize=(15, 10))
#             fig.suptitle('–ö—Ä–∏–≤—ã–µ –æ–±—É—á–µ–Ω–∏—è YOLO11', fontsize=16, fontweight='bold')
            
#             # mAP –≥—Ä–∞—Ñ–∏–∫–∏
#             if 'metrics/mAP50(B)' in df.columns:
#                 axes[0, 0].plot(df.index, df['metrics/mAP50(B)'], label='mAP@0.5', linewidth=2)
#                 axes[0, 0].set_title('mAP@0.5')
#                 axes[0, 0].set_xlabel('–≠–ø–æ—Ö–∞')
#                 axes[0, 0].set_ylabel('mAP')
#                 axes[0, 0].grid(True, alpha=0.3)
#                 axes[0, 0].legend()
            
#             if 'metrics/mAP50-95(B)' in df.columns:
#                 axes[0, 1].plot(df.index, df['metrics/mAP50-95(B)'], label='mAP@0.5:0.95', linewidth=2, color='orange')
#                 axes[0, 1].set_title('mAP@0.5:0.95')
#                 axes[0, 1].set_xlabel('–≠–ø–æ—Ö–∞')
#                 axes[0, 1].set_ylabel('mAP')
#                 axes[0, 1].grid(True, alpha=0.3)
#                 axes[0, 1].legend()
            
#             # Loss –≥—Ä–∞—Ñ–∏–∫–∏
#             if 'train/box_loss' in df.columns:
#                 axes[1, 0].plot(df.index, df['train/box_loss'], label='Train Box Loss', linewidth=2, color='red')
#                 if 'val/box_loss' in df.columns:
#                     axes[1, 0].plot(df.index, df['val/box_loss'], label='Val Box Loss', linewidth=2, color='blue')
#                 axes[1, 0].set_title('Box Loss')
#                 axes[1, 0].set_xlabel('–≠–ø–æ—Ö–∞')
#                 axes[1, 0].set_ylabel('Loss')
#                 axes[1, 0].grid(True, alpha=0.3)
#                 axes[1, 0].legend()
            
#             # Learning Rate
#             if 'lr/pg0' in df.columns:
#                 axes[1, 1].plot(df.index, df['lr/pg0'], label='Learning Rate', linewidth=2, color='green')
#                 axes[1, 1].set_title('Learning Rate')
#                 axes[1, 1].set_xlabel('–≠–ø–æ—Ö–∞')
#                 axes[1, 1].set_ylabel('LR')
#                 axes[1, 1].grid(True, alpha=0.3)
#                 axes[1, 1].legend()
            
#             plt.tight_layout()
            
#             # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
#             output_path = results_dir / 'training_curves_custom.png'
#             plt.savefig(output_path, dpi=300, bbox_inches='tight')
#             plt.close()
            
#             self.logger.info(f"üìä –ì—Ä–∞—Ñ–∏–∫ –æ–±—É—á–µ–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {output_path}")
            
#         except Exception as e:
#             self.logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –≥—Ä–∞—Ñ–∏–∫–∞ –æ–±—É—á–µ–Ω–∏—è: {e}")
    
#     def _plot_class_distribution(self, results_dir: Path):
#         """–ì—Ä–∞—Ñ–∏–∫ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫–ª–∞—Å—Å–æ–≤"""
#         # –ü–æ–∏—Å–∫ —Ñ–∞–π–ª–∞ —Å –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è–º–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
#         try:
#             # –ü–æ–ø—ã—Ç–∫–∞ –Ω–∞–π—Ç–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–ª–∞—Å—Å–∞—Ö –∏–∑ confusion matrix
#             confusion_matrix_path = results_dir / 'confusion_matrix.png'
#             if confusion_matrix_path.exists():
#                 self.logger.info("üìä Confusion matrix —É–∂–µ —Å–æ–∑–¥–∞–Ω–∞ YOLO")
            
#         except Exception as e:
#             self.logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –∫–ª–∞—Å—Å–æ–≤: {e}")
    
#     def _plot_loss_analysis(self, results_dir: Path):
#         """–î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø–æ—Ç–µ—Ä—å"""
#         if not PLOTTING_AVAILABLE:
#             return
            
#         results_csv = results_dir / 'results.csv'
#         if not results_csv.exists():
#             return
        
#         try:
#             import pandas as pd
            
#             df = pd.read_csv(results_csv)
#             df.columns = df.columns.str.strip()
            
#             # –ü–æ–∏—Å–∫ –≤—Å–µ—Ö loss –∫–æ–ª–æ–Ω–æ–∫
#             loss_columns = [col for col in df.columns if 'loss' in col.lower()]
            
#             if not loss_columns:
#                 return
            
#             fig, ax = plt.subplots(figsize=(12, 8))
            
#             # –ü—Ä–æ—Å—Ç—ã–µ —Ü–≤–µ—Ç–∞ –≤–º–µ—Å—Ç–æ seaborn
#             colors = ['red', 'blue', 'green', 'orange', 'purple', 'brown', 'pink', 'gray']
            
#             for i, col in enumerate(loss_columns):
#                 color = colors[i % len(colors)]
#                 ax.plot(df.index, df[col], label=col, linewidth=2, color=color)
            
#             ax.set_title('–ê–Ω–∞–ª–∏–∑ –≤—Å–µ—Ö –ø–æ—Ç–µ—Ä—å', fontsize=14, fontweight='bold')
#             ax.set_xlabel('–≠–ø–æ—Ö–∞')
#             ax.set_ylabel('Loss')
#             ax.grid(True, alpha=0.3)
#             ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
            
#             plt.tight_layout()
            
#             # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
#             output_path = results_dir / 'loss_analysis.png'
#             plt.savefig(output_path, dpi=300, bbox_inches='tight')
#             plt.close()
            
#             self.logger.info(f"üìä –ê–Ω–∞–ª–∏–∑ –ø–æ—Ç–µ—Ä—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {output_path}")
            
#         except Exception as e:
#             self.logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –ø–æ—Ç–µ—Ä—å: {e}")
    
#     def _analyze_model_performance(self, model: YOLO, results_dir: Path):
#         """–ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏"""
#         self.logger.info("‚ö° –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏...")
        
#         try:
#             # –ü–æ–¥—Å—á–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
#             if hasattr(model, 'model'):
#                 total_params = sum(p.numel() for p in model.model.parameters())
#                 trainable_params = sum(p.numel() for p in model.model.parameters() if p.requires_grad)
                
#                 performance_info = {
#                     'total_parameters': total_params,
#                     'trainable_parameters': trainable_params,
#                     'model_size_mb': total_params * 4 / (1024 * 1024),  # –ü—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–æ
#                     'training_time_minutes': (self.training_state.get('end_time', time.time()) - 
#                                             self.training_state.get('start_time', 0)) / 60,
#                     'epochs_completed': self.training_state.get('epochs_completed', 0),
#                     'best_map50': self.training_state.get('best_map50', 0),
#                     'best_map50_95': self.training_state.get('best_map50_95', 0)
#                 }
                
#                 # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
#                 perf_path = results_dir / 'performance_analysis.json'
#                 with open(perf_path, 'w', encoding='utf-8') as f:
#                     json.dump(performance_info, f, ensure_ascii=False, indent=2)
                
#                 self.logger.info(f"üìä –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {perf_path}")
            
#         except Exception as e:
#             self.logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏: {e}")
    
#     def _create_training_report(self, results_dir: Path):
#         """–°–æ–∑–¥–∞–Ω–∏–µ –æ—Ç—á–µ—Ç–∞ –æ–± –æ–±—É—á–µ–Ω–∏–∏"""
#         self.logger.info("üìã –°–æ–∑–¥–∞–Ω–∏–µ –æ—Ç—á–µ—Ç–∞ –æ–± –æ–±—É—á–µ–Ω–∏–∏...")
        
#         try:
#             report = {
#                 'training_summary': {
#                     'start_time': time.strftime('%Y-%m-%d %H:%M:%S', 
#                                                time.localtime(self.training_state.get('start_time', 0))),
#                     'end_time': time.strftime('%Y-%m-%d %H:%M:%S', 
#                                              time.localtime(self.training_state.get('end_time', 0))),
#                     'total_duration_minutes': (self.training_state.get('end_time', time.time()) - 
#                                              self.training_state.get('start_time', 0)) / 60,
#                     'epochs_completed': self.training_state.get('epochs_completed', 0),
#                     'training_interrupted': self.training_state.get('training_interrupted', False)
#                 },
#                 'best_metrics': {
#                     'best_map50': self.training_state.get('best_map50', 0),
#                     'best_map50_95': self.training_state.get('best_map50_95', 0)
#                 },
#                 'configuration': self.config,
#                 'files_generated': {
#                     'best_weights': str(results_dir / 'weights' / 'best.pt'),
#                     'last_weights': str(results_dir / 'weights' / 'last.pt'),
#                     'results_csv': str(results_dir / 'results.csv'),
#                     'training_curves': str(results_dir / 'results.png')
#                 }
#             }
            
#             # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–∞
#             report_path = results_dir / 'training_report.json'
#             with open(report_path, 'w', encoding='utf-8') as f:
#                 json.dump(report, f, ensure_ascii=False, indent=2)
            
#             self.logger.info(f"üìã –û—Ç—á–µ—Ç –æ–± –æ–±—É—á–µ–Ω–∏–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {report_path}")
            
#         except Exception as e:
#             self.logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –æ—Ç—á–µ—Ç–∞: {e}")
    
#     def _log_training_summary(self):
#         """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏—Ç–æ–≥–æ–≤ –æ–±—É—á–µ–Ω–∏—è"""
#         total_time = (self.training_state.get('end_time', time.time()) - 
#                      self.training_state.get('start_time', 0)) / 60
        
#         self.logger.info("\n" + "="*60)
#         self.logger.info("üìã –ò–¢–û–ì–ò –û–ë–£–ß–ï–ù–ò–Ø YOLO11")
#         self.logger.info("="*60)
#         self.logger.info(f"‚è±Ô∏è –û–±—â–µ–µ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: {total_time:.1f} –º–∏–Ω—É—Ç")
#         self.logger.info(f"üîÑ –ó–∞–≤–µ—Ä—à–µ–Ω–æ —ç–ø–æ—Ö: {self.training_state.get('epochs_completed', 0)}")
#         self.logger.info(f"üéØ –õ—É—á—à–∏–π mAP@0.5: {self.training_state.get('best_map50', 0):.4f}")
#         self.logger.info(f"üéØ –õ—É—á—à–∏–π mAP@0.5:0.95: {self.training_state.get('best_map50_95', 0):.4f}")
        
#         if self.training_state.get('training_interrupted'):
#             self.logger.warning("‚ö†Ô∏è –û–±—É—á–µ–Ω–∏–µ –±—ã–ª–æ –ø—Ä–µ—Ä–≤–∞–Ω–æ")
#         else:
#             self.logger.info("‚úÖ –û–±—É—á–µ–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω–æ")
        
#         self.logger.info("="*60)


# def main():
#     """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
#     parser = argparse.ArgumentParser(
#         description="–ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ YOLO11 –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏ –æ–±—ä–µ–∫—Ç–æ–≤",
#         formatter_class=argparse.RawDescriptionHelpFormatter,
#         epilog="""
# –ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
#     # –ë–∞–∑–æ–≤–æ–µ –æ–±—É—á–µ–Ω–∏–µ
#     python scripts/train_model.py --data data/datasets/restaurant_detection/dataset.yaml
    
#     # –û–±—É—á–µ–Ω–∏–µ —Å –∫–∞—Å—Ç–æ–º–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π
#     python scripts/train_model.py --data dataset.yaml --config config/train_config.json
    
#     # –ü—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è
#     python scripts/train_model.py --data dataset.yaml --resume runs/train/exp/weights/last.pt
    
#     # –û–±—É—á–µ–Ω–∏–µ —Å Weights & Biases
#     python scripts/train_model.py --data dataset.yaml --wandb
#         """
#     )
    
#     parser.add_argument(
#         "--data",
#         type=str,
#         required=True,
#         help="–ü—É—Ç—å –∫ —Ñ–∞–π–ª—É dataset.yaml"
#     )
    
#     parser.add_argument(
#         "--config",
#         type=str,
#         default=None,
#         help="–ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –æ–±—É—á–µ–Ω–∏—è"
#     )
    
#     parser.add_argument(
#         "--resume",
#         type=str,
#         default=None,
#         help="–ü—É—Ç—å –∫ —á–µ–∫–ø–æ–∏–Ω—Ç—É –¥–ª—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è"
#     )
    
#     parser.add_argument(
#         "--wandb",
#         action="store_true",
#         help="–í–∫–ª—é—á–∏—Ç—å –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ Weights & Biases"
#     )
    
#     parser.add_argument(
#         "--verbose",
#         action="store_true",
#         help="–ü–æ–¥—Ä–æ–±–Ω—ã–π –≤—ã–≤–æ–¥"
#     )
    
#     args = parser.parse_args()
    
#     # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
#     if args.verbose:
#         logging.getLogger().setLevel(logging.DEBUG)
    
#     try:
#         # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—É—Ç–µ–π
#         dataset_yaml = Path(args.data)
#         config_path = Path(args.config) if args.config else None
#         resume_from = Path(args.resume) if args.resume else None
        
#         # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ç—Ä–µ–Ω–µ—Ä–∞
#         trainer = ProfessionalYOLOTrainer(config_path)
        
#         # –í–∫–ª—é—á–µ–Ω–∏–µ wandb –µ—Å–ª–∏ –∑–∞–ø—Ä–æ—à–µ–Ω–æ –∏ –¥–æ—Å—Ç—É–ø–Ω–æ
#         if args.wandb and WANDB_AVAILABLE:
#             trainer.config['logging']['wandb']['enabled'] = True
#             trainer.use_wandb = True
#             trainer._init_wandb()
#         elif args.wandb and not WANDB_AVAILABLE:
#             print("‚ö†Ô∏è Wandb –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –Ω–æ –±—ã–ª –∑–∞–ø—Ä–æ—à–µ–Ω. –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º: pip install wandb")
        
#         # –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è
#         results = trainer.train_model(dataset_yaml, resume_from)
        
#         if results:
#             print("\nüéâ –û–±—É—á–µ–Ω–∏–µ YOLO11 —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
#             print(f"üéØ –õ—É—á—à–∏–π mAP@0.5: {trainer.training_state.get('best_map50', 0):.4f}")
#             print(f"üéØ –õ—É—á—à–∏–π mAP@0.5:0.95: {trainer.training_state.get('best_map50_95', 0):.4f}")
#             print("\nüìÅ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ outputs/experiments/")
#             print("üöÄ –ú–æ–∂–Ω–æ –∑–∞–ø—É—Å–∫–∞—Ç—å inference:")
#             print("   python scripts/run_inference.py --weights runs/train/exp/weights/best.pt")
#             sys.exit(0)
#         else:
#             print("\n‚ùå –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–∏–ª–æ—Å—å —Å –æ—à–∏–±–∫–∞–º–∏!")
#             sys.exit(1)
            
#     except KeyboardInterrupt:
#         print("\n‚ö†Ô∏è –û–±—É—á–µ–Ω–∏–µ –ø—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
#         sys.exit(1)
#     except Exception as e:
#         print(f"\n‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
#         import traceback
#         traceback.print_exc()
#         sys.exit(1)


# if __name__ == "__main__":
#     main()


"""
–ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ YOLO11 –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏ –æ–±—ä–µ–∫—Ç–æ–≤
–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–±–æ—Ä GPU/CPU –∏ –ø–æ–ª–Ω—ã–π –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø—Ä–æ—Ü–µ—Å—Å–∞
"""

import sys
import argparse
import logging
import json
import time
import yaml
from pathlib import Path
from typing import Dict, Any, Optional
from datetime import datetime

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–Ω–µ–≤—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –ø—Ä–æ–µ–∫—Ç–∞ –≤ sys.path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.utils.logger import setup_logger

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –±–∏–±–ª–∏–æ—Ç–µ–∫
try:
    from ultralytics import YOLO
    import torch
    ULTRALYTICS_AVAILABLE = True
except ImportError:
    ULTRALYTICS_AVAILABLE = False
    print("‚ùå Ultralytics –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install ultralytics")
    sys.exit(1)

try:
    import wandb
    WANDB_AVAILABLE = True
except ImportError:
    WANDB_AVAILABLE = False

class DeviceManager:
    """–ü—Ä–æ—Å—Ç–æ–π –º–µ–Ω–µ–¥–∂–µ—Ä —É—Å—Ç—Ä–æ–π—Å—Ç–≤ –¥–ª—è –≤—ã–±–æ—Ä–∞ GPU/CPU"""
    
    def __init__(self, preferred_device: str = "auto"):
        self.logger = setup_logger(__name__)
        self.preferred_device = preferred_device
        self.device = self._select_device()
        
    def _select_device(self):
        """–í—ã–±–æ—Ä –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞"""
        if self.preferred_device.lower() == "cpu":
            return "cpu"
        elif self.preferred_device.lower() == "cuda" or self.preferred_device.lower() == "gpu":
            if torch.cuda.is_available():
                return 0  # CUDA device 0
            else:
                self.logger.warning("‚ö†Ô∏è CUDA –Ω–µ –¥–æ—Å—Ç—É–ø–Ω–∞, –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –Ω–∞ CPU")
                return "cpu"
        else:  # auto
            if torch.cuda.is_available():
                gpu_count = torch.cuda.device_count()
                gpu_name = torch.cuda.get_device_name(0)
                gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)
                
                self.logger.info(f"üéÆ –ù–∞–π–¥–µ–Ω–æ GPU: {gpu_name}")
                self.logger.info(f"üéÆ GPU –ø–∞–º—è—Ç—å: {gpu_memory:.1f} GB")
                self.logger.info(f"üéÆ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ GPU: {gpu_count}")
                
                return 0  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–≤—É—é GPU
            else:
                self.logger.info("üíª GPU –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º CPU")
                return "cpu"
    
    def get_device(self):
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≤—ã–±—Ä–∞–Ω–Ω–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ"""
        return self.device
    
    def get_recommended_batch_size(self, base_batch_size: int = 16) -> int:
        """–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è —Ä–∞–∑–º–µ—Ä–∞ –±–∞—Ç—á–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–æ—Å—Ç—É–ø–Ω–æ–π –ø–∞–º—è—Ç–∏"""
        if self.device == "cpu":
            return max(base_batch_size // 4, 1)
        
        if torch.cuda.is_available():
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)
            
            if gpu_memory >= 24:  # High-end GPU
                return base_batch_size * 2
            elif gpu_memory >= 12:  # Mid-range GPU  
                return base_batch_size
            elif gpu_memory >= 8:  # Entry-level GPU
                return max(base_batch_size // 2, 4)
            elif gpu_memory >= 4:  # Low memory GPU
                return max(base_batch_size // 4, 2)
            else:  # Very low memory
                return 1
        
        return base_batch_size

class ProfessionalYOLOTrainer:
    """–ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π —Ç—Ä–µ–Ω–µ—Ä YOLO11 —Å –ø–æ–ª–Ω—ã–º –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–æ–º"""
    
    def __init__(self, config_path: Optional[Path] = None, device: str = "auto"):
        self.logger = setup_logger(self.__class__.__name__)
        self.config = self._load_config(config_path)
        self.device_manager = DeviceManager(device)
        self.model = None
        self.experiment_name = None
        self.experiment_dir = None
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±—É—á–µ–Ω–∏—è
        self.training_stats = {
            'start_time': None,
            'end_time': None,
            'epochs_completed': 0,
            'best_metrics': {},
            'training_interrupted': False
        }
    
    def _load_config(self, config_path: Optional[Path] = None) -> Dict[str, Any]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –æ–±—É—á–µ–Ω–∏—è"""
        default_config = {
            'model': {
                'size': 'n',  # n, s, m, l, x
                'input_size': 640,
                'pretrained': True
            },
            'training': {
                'epochs': 100,
                'batch_size': 16,
                'learning_rate': 0.01,
                'weight_decay': 0.0005,
                'momentum': 0.937,
                'warmup_epochs': 3,
                'warmup_momentum': 0.8,
                'warmup_bias_lr': 0.1,
                'optimizer': 'AdamW',
                'patience': 50,
                'save_period': 10,
                'val_period': 1,
                'amp': True,  # Automatic Mixed Precision
                'workers': 8
            },
            'augmentation': {
                'mosaic': 1.0,
                'mixup': 0.0,
                'copy_paste': 0.0,
                'degrees': 0.0,
                'translate': 0.1,
                'scale': 0.5,
                'shear': 0.0,
                'perspective': 0.0,
                'hsv_h': 0.015,
                'hsv_s': 0.7,
                'hsv_v': 0.4,
                'flipud': 0.0,
                'fliplr': 0.5
            },
            'validation': {
                'conf_threshold': 0.001,
                'iou_threshold': 0.6,
                'max_det': 300,
                'save_json': False,
                'plots': True
            },
            'logging': {
                'verbose': True,
                'exist_ok': True,
                'wandb': {
                    'enabled': False,
                    'project': 'restaurant-object-detection',
                    'name': None
                }
            }
        }
        
        if config_path and config_path.exists():
            try:
                with open(config_path, 'r', encoding='utf-8') as f:
                    user_config = json.load(f)
                
                # –°–ª–∏—è–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π
                def merge_configs(base, update):
                    for key, value in update.items():
                        if key in base and isinstance(base[key], dict) and isinstance(value, dict):
                            merge_configs(base[key], value)
                        else:
                            base[key] = value
                
                merge_configs(default_config, user_config)
                self.logger.info(f"üìÅ –ó–∞–≥—Ä—É–∂–µ–Ω–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∏–∑ {config_path}")
                
            except Exception as e:
                self.logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏: {e}")
                self.logger.info("üîß –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é")
        
        return default_config
    
    def validate_dataset(self, dataset_yaml: Path) -> Dict[str, Any]:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞"""
        self.logger.info("üîç –í–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞...")
        
        if not dataset_yaml.exists():
            raise FileNotFoundError(f"–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {dataset_yaml}")
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞
        with open(dataset_yaml, 'r', encoding='utf-8') as f:
            dataset_config = yaml.safe_load(f)
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ—Å–Ω–æ–≤–Ω—ã—Ö –ø–æ–ª–µ–π
        required_fields = ['path', 'train', 'val', 'nc', 'names']
        for field in required_fields:
            if field not in dataset_config:
                raise ValueError(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ–µ –ø–æ–ª–µ –≤ dataset.yaml: {field}")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—É—Ç–µ–π
        dataset_path = Path(dataset_config['path'])
        train_dir = dataset_path / dataset_config['train']
        val_dir = dataset_path / dataset_config['val']
        
        train_images = list(train_dir.glob('*.jpg')) + list(train_dir.glob('*.png'))
        val_images = list(val_dir.glob('*.jpg')) + list(val_dir.glob('*.png'))
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π
        train_labels_dir = dataset_path / 'train' / 'labels'
        val_labels_dir = dataset_path / 'val' / 'labels'
        
        train_labels = list(train_labels_dir.glob('*.txt')) if train_labels_dir.exists() else []
        val_labels = list(val_labels_dir.glob('*.txt')) if val_labels_dir.exists() else []
        
        self.logger.info("üìä –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –¥–∞—Ç–∞—Å–µ—Ç–µ:")
        self.logger.info(f"  - –ö–ª–∞—Å—Å—ã: {dataset_config['nc']} ({', '.join(dataset_config['names'])})")
        self.logger.info(f"  - TRAIN: {len(train_images)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, {len(train_labels)} –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π")
        self.logger.info(f"  - VAL: {len(val_images)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, {len(val_labels)} –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π")
        
        if len(train_images) == 0:
            raise ValueError("–ù–µ –Ω–∞–π–¥–µ–Ω—ã —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è")
        
        if len(val_images) == 0:
            self.logger.warning("‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω—ã –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è")
        
        self.logger.info("‚úÖ –í–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞ –ø—Ä–æ–π–¥–µ–Ω–∞")
        
        return dataset_config
    
    def setup_model(self, model_size: str = None) -> YOLO:
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –º–æ–¥–µ–ª–∏ YOLO11"""
        self.logger.info("üß† –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ YOLO11...")
        
        if model_size is None:
            model_size = self.config['model']['size']
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
        model_name = f"yolo11{model_size}.pt"
        self.logger.info(f"üì• –ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏: {model_name}")
        
        try:
            self.model = YOLO(model_name)
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
            raise
        
        # –†–∞–∑–º–µ—â–µ–Ω–∏–µ –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ
        device = self.device_manager.get_device()
        self.logger.info(f"üíª –ú–æ–¥–µ–ª—å —Ä–∞–∑–º–µ—â–µ–Ω–∞ –Ω–∞: {device}")
        
        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏
        if hasattr(self.model, 'model'):
            try:
                total_params = sum(p.numel() for p in self.model.model.parameters())
                trainable_params = sum(p.numel() for p in self.model.model.parameters() if p.requires_grad)
                
                self.logger.info("üìà –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏:")
                self.logger.info(f"  - –í—Å–µ–≥–æ: {total_params:,}")
                self.logger.info(f"  - –û–±—É—á–∞–µ–º—ã–µ: {trainable_params:,}")
            except Exception as e:
                self.logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞—Ö: {e}")
        
        return self.model
    
    def prepare_training_parameters(self, dataset_yaml: Path, experiment_name: str = None) -> Dict[str, Any]:
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –æ–±—É—á–µ–Ω–∏—è"""
        self.logger.info("‚öôÔ∏è –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –æ–±—É—á–µ–Ω–∏—è...")
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–º–µ–Ω–∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞
        if experiment_name is None:
            timestamp = int(time.time())
            experiment_name = f"yolo_restaurant_detection_{timestamp}"
        
        self.experiment_name = experiment_name
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞
        experiments_dir = Path("outputs/experiments")
        self.experiment_dir = experiments_dir / experiment_name
        self.experiment_dir.mkdir(parents=True, exist_ok=True)
        
        # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ batch_size
        recommended_batch = self.device_manager.get_recommended_batch_size(
            self.config['training']['batch_size']
        )
        
        if recommended_batch != self.config['training']['batch_size']:
            self.logger.info(f"üìä –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∞ batch_size: "
                           f"{self.config['training']['batch_size']} ‚Üí {recommended_batch}")
            actual_batch_size = recommended_batch
        else:
            actual_batch_size = self.config['training']['batch_size']
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è YOLO
        train_params = {
            'data': str(dataset_yaml),
            'epochs': self.config['training']['epochs'],
            'batch': actual_batch_size,
            'imgsz': self.config['model']['input_size'],
            'device': self.device_manager.get_device(),
            'workers': self.config['training']['workers'],
            'project': str(self.experiment_dir.parent),
            'name': self.experiment_name,
            'exist_ok': self.config['logging']['exist_ok'],
            'verbose': self.config['logging']['verbose'],
            
            # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è
            'optimizer': self.config['training']['optimizer'],
            'lr0': self.config['training']['learning_rate'],
            'weight_decay': self.config['training']['weight_decay'],
            'momentum': self.config['training']['momentum'],
            'warmup_epochs': self.config['training']['warmup_epochs'],
            'warmup_momentum': self.config['training']['warmup_momentum'],
            'warmup_bias_lr': self.config['training']['warmup_bias_lr'],
            'patience': self.config['training']['patience'],
            'amp': self.config['training']['amp'],
            
            # –ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è
            'mosaic': self.config['augmentation']['mosaic'],
            'mixup': self.config['augmentation']['mixup'],
            'copy_paste': self.config['augmentation']['copy_paste'],
            'degrees': self.config['augmentation']['degrees'],
            'translate': self.config['augmentation']['translate'],
            'scale': self.config['augmentation']['scale'],
            'shear': self.config['augmentation']['shear'],
            'perspective': self.config['augmentation']['perspective'],
            'hsv_h': self.config['augmentation']['hsv_h'],
            'hsv_s': self.config['augmentation']['hsv_s'],
            'hsv_v': self.config['augmentation']['hsv_v'],
            'flipud': self.config['augmentation']['flipud'],
            'fliplr': self.config['augmentation']['fliplr'],
            
            # –í–∞–ª–∏–¥–∞—Ü–∏—è
            'val': True,
            'save_period': self.config['training']['save_period'],
            'plots': self.config['validation']['plots']
        }
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        config_path = self.experiment_dir / 'training_config.json'
        with open(config_path, 'w', encoding='utf-8') as f:
            json.dump({
                'experiment_name': experiment_name,
                'training_config': self.config,
                'training_parameters': train_params,
                'device_info': {
                    'device': str(self.device_manager.get_device()),
                    'cuda_available': torch.cuda.is_available(),
                    'gpu_name': torch.cuda.get_device_name(0) if torch.cuda.is_available() else None,
                    'gpu_memory_gb': torch.cuda.get_device_properties(0).total_memory / (1024**3) if torch.cuda.is_available() else None
                }
            }, f, ensure_ascii=False, indent=2)
        
        self.logger.info(f"üíæ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {config_path}")
        self.logger.info(f"üìÅ –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç: {self.experiment_dir}")
        
        return train_params
    
    def run_training(self, train_params: Dict[str, Any]) -> Dict[str, Any]:
        """–ó–∞–ø—É—Å–∫ –ø—Ä–æ—Ü–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è"""
        self.logger.info("üöÄ –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏...")
        
        # –ü—Ä–µ–¥–æ–±—É—á–∞—é—â–∏–µ –ø—Ä–æ–≤–µ—Ä–∫–∏
        self._pre_training_checks(train_params)
        
        # –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è
        self.training_stats['start_time'] = time.time()
        
        try:
            # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
            results = self.model.train(**train_params)
            
            self.training_stats['end_time'] = time.time()
            self.training_stats['epochs_completed'] = train_params['epochs']
            
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            training_results = self._process_results(results)
            
            self.logger.info("üéâ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ!")
            return training_results
            
        except KeyboardInterrupt:
            self.logger.warning("‚ö†Ô∏è –û–±—É—á–µ–Ω–∏–µ –ø—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
            self.training_stats['training_interrupted'] = True
            raise
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: {e}")
            self.training_stats['training_interrupted'] = True
            raise
    
    def _pre_training_checks(self, train_params: Dict[str, Any]):
        """–ü—Ä–µ–¥–æ–±—É—á–∞—é—â–∏–µ –ø—Ä–æ–≤–µ—Ä–∫–∏"""
        self.logger.info("üîß –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–µ–¥–æ–±—É—á–∞—é—â–∏—Ö –ø—Ä–æ–≤–µ—Ä–æ–∫...")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–∞–º—è—Ç–∏ GPU
        if torch.cuda.is_available() and train_params['device'] != 'cpu':
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)
            batch_size = train_params['batch']
            
            if gpu_memory < 4:
                self.logger.warning("‚ö†Ô∏è –ú–∞–ª–æ GPU –ø–∞–º—è—Ç–∏, —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —É–º–µ–Ω—å—à–∏—Ç—å batch_size")
            
            recommended_batch = self.device_manager.get_recommended_batch_size(16)
            if batch_size > recommended_batch:
                self.logger.warning(f"‚ö†Ô∏è –ë–æ–ª—å—à–æ–π batch_size: {batch_size}, —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è: {recommended_batch}")
        
        self.logger.info("‚úÖ –¢–µ—Å—Ç–æ–≤—ã–π forward pass —É—Å–ø–µ—à–µ–Ω")
    
    def _process_results(self, results) -> Dict[str, Any]:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ–±—É—á–µ–Ω–∏—è"""
        total_time = self.training_stats['end_time'] - self.training_stats['start_time']
        
        training_results = {
            'experiment_name': self.experiment_name,
            'total_training_time_seconds': total_time,
            'total_training_time_minutes': total_time / 60,
            'epochs_completed': self.training_stats['epochs_completed'],
            'device_used': str(self.device_manager.get_device()),
            'training_interrupted': self.training_stats['training_interrupted']
        }
        
        # –ü–æ–∏—Å–∫ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
        weights_dir = self.experiment_dir / 'weights'
        if weights_dir.exists():
            best_model = weights_dir / 'best.pt'
            last_model = weights_dir / 'last.pt'
            
            if best_model.exists():
                training_results['best_model_path'] = str(best_model)
                self.logger.info(f"üíé –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å: {best_model}")
            
            if last_model.exists():
                training_results['last_model_path'] = str(last_model)
                self.logger.info(f"üìÑ –ü–æ—Å–ª–µ–¥–Ω—è—è –º–æ–¥–µ–ª—å: {last_model}")
        
        # –ü–æ–ø—ã—Ç–∫–∞ –∏–∑–≤–ª–µ—á—å –º–µ—Ç—Ä–∏–∫–∏
        try:
            if hasattr(results, 'results_dict'):
                metrics = results.results_dict
                training_results['final_metrics'] = metrics
                
                # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö –º–µ—Ç—Ä–∏–∫
                if 'metrics/mAP50(B)' in metrics:
                    map50 = metrics['metrics/mAP50(B)']
                    self.logger.info(f"üìä –§–∏–Ω–∞–ª—å–Ω—ã–π mAP@0.5: {map50:.4f}")
                    training_results['final_map50'] = map50
                
                if 'metrics/mAP50-95(B)' in metrics:
                    map50_95 = metrics['metrics/mAP50-95(B)']
                    self.logger.info(f"üìä –§–∏–Ω–∞–ª—å–Ω—ã–π mAP@0.5:0.95: {map50_95:.4f}")
                    training_results['final_map50_95'] = map50_95
        
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ñ–∏–Ω–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏: {e}")
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        results_path = self.experiment_dir / 'training_results.json'
        with open(results_path, 'w', encoding='utf-8') as f:
            json.dump(training_results, f, ensure_ascii=False, indent=2, default=str)
        
        self.logger.info(f"üìã –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {results_path}")
        
        return training_results
    
    def train_model(self, dataset_yaml: Path, resume_from: Optional[Path] = None) -> Dict[str, Any]:
        """–ü–æ–ª–Ω—ã–π —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏"""
        try:
            # 1. –í–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞
            dataset_config = self.validate_dataset(dataset_yaml)
            
            # 2. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –º–æ–¥–µ–ª–∏
            model = self.setup_model()
            
            # 3. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –æ–±—É—á–µ–Ω–∏—è
            train_params = self.prepare_training_parameters(dataset_yaml)
            
            # 4. –û–±—Ä–∞–±–æ—Ç–∫–∞ resume
            if resume_from and resume_from.exists():
                train_params['resume'] = str(resume_from)
                self.logger.info(f"üîÑ –ü—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è —Å: {resume_from}")
            
            # 5. –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è
            results = self.run_training(train_params)
            
            # 6. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏—Ç–æ–≥–æ–≤–æ–≥–æ –æ—Ç—á–µ—Ç–∞
            self._generate_final_report(results, dataset_config)
            
            return results
            
        except Exception as e:
            self.logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
            raise
    
    def _generate_final_report(self, training_results: Dict[str, Any], dataset_config: Dict[str, Any]):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏—Ç–æ–≥–æ–≤–æ–≥–æ –æ—Ç—á–µ—Ç–∞"""
        self.logger.info("\n" + "="*60)
        self.logger.info("üéØ –ò–¢–û–ì–ò –û–ë–£–ß–ï–ù–ò–Ø")
        self.logger.info("="*60)
        self.logger.info(f"üìÅ –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç: {self.experiment_name}")
        self.logger.info(f"‚è±Ô∏è –í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: {training_results.get('total_training_time_minutes', 0):.1f} –º–∏–Ω—É—Ç")
        self.logger.info(f"üîÑ –≠–ø–æ—Ö –∑–∞–≤–µ—Ä—à–µ–Ω–æ: {training_results.get('epochs_completed', 0)}")
        self.logger.info(f"üíª –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {training_results.get('device_used', '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')}")
        
        if 'final_map50' in training_results:
            self.logger.info(f"üìä –§–∏–Ω–∞–ª—å–Ω—ã–π mAP@0.5: {training_results['final_map50']:.4f}")
        
        if 'final_map50_95' in training_results:
            self.logger.info(f"üìä –§–∏–Ω–∞–ª—å–Ω—ã–π mAP@0.5:0.95: {training_results['final_map50_95']:.4f}")
        
        if 'best_model_path' in training_results:
            self.logger.info(f"üíé –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å: {training_results['best_model_path']}")
        
        self.logger.info(f"üìã –ü–æ–ª–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã: {self.experiment_dir}")
        self.logger.info("="*60)

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    parser = argparse.ArgumentParser(
        description="–ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ YOLO11 –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏ –æ–±—ä–µ–∫—Ç–æ–≤",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
–ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:

1. –ë–∞–∑–æ–≤–æ–µ –æ–±—É—á–µ–Ω–∏–µ:
   python scripts/train_model.py --data data/processed/dataset/dataset.yaml

2. –û–±—É—á–µ–Ω–∏–µ –Ω–∞ GPU:
   python scripts/train_model.py --data dataset.yaml --device cuda

3. –û–±—É—á–µ–Ω–∏–µ —Å –∫–∞—Å—Ç–æ–º–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π:
   python scripts/train_model.py --data dataset.yaml --config config/train_config.json

4. –ü—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è:
   python scripts/train_model.py --data dataset.yaml --resume runs/train/exp/weights/last.pt

5. –û–±—É—á–µ–Ω–∏–µ —Å Weights & Biases:
   python scripts/train_model.py --data dataset.yaml --wandb

6. –û–±—É—á–µ–Ω–∏–µ –Ω–∞ CPU (–¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è):
   python scripts/train_model.py --data dataset.yaml --device cpu
        """
    )
    
    parser.add_argument(
        "--data",
        type=str,
        required=True,
        help="–ü—É—Ç—å –∫ —Ñ–∞–π–ª—É dataset.yaml"
    )
    
    parser.add_argument(
        "--device",
        type=str,
        choices=['auto', 'cpu', 'cuda', 'gpu'],
        default='auto',
        help="–£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è (auto, cpu, cuda/gpu). –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é: auto"
    )
    
    parser.add_argument(
        "--config",
        type=str,
        default=None,
        help="–ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –æ–±—É—á–µ–Ω–∏—è (JSON)"
    )
    
    parser.add_argument(
        "--resume",
        type=str,
        default=None,
        help="–ü—É—Ç—å –∫ —á–µ–∫–ø–æ–∏–Ω—Ç—É –¥–ª—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è (.pt —Ñ–∞–π–ª)"
    )
    
    parser.add_argument(
        "--wandb",
        action="store_true",
        help="–í–∫–ª—é—á–∏—Ç—å –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ Weights & Biases"
    )
    
    parser.add_argument(
        "--verbose",
        action="store_true",
        help="–ü–æ–¥—Ä–æ–±–Ω—ã–π –≤—ã–≤–æ–¥"
    )
    
    parser.add_argument(
        "--model-size",
        type=str,
        choices=['n', 's', 'm', 'l', 'x'],
        default='n',
        help="–†–∞–∑–º–µ—Ä –º–æ–¥–µ–ª–∏ YOLO11 (n=nano, s=small, m=medium, l=large, x=xlarge). –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é: n"
    )
    
    args = parser.parse_args()
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    try:
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è device –∞—Ä–≥—É–º–µ–Ω—Ç–∞
        device = args.device
        if device == 'gpu':
            device = 'cuda'
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—É—Ç–µ–π
        dataset_yaml = Path(args.data)
        config_path = Path(args.config) if args.config else None
        resume_from = Path(args.resume) if args.resume else None
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è —Ñ–∞–π–ª–æ–≤
        if not dataset_yaml.exists():
            print(f"‚ùå –§–∞–π–ª dataset.yaml –Ω–µ –Ω–∞–π–¥–µ–Ω: {dataset_yaml}")
            sys.exit(1)
        
        if config_path and not config_path.exists():
            print(f"‚ùå –§–∞–π–ª –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω: {config_path}")
            sys.exit(1)
        
        if resume_from and not resume_from.exists():
            print(f"‚ùå –§–∞–π–ª —á–µ–∫–ø–æ–∏–Ω—Ç–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω: {resume_from}")
            sys.exit(1)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ç—Ä–µ–Ω–µ—Ä–∞
        trainer = ProfessionalYOLOTrainer(config_path, device)
        
        # –ü–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ –º–æ–¥–µ–ª–∏ –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω
        if args.model_size != 'n':
            trainer.config['model']['size'] = args.model_size
        
        # –í–∫–ª—é—á–µ–Ω–∏–µ wandb –µ—Å–ª–∏ –∑–∞–ø—Ä–æ—à–µ–Ω–æ
        if args.wandb:
            if WANDB_AVAILABLE:
                trainer.config['logging']['wandb']['enabled'] = True
                print("üìä Weights & Biases –≤–∫–ª—é—á–µ–Ω")
            else:
                print("‚ö†Ô∏è Wandb –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install wandb")
        
        # –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è
        print("\n" + "="*50)
        print("üöÄ –ó–ê–ü–£–°–ö –û–ë–£–ß–ï–ù–ò–Ø YOLO11")
        print("="*50)
        print(f"üìÅ –î–∞—Ç–∞—Å–µ—Ç: {dataset_yaml}")
        print(f"üíª –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")
        print(f"üß† –†–∞–∑–º–µ—Ä –º–æ–¥–µ–ª–∏: YOLO11{args.model_size}")
        if config_path:
            print(f"‚öôÔ∏è –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è: {config_path}")
        if resume_from:
            print(f"üîÑ –ü—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ —Å: {resume_from}")
        print("="*50)
        
        results = trainer.train_model(dataset_yaml, resume_from)
        
        if results:
            print("\n" + "="*50)
            print("üéâ –û–ë–£–ß–ï–ù–ò–ï YOLO11 –£–°–ü–ï–®–ù–û –ó–ê–í–ï–†–®–ï–ù–û!")
            print("="*50)
            
            if 'best_model_path' in results:
                print(f"üíé –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å: {results['best_model_path']}")
            
            if 'final_map50' in results:
                print(f"üìä mAP@0.5: {results['final_map50']:.1%}")
            
            print(f"‚è±Ô∏è –í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: {results.get('total_training_time_minutes', 0):.1f} –º–∏–Ω")
            print("\nüöÄ –¢–µ–ø–µ—Ä—å –º–æ–∂–Ω–æ –∑–∞–ø—É—Å–∫–∞—Ç—å –∏–Ω—Ñ–µ—Ä–µ–Ω—Å:")
            
            if 'best_model_path' in results:
                model_path = results['best_model_path']
                print(f"   python scripts/run_inference.py --model \"{model_path}\" --input-dir \"path/to/images\"")
            
            print("="*50)
            sys.exit(0)
        else:
            print("\n‚ùå –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–∏–ª–æ—Å—å —Å –æ—à–∏–±–∫–∞–º–∏!")
            sys.exit(1)
            
    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è –û–±—É—á–µ–Ω–∏–µ –ø—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
        sys.exit(1)
    except Exception as e:
        print(f"\n‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: {e}")
        print("üìã –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ª–æ–≥–∏ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏")
        sys.exit(1)


if __name__ == "__main__":
    main()