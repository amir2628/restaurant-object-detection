"""
–£–ª—É—á—à–µ–Ω–Ω—ã–π —Å–∫—Ä–∏–ø—Ç –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö —Å –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–π –∞–Ω–Ω–æ—Ç–∞—Ü–∏–µ–π
–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–æ–∑–¥–∞–µ—Ç –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è YOLO11
"""

import sys
import logging
import argparse
from pathlib import Path
from typing import Dict, Any
import time
import json
import shutil

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–Ω–µ–≤—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –ø—Ä–æ–µ–∫—Ç–∞ –≤ sys.path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.data.video_processor import VideoProcessor
from src.data.annotator import SmartAnnotator
from src.data.dataset_builder import DatasetBuilder
from src.utils.logger import setup_logger


class EnhancedDataPipeline:
    """
    –ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –∞–Ω–Ω–æ—Ç–∞—Ü–∏–µ–π
    """
    
    def __init__(self, config_path: Path = None):
        self.logger = setup_logger(self.__class__.__name__)
        self.config = self._load_config(config_path)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
        self.video_processor = VideoProcessor()
        self.annotator = SmartAnnotator()
        self.dataset_builder = DatasetBuilder()
        
        # –î–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        self.raw_data_dir = Path("data/raw")
        self.processed_dir = Path("data/processed")
        self.annotations_dir = Path("data/annotations")
        self.dataset_dir = Path("data/datasets")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.pipeline_stats = {
            'start_time': None,
            'end_time': None,
            'total_videos': 0,
            'total_frames': 0,
            'total_annotations': 0,
            'stages_completed': [],
            'errors': []
        }
    
    def _load_config(self, config_path: Path = None) -> Dict[str, Any]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –ø–∞–π–ø–ª–∞–π–Ω–∞"""
        default_config = {
            'video_processing': {
                'fps_extraction': 2.0,  # –ö–∞–¥—Ä–æ–≤ –≤ —Å–µ–∫—É–Ω–¥—É –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è
                'min_frame_interval': 0.5,  # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –∏–Ω—Ç–µ—Ä–≤–∞–ª –º–µ–∂–¥—É –∫–∞–¥—Ä–∞–º–∏
                'quality_threshold': 0.7,  # –ü–æ—Ä–æ–≥ –∫–∞—á–µ—Å—Ç–≤–∞ –∫–∞–¥—Ä–æ–≤
                'max_frames_per_video': 500,  # –ú–∞–∫—Å–∏–º—É–º –∫–∞–¥—Ä–æ–≤ –∏–∑ –æ–¥–Ω–æ–≥–æ –≤–∏–¥–µ–æ
                'frame_formats': ['.jpg', '.png'],
                'video_formats': ['.mp4', '.avi', '.mov', '.mkv', '.wmv']
            },
            'annotation': {
                'batch_size': 16,
                'num_workers': 4,
                'confidence_threshold': 0.25,
                'enable_quality_check': True,
                'auto_validation': True
            },
            'dataset': {
                'train_ratio': 0.7,
                'val_ratio': 0.2,
                'test_ratio': 0.1,
                'min_images_per_split': 10,
                'enable_augmentation': True,
                'stratify_by_class': True
            },
            'quality_control': {
                'min_detections_per_image': 0,  # –†–∞–∑—Ä–µ—à–∞–µ–º —Ñ–æ–Ω–æ–≤—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                'max_detections_per_image': 50,
                'validate_annotations': True,
                'generate_reports': True
            }
        }
        
        if config_path and config_path.exists():
            with open(config_path, 'r', encoding='utf-8') as f:
                user_config = json.load(f)
                # –ì–ª—É–±–æ–∫–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
                self._deep_update(default_config, user_config)
        
        return default_config
    
    def _deep_update(self, base_dict: Dict, update_dict: Dict):
        """–ì–ª—É–±–æ–∫–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å–ª–æ–≤–∞—Ä—è"""
        for key, value in update_dict.items():
            if key in base_dict and isinstance(base_dict[key], dict) and isinstance(value, dict):
                self._deep_update(base_dict[key], value)
            else:
                base_dict[key] = value
    
    def run_complete_pipeline(self, 
                            input_path: Path,
                            force_reprocess: bool = False,
                            skip_video_processing: bool = False,
                            skip_annotation: bool = False) -> bool:
        """
        –ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ –ø–∞–π–ø–ª–∞–π–Ω–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö
        
        Args:
            input_path: –ü—É—Ç—å –∫ –≤—Ö–æ–¥–Ω—ã–º –¥–∞–Ω–Ω—ã–º (–≤–∏–¥–µ–æ –∏–ª–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è)
            force_reprocess: –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –ø–µ—Ä–µ–æ–±—Ä–∞–±–æ—Ç–∫–∞
            skip_video_processing: –ü—Ä–æ–ø—É—Å—Ç–∏—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫—É –≤–∏–¥–µ–æ
            skip_annotation: –ü—Ä–æ–ø—É—Å—Ç–∏—Ç—å –∞–Ω–Ω–æ—Ç–∞—Ü–∏—é (–∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ)
            
        Returns:
            True –µ—Å–ª–∏ —É—Å–ø–µ—à–Ω–æ, False –∏–Ω–∞—á–µ
        """
        self.pipeline_stats['start_time'] = time.time()
        self.logger.info("üöÄ –ó–∞–ø—É—Å–∫ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–≥–æ –ø–∞–π–ø–ª–∞–π–Ω–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö")
        self.logger.info(f"üìÇ –í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ: {input_path}")
        
        try:
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π
            self._prepare_directories(force_reprocess)
            
            # –≠—Ç–∞–ø 1: –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤–∏–¥–µ–æ (–µ—Å–ª–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ)
            if not skip_video_processing:
                frames_dir = self._process_videos(input_path)
                if frames_dir is None:
                    # –ï—Å–ª–∏ –≤–∏–¥–µ–æ –Ω–µ—Ç, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –Ω–∞–ø—Ä—è–º—É—é
                    frames_dir = input_path if input_path.is_dir() else input_path.parent
            else:
                frames_dir = self.processed_dir / "frames"
            
            self.pipeline_stats['stages_completed'].append('video_processing')
            
            # –≠—Ç–∞–ø 2: –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è
            if not skip_annotation:
                self._create_professional_annotations(frames_dir)
                self.pipeline_stats['stages_completed'].append('annotation')
            
            # –≠—Ç–∞–ø 3: –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞
            self._build_dataset()
            self.pipeline_stats['stages_completed'].append('dataset_building')
            
            # –≠—Ç–∞–ø 4: –í–∞–ª–∏–¥–∞—Ü–∏—è –∫–∞—á–µ—Å—Ç–≤–∞
            self._validate_dataset_quality()
            self.pipeline_stats['stages_completed'].append('quality_validation')
            
            # –≠—Ç–∞–ø 5: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–æ–≤
            if self.config['quality_control']['generate_reports']:
                self._generate_pipeline_reports()
                self.pipeline_stats['stages_completed'].append('reporting')
            
            # –§–∏–Ω–∞–ª–∏–∑–∞—Ü–∏—è
            self._finalize_pipeline()
            
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ –ø–∞–π–ø–ª–∞–π–Ω–µ: {e}")
            self.pipeline_stats['errors'].append(str(e))
            return False
        
        finally:
            self.pipeline_stats['end_time'] = time.time()
            self._log_pipeline_summary()
    
    def _prepare_directories(self, force_reprocess: bool):
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π"""
        self.logger.info("üìÅ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π...")
        
        directories = [
            self.raw_data_dir,
            self.processed_dir,
            self.annotations_dir,
            self.dataset_dir
        ]
        
        for directory in directories:
            if force_reprocess and directory.exists():
                self.logger.info(f"üóëÔ∏è –û—á–∏—Å—Ç–∫–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏: {directory}")
                shutil.rmtree(directory)
            
            directory.mkdir(parents=True, exist_ok=True)
            self.logger.debug(f"‚úÖ –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –≥–æ—Ç–æ–≤–∞: {directory}")
    
    def _process_videos(self, input_path: Path) -> Path:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –≤–∏–¥–µ–æ—Ñ–∞–π–ª–æ–≤ –∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–∞–¥—Ä–æ–≤"""
        self.logger.info("üé¨ –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤–∏–¥–µ–æ—Ñ–∞–π–ª–æ–≤...")
        
        frames_output_dir = self.processed_dir / "frames"
        frames_output_dir.mkdir(parents=True, exist_ok=True)
        
        video_extensions = set(self.config['video_processing']['video_formats'])
        
        # –ü–æ–∏—Å–∫ –≤–∏–¥–µ–æ—Ñ–∞–π–ª–æ–≤
        video_files = []
        if input_path.is_file() and input_path.suffix.lower() in video_extensions:
            video_files = [input_path]
        elif input_path.is_dir():
            for ext in video_extensions:
                video_files.extend(list(input_path.glob(f"**/*{ext}")))
                video_files.extend(list(input_path.glob(f"**/*{ext.upper()}")))
        
        if not video_files:
            self.logger.info("üì∏ –í–∏–¥–µ–æ—Ñ–∞–π–ª—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã, –∏—â–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è...")
            return None
        
        self.logger.info(f"üé• –ù–∞–π–¥–µ–Ω–æ –≤–∏–¥–µ–æ—Ñ–∞–π–ª–æ–≤: {len(video_files)}")
        self.pipeline_stats['total_videos'] = len(video_files)
        
        total_frames = 0
        
        for video_file in video_files:
            self.logger.info(f"‚öôÔ∏è –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤–∏–¥–µ–æ: {video_file.name}")
            
            try:
                # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–∞–¥—Ä–æ–≤
                frames = self.video_processor.extract_frames(
                    video_path=video_file,
                    output_dir=frames_output_dir,
                    fps=self.config['video_processing']['fps_extraction'],
                    max_frames=self.config['video_processing']['max_frames_per_video']
                )
                
                extracted_count = len(frames) if frames else 0
                total_frames += extracted_count
                
                self.logger.info(f"‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–æ –∫–∞–¥—Ä–æ–≤ –∏–∑ {video_file.name}: {extracted_count}")
                
            except Exception as e:
                self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {video_file}: {e}")
                self.pipeline_stats['errors'].append(f"Video processing error: {video_file}: {e}")
        
        self.pipeline_stats['total_frames'] = total_frames
        self.logger.info(f"üé¨ –ó–∞–≤–µ—Ä—à–µ–Ω–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –≤–∏–¥–µ–æ. –í—Å–µ–≥–æ –∫–∞–¥—Ä–æ–≤: {total_frames}")
        
        return frames_output_dir
    
    def _create_professional_annotations(self, frames_dir: Path):
        """–°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã—Ö –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π"""
        self.logger.info("üß† –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã—Ö –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π...")
        
        # –ü–æ–∏—Å–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
        image_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.tiff'}
        image_files = []
        for ext in image_extensions:
            image_files.extend(list(frames_dir.glob(f"*{ext}")))
            image_files.extend(list(frames_dir.glob(f"*{ext.upper()}")))
        
        if not image_files:
            raise ValueError(f"–ù–µ –Ω–∞–π–¥–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ {frames_dir}")
        
        self.logger.info(f"üñºÔ∏è –ù–∞–π–¥–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏: {len(image_files)}")
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π –¥–ª—è train, val, test
        for split in ['train', 'val', 'test']:
            split_images_dir = self.dataset_dir / split / 'images'
            split_labels_dir = self.dataset_dir / split / 'labels'
            
            # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π
            split_images_dir.mkdir(parents=True, exist_ok=True)
            split_labels_dir.mkdir(parents=True, exist_ok=True)
            
            # –í—Ä–µ–º–µ–Ω–Ω–æ–µ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏
            # (–±—É–¥–µ—Ç –ø–µ—Ä–µ–æ—Ä–≥–∞–Ω–∏–∑–æ–≤–∞–Ω–æ –≤ dataset_builder)
            
        # –û—Å–Ω–æ–≤–Ω–∞—è –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è
        annotations_output_dir = self.annotations_dir / "auto_generated"
        annotations_output_dir.mkdir(parents=True, exist_ok=True)
        
        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∞–Ω–Ω–æ—Ç–∞—Ç–æ—Ä–∞
        annotation_config = {
            'models': {
                'yolo11n': {'weight': 0.3, 'confidence': 0.15},
                'yolo11s': {'weight': 0.4, 'confidence': 0.2},
                'yolo11m': {'weight': 0.3, 'confidence': 0.25}
            },
            'filtering': {
                'min_confidence': self.config['annotation']['confidence_threshold'],
                'min_area': 200,
                'max_area_ratio': 0.9,
                'min_aspect_ratio': 0.1,
                'max_aspect_ratio': 10.0,
                'edge_threshold': 10
            },
            'restaurant_classes': [
                'person', 'chair', 'dining table', 'cup', 'fork', 'knife',
                'spoon', 'bowl', 'bottle', 'wine glass', 'sandwich', 'pizza',
                'cake', 'apple', 'banana', 'orange', 'cell phone', 'laptop', 'book'
            ]
        }
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –∞–Ω–Ω–æ—Ç–∞—Ç–æ—Ä–∞
        config_path = annotations_output_dir / "annotator_config.json"
        with open(config_path, 'w', encoding='utf-8') as f:
            json.dump(annotation_config, f, ensure_ascii=False, indent=2)
        
        # –ó–∞–ø—É—Å–∫ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏
        annotation_stats = self.annotator.annotate_dataset(
            images_dir=frames_dir,
            output_dir=annotations_output_dir,
            batch_size=self.config['annotation']['batch_size'],
            num_workers=self.config['annotation']['num_workers']
        )
        
        self.pipeline_stats['total_annotations'] = annotation_stats.get('total_detections', 0)
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π
        if self.config['annotation']['auto_validation']:
            self._validate_annotations(annotations_output_dir, frames_dir)
        
        self.logger.info("‚úÖ –ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–∞—è –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
        self.logger.info(f"üìä –°–æ–∑–¥–∞–Ω–æ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π: {annotation_stats.get('processed_images', 0)}")
        self.logger.info(f"üéØ –í—Å–µ–≥–æ –¥–µ—Ç–µ–∫—Ü–∏–π: {annotation_stats.get('total_detections', 0)}")
    
    def _validate_annotations(self, annotations_dir: Path, images_dir: Path):
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π"""
        self.logger.info("üîç –í–∞–ª–∏–¥–∞—Ü–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π...")
        
        from src.data.annotator import AnnotationValidator
        
        validator = AnnotationValidator()
        
        # –ü–æ–ª—É—á–µ–Ω–∏–µ –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π
        annotation_files = list(annotations_dir.glob("*.txt"))
        
        validation_results = {
            'total_files': len(annotation_files),
            'valid_files': 0,
            'invalid_files': 0,
            'issues': []
        }
        
        for annotation_file in annotation_files:
            # –ü–æ–∏—Å–∫ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            image_file = None
            for ext in ['.jpg', '.jpeg', '.png', '.bmp']:
                potential_image = images_dir / f"{annotation_file.stem}{ext}"
                if potential_image.exists():
                    image_file = potential_image
                    break
            
            # –í–∞–ª–∏–¥–∞—Ü–∏—è
            result = validator.validate_annotation_file(annotation_file, image_file)
            
            if result['valid']:
                validation_results['valid_files'] += 1
            else:
                validation_results['invalid_files'] += 1
                validation_results['issues'].extend(result['issues'])
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
        validation_path = annotations_dir / "validation_report.json"
        with open(validation_path, 'w', encoding='utf-8') as f:
            json.dump(validation_results, f, ensure_ascii=False, indent=2)
        
        success_rate = validation_results['valid_files'] / validation_results['total_files'] * 100
        self.logger.info(f"‚úÖ –í–∞–ª–∏–¥–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –£—Å–ø–µ—à–Ω–æ—Å—Ç—å: {success_rate:.1f}%")
        
        if validation_results['invalid_files'] > 0:
            self.logger.warning(f"‚ö†Ô∏è –ù–∞–π–¥–µ–Ω–æ –ø—Ä–æ–±–ª–µ–º –≤ {validation_results['invalid_files']} —Ñ–∞–π–ª–∞—Ö")
    
    def _build_dataset(self):
        """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞"""
        self.logger.info("üèóÔ∏è –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞...")
        
        # –ü—É—Ç–∏ –∫ –¥–∞–Ω–Ω—ã–º
        frames_dir = self.processed_dir / "frames"
        annotations_dir = self.annotations_dir / "auto_generated"
        final_dataset_dir = self.dataset_dir / "restaurant_detection"
        
        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è dataset builder
        dataset_config = {
            'train_split': self.config['dataset']['train_ratio'],
            'val_split': self.config['dataset']['val_ratio'],
            'test_split': self.config['dataset']['test_ratio'],
            'stratify': self.config['dataset']['stratify_by_class'],
            'min_images_per_split': self.config['dataset']['min_images_per_split']
        }
        
        # –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞
        try:
            dataset_info = self.dataset_builder.build_dataset(
                images_dir=frames_dir,
                annotations_dir=annotations_dir,
                output_dir=final_dataset_dir,
                train_split=dataset_config['train_split'],
                val_split=dataset_config['val_split'],
                test_split=dataset_config['test_split']
            )
            
            self.logger.info("‚úÖ –î–∞—Ç–∞—Å–µ—Ç —É—Å–ø–µ—à–Ω–æ –ø–æ—Å—Ç—Ä–æ–µ–Ω")
            self.logger.info(f"üìà –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞:")
            for split, info in dataset_info.get('splits', {}).items():
                self.logger.info(f"  - {split}: {info.get('images_count', 0)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞: {e}")
            raise
    
    def _validate_dataset_quality(self):
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞"""
        self.logger.info("üî¨ –í–∞–ª–∏–¥–∞—Ü–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞...")
        
        dataset_dir = self.dataset_dir / "restaurant_detection"
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
        required_dirs = [
            dataset_dir / "train" / "images",
            dataset_dir / "train" / "labels",
            dataset_dir / "val" / "images", 
            dataset_dir / "val" / "labels",
            dataset_dir / "test" / "images",
            dataset_dir / "test" / "labels"
        ]
        
        structure_ok = True
        for required_dir in required_dirs:
            if not required_dir.exists():
                self.logger.error(f"‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {required_dir}")
                structure_ok = False
        
        if not structure_ok:
            raise ValueError("–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ –Ω–∞—Ä—É—à–µ–Ω–∞")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ dataset.yaml
        dataset_yaml = dataset_dir / "dataset.yaml"
        if not dataset_yaml.exists():
            self.logger.warning("‚ö†Ô∏è –û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç dataset.yaml, —Å–æ–∑–¥–∞–Ω–∏–µ...")
            self._create_dataset_yaml(dataset_dir)
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ splits
        for split in ['train', 'val', 'test']:
            images_dir = dataset_dir / split / "images"
            labels_dir = dataset_dir / split / "labels"
            
            image_count = len(list(images_dir.glob("*")))
            label_count = len(list(labels_dir.glob("*.txt")))
            
            self.logger.info(f"üìä {split.upper()}: {image_count} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, {label_count} –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π")
            
            if image_count != label_count:
                self.logger.warning(f"‚ö†Ô∏è –ù–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –≤ {split}: –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π={image_count}, –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π={label_count}")
        
        self.logger.info("‚úÖ –í–∞–ª–∏–¥–∞—Ü–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
    
    def _create_dataset_yaml(self, dataset_dir: Path):
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ñ–∞–π–ª–∞ dataset.yaml"""
        from src.data.annotator import create_dataset_yaml
        
        # –ü–æ–ª—É—á–µ–Ω–∏–µ –º–∞–ø–ø–∏–Ω–≥–∞ –∫–ª–∞—Å—Å–æ–≤
        class_mapping = {
            'person': 0, 'chair': 1, 'dining table': 2, 'cup': 3, 'fork': 4, 'knife': 5,
            'spoon': 6, 'bowl': 7, 'bottle': 8, 'wine glass': 9, 'sandwich': 10, 'pizza': 11,
            'cake': 12, 'apple': 13, 'banana': 14, 'orange': 15, 'cell phone': 16, 'laptop': 17, 'book': 18
        }
        
        yaml_path = create_dataset_yaml(dataset_dir, class_mapping)
        self.logger.info(f"üìÑ –°–æ–∑–¥–∞–Ω dataset.yaml: {yaml_path}")
    
    def _generate_pipeline_reports(self):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–æ–≤ –æ —Ä–∞–±–æ—Ç–µ –ø–∞–π–ø–ª–∞–π–Ω–∞"""
        self.logger.info("üìã –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–æ–≤...")
        
        reports_dir = Path("outputs/reports")
        reports_dir.mkdir(parents=True, exist_ok=True)
        
        # –û—Ç—á–µ—Ç –æ –ø–∞–π–ø–ª–∞–π–Ω–µ
        pipeline_report = {
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
            'pipeline_stats': self.pipeline_stats,
            'configuration': self.config,
            'dataset_location': str(self.dataset_dir / "restaurant_detection"),
            'total_processing_time': self.pipeline_stats.get('end_time', time.time()) - self.pipeline_stats.get('start_time', 0)
        }
        
        report_path = reports_dir / "pipeline_report.json"
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(pipeline_report, f, ensure_ascii=False, indent=2)
        
        self.logger.info(f"üìä –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {report_path}")
    
    def _finalize_pipeline(self):
        """–§–∏–Ω–∞–ª–∏–∑–∞—Ü–∏—è –ø–∞–π–ø–ª–∞–π–Ω–∞"""
        self.logger.info("üéØ –§–∏–Ω–∞–ª–∏–∑–∞—Ü–∏—è –ø–∞–π–ø–ª–∞–π–Ω–∞...")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        dataset_dir = self.dataset_dir / "restaurant_detection"
        dataset_yaml = dataset_dir / "dataset.yaml"
        
        if dataset_yaml.exists():
            self.logger.info(f"‚úÖ –î–∞—Ç–∞—Å–µ—Ç –≥–æ—Ç–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è YOLO11!")
            self.logger.info(f"üìÇ –ü—É—Ç—å –∫ –¥–∞—Ç–∞—Å–µ—Ç—É: {dataset_dir}")
            self.logger.info(f"üìÑ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è: {dataset_yaml}")
            self.logger.info(f"üöÄ –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è: python scripts/train_model.py --data {dataset_yaml}")
        else:
            self.logger.error("‚ùå –§–∞–π–ª dataset.yaml –Ω–µ –Ω–∞–π–¥–µ–Ω!")
    
    def _log_pipeline_summary(self):
        """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏—Ç–æ–≥–æ–≤ —Ä–∞–±–æ—Ç—ã –ø–∞–π–ø–ª–∞–π–Ω–∞"""
        total_time = self.pipeline_stats.get('end_time', time.time()) - self.pipeline_stats.get('start_time', 0)
        
        self.logger.info("\n" + "="*60)
        self.logger.info("üìã –ò–¢–û–ì–ò –†–ê–ë–û–¢–´ –ü–ê–ô–ü–õ–ê–ô–ù–ê")
        self.logger.info("="*60)
        self.logger.info(f"‚è±Ô∏è –û–±—â–µ–µ –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {total_time/60:.1f} –º–∏–Ω—É—Ç")
        self.logger.info(f"üé¨ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –≤–∏–¥–µ–æ: {self.pipeline_stats['total_videos']}")
        self.logger.info(f"üñºÔ∏è –ò–∑–≤–ª–µ—á–µ–Ω–æ –∫–∞–¥—Ä–æ–≤: {self.pipeline_stats['total_frames']}")
        self.logger.info(f"üéØ –°–æ–∑–¥–∞–Ω–æ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π: {self.pipeline_stats['total_annotations']}")
        self.logger.info(f"‚úÖ –ó–∞–≤–µ—Ä—à–µ–Ω–Ω—ã–µ —ç—Ç–∞–ø—ã: {', '.join(self.pipeline_stats['stages_completed'])}")
        
        if self.pipeline_stats['errors']:
            self.logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∏: {len(self.pipeline_stats['errors'])}")
            for error in self.pipeline_stats['errors']:
                self.logger.warning(f"   - {error}")
        
        self.logger.info("="*60)


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    parser = argparse.ArgumentParser(
        description="–ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è YOLO11",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
–ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
    # –ü–æ–ª–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω —Å –≤–∏–¥–µ–æ
    python scripts/prepare_data.py --input data/raw/videos --config config/pipeline_config.json
    
    # –¢–æ–ª—å–∫–æ –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
    python scripts/prepare_data.py --input data/processed/frames --skip-video-processing
    
    # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –ø–µ—Ä–µ–æ–±—Ä–∞–±–æ—Ç–∫–∞
    python scripts/prepare_data.py --input data/raw --force-reprocess
        """
    )
    
    parser.add_argument(
        "--input", 
        type=str, 
        required=True,
        help="–ü—É—Ç—å –∫ –≤—Ö–æ–¥–Ω—ã–º –¥–∞–Ω–Ω—ã–º (–≤–∏–¥–µ–æ –∏–ª–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è)"
    )
    
    parser.add_argument(
        "--config",
        type=str,
        default=None,
        help="–ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ (JSON)"
    )
    
    parser.add_argument(
        "--force-reprocess",
        action="store_true",
        help="–ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –ø–µ—Ä–µ–æ–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö"
    )
    
    parser.add_argument(
        "--skip-video-processing",
        action="store_true", 
        help="–ü—Ä–æ–ø—É—Å—Ç–∏—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫—É –≤–∏–¥–µ–æ (—Ä–∞–±–æ—Ç–∞—Ç—å —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –∫–∞–¥—Ä–∞–º–∏)"
    )
    
    parser.add_argument(
        "--skip-annotation",
        action="store_true",
        help="–ü—Ä–æ–ø—É—Å—Ç–∏—Ç—å –∞–Ω–Ω–æ—Ç–∞—Ü–∏—é (–∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏)"
    )
    
    parser.add_argument(
        "--verbose",
        action="store_true",
        help="–ü–æ–¥—Ä–æ–±–Ω—ã–π –≤—ã–≤–æ–¥"
    )
    
    args = parser.parse_args()
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    try:
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–∞–π–ø–ª–∞–π–Ω–∞
        config_path = Path(args.config) if args.config else None
        pipeline = EnhancedDataPipeline(config_path)
        
        # –ó–∞–ø—É—Å–∫ –ø–∞–π–ø–ª–∞–π–Ω–∞
        success = pipeline.run_complete_pipeline(
            input_path=Path(args.input),
            force_reprocess=args.force_reprocess,
            skip_video_processing=args.skip_video_processing,
            skip_annotation=args.skip_annotation
        )
        
        if success:
            print("\nüéâ –ü–∞–π–ø–ª–∞–π–Ω –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω!")
            print("üöÄ –¢–µ–ø–µ—Ä—å –º–æ–∂–Ω–æ –∑–∞–ø—É—Å–∫–∞—Ç—å –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏:")
            print("   python scripts/train_model.py --data data/datasets/restaurant_detection/dataset.yaml")
            sys.exit(0)
        else:
            print("\n‚ùå –ü–∞–π–ø–ª–∞–π–Ω –∑–∞–≤–µ—Ä—à–∏–ª—Å—è —Å –æ—à–∏–±–∫–∞–º–∏!")
            sys.exit(1)
            
    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è –ü–∞–π–ø–ª–∞–π–Ω –ø—Ä–µ—Ä–≤–∞–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
        sys.exit(1)
    except Exception as e:
        print(f"\n‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()