# """
# –£–ª—É—á—à–µ–Ω–Ω—ã–π —Å–∫—Ä–∏–ø—Ç –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö —Å –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–π –∞–Ω–Ω–æ—Ç–∞—Ü–∏–µ–π
# –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–æ–∑–¥–∞–µ—Ç –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è YOLO11
# """

# import sys
# import logging
# import argparse
# from pathlib import Path
# from typing import Dict, Any
# import time
# import json
# import shutil

# # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–Ω–µ–≤—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –ø—Ä–æ–µ–∫—Ç–∞ –≤ sys.path
# project_root = Path(__file__).parent.parent
# sys.path.insert(0, str(project_root))

# from src.data.video_processor import VideoProcessor
# from src.data.annotator import SmartAnnotator
# from src.data.dataset_builder import DatasetBuilder
# from src.utils.logger import setup_logger


# class EnhancedDataPipeline:
#     """
#     –ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –∞–Ω–Ω–æ—Ç–∞—Ü–∏–µ–π
#     """
    
#     def __init__(self, config_path: Path = None):
#         self.logger = setup_logger(self.__class__.__name__)
#         self.config = self._load_config(config_path)
        
#         # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
#         self.video_processor = VideoProcessor()
#         self.annotator = SmartAnnotator()
#         self.dataset_builder = DatasetBuilder()
        
#         # –î–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
#         self.raw_data_dir = Path("data/raw")
#         self.processed_dir = Path("data/processed")
#         self.annotations_dir = Path("data/annotations")
#         self.dataset_dir = Path("data/datasets")
        
#         # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
#         self.pipeline_stats = {
#             'start_time': None,
#             'end_time': None,
#             'total_videos': 0,
#             'total_frames': 0,
#             'total_annotations': 0,
#             'stages_completed': [],
#             'errors': []
#         }
    
#     def _load_config(self, config_path: Path = None) -> Dict[str, Any]:
#         """–ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –ø–∞–π–ø–ª–∞–π–Ω–∞"""
#         default_config = {
#             'video_processing': {
#                 'fps_extraction': 2.0,  # –ö–∞–¥—Ä–æ–≤ –≤ —Å–µ–∫—É–Ω–¥—É –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è
#                 'min_frame_interval': 0.5,  # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –∏–Ω—Ç–µ—Ä–≤–∞–ª –º–µ–∂–¥—É –∫–∞–¥—Ä–∞–º–∏
#                 'quality_threshold': 0.7,  # –ü–æ—Ä–æ–≥ –∫–∞—á–µ—Å—Ç–≤–∞ –∫–∞–¥—Ä–æ–≤
#                 'max_frames_per_video': 500,  # –ú–∞–∫—Å–∏–º—É–º –∫–∞–¥—Ä–æ–≤ –∏–∑ –æ–¥–Ω–æ–≥–æ –≤–∏–¥–µ–æ
#                 'frame_formats': ['.jpg', '.png'],
#                 'video_formats': ['.mp4', '.avi', '.mov', '.mkv', '.wmv']
#             },
#             'annotation': {
#                 'batch_size': 16,
#                 'num_workers': 4,
#                 'confidence_threshold': 0.25,
#                 'enable_quality_check': True,
#                 'auto_validation': True
#             },
#             'dataset': {
#                 'train_ratio': 0.7,
#                 'val_ratio': 0.2,
#                 'test_ratio': 0.1,
#                 'min_images_per_split': 10,
#                 'enable_augmentation': True,
#                 'stratify_by_class': True
#             },
#             'quality_control': {
#                 'min_detections_per_image': 0,  # –†–∞–∑—Ä–µ—à–∞–µ–º —Ñ–æ–Ω–æ–≤—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
#                 'max_detections_per_image': 50,
#                 'validate_annotations': True,
#                 'generate_reports': True
#             }
#         }
        
#         if config_path and config_path.exists():
#             with open(config_path, 'r', encoding='utf-8') as f:
#                 user_config = json.load(f)
#                 # –ì–ª—É–±–æ–∫–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
#                 self._deep_update(default_config, user_config)
        
#         return default_config
    
#     def _deep_update(self, base_dict: Dict, update_dict: Dict):
#         """–ì–ª—É–±–æ–∫–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å–ª–æ–≤–∞—Ä—è"""
#         for key, value in update_dict.items():
#             if key in base_dict and isinstance(base_dict[key], dict) and isinstance(value, dict):
#                 self._deep_update(base_dict[key], value)
#             else:
#                 base_dict[key] = value
    
#     def run_complete_pipeline(self, 
#                             input_path: Path,
#                             force_reprocess: bool = False,
#                             skip_video_processing: bool = False,
#                             skip_annotation: bool = False) -> bool:
#         """
#         –ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ –ø–∞–π–ø–ª–∞–π–Ω–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö
        
#         Args:
#             input_path: –ü—É—Ç—å –∫ –≤—Ö–æ–¥–Ω—ã–º –¥–∞–Ω–Ω—ã–º (–≤–∏–¥–µ–æ –∏–ª–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è)
#             force_reprocess: –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –ø–µ—Ä–µ–æ–±—Ä–∞–±–æ—Ç–∫–∞
#             skip_video_processing: –ü—Ä–æ–ø—É—Å—Ç–∏—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫—É –≤–∏–¥–µ–æ
#             skip_annotation: –ü—Ä–æ–ø—É—Å—Ç–∏—Ç—å –∞–Ω–Ω–æ—Ç–∞—Ü–∏—é (–∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ)
            
#         Returns:
#             True –µ—Å–ª–∏ —É—Å–ø–µ—à–Ω–æ, False –∏–Ω–∞—á–µ
#         """
#         self.pipeline_stats['start_time'] = time.time()
#         self.logger.info("üöÄ –ó–∞–ø—É—Å–∫ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–≥–æ –ø–∞–π–ø–ª–∞–π–Ω–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö")
#         self.logger.info(f"üìÇ –í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ: {input_path}")
        
#         try:
#             # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π
#             self._prepare_directories(force_reprocess)
            
#             # –≠—Ç–∞–ø 1: –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤–∏–¥–µ–æ (–µ—Å–ª–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ)
#             if not skip_video_processing:
#                 frames_dir = self._process_videos(input_path)
#                 if frames_dir is None:
#                     # –ï—Å–ª–∏ –≤–∏–¥–µ–æ –Ω–µ—Ç, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –Ω–∞–ø—Ä—è–º—É—é
#                     frames_dir = input_path if input_path.is_dir() else input_path.parent
#             else:
#                 frames_dir = self.processed_dir / "frames"
            
#             self.pipeline_stats['stages_completed'].append('video_processing')
            
#             # –≠—Ç–∞–ø 2: –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è
#             if not skip_annotation:
#                 self._create_professional_annotations(frames_dir)
#                 self.pipeline_stats['stages_completed'].append('annotation')
            
#             # –≠—Ç–∞–ø 3: –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞
#             self._build_dataset()
#             self.pipeline_stats['stages_completed'].append('dataset_building')
            
#             # –≠—Ç–∞–ø 4: –í–∞–ª–∏–¥–∞—Ü–∏—è –∫–∞—á–µ—Å—Ç–≤–∞
#             self._validate_dataset_quality()
#             self.pipeline_stats['stages_completed'].append('quality_validation')
            
#             # –≠—Ç–∞–ø 5: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–æ–≤
#             if self.config['quality_control']['generate_reports']:
#                 self._generate_pipeline_reports()
#                 self.pipeline_stats['stages_completed'].append('reporting')
            
#             # –§–∏–Ω–∞–ª–∏–∑–∞—Ü–∏—è
#             self._finalize_pipeline()
            
#             return True
            
#         except Exception as e:
#             self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ –ø–∞–π–ø–ª–∞–π–Ω–µ: {e}")
#             self.pipeline_stats['errors'].append(str(e))
#             return False
        
#         finally:
#             self.pipeline_stats['end_time'] = time.time()
#             self._log_pipeline_summary()
    
#     def _prepare_directories(self, force_reprocess: bool):
#         """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π"""
#         self.logger.info("üìÅ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π...")
        
#         directories = [
#             self.raw_data_dir,
#             self.processed_dir,
#             self.annotations_dir,
#             self.dataset_dir
#         ]
        
#         for directory in directories:
#             if force_reprocess and directory.exists():
#                 self.logger.info(f"üóëÔ∏è –û—á–∏—Å—Ç–∫–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏: {directory}")
#                 shutil.rmtree(directory)
            
#             directory.mkdir(parents=True, exist_ok=True)
#             self.logger.debug(f"‚úÖ –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –≥–æ—Ç–æ–≤–∞: {directory}")
    
#     def _process_videos(self, input_path: Path) -> Path:
#         """–û–±—Ä–∞–±–æ—Ç–∫–∞ –≤–∏–¥–µ–æ—Ñ–∞–π–ª–æ–≤ –∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–∞–¥—Ä–æ–≤"""
#         self.logger.info("üé¨ –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤–∏–¥–µ–æ—Ñ–∞–π–ª–æ–≤...")
        
#         frames_output_dir = self.processed_dir / "frames"
#         frames_output_dir.mkdir(parents=True, exist_ok=True)
        
#         video_extensions = set(self.config['video_processing']['video_formats'])
        
#         # –ü–æ–∏—Å–∫ –≤–∏–¥–µ–æ—Ñ–∞–π–ª–æ–≤
#         video_files = []
#         if input_path.is_file() and input_path.suffix.lower() in video_extensions:
#             video_files = [input_path]
#         elif input_path.is_dir():
#             for ext in video_extensions:
#                 video_files.extend(list(input_path.glob(f"**/*{ext}")))
#                 video_files.extend(list(input_path.glob(f"**/*{ext.upper()}")))
        
#         if not video_files:
#             self.logger.info("üì∏ –í–∏–¥–µ–æ—Ñ–∞–π–ª—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã, –∏—â–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è...")
#             return None
        
#         self.logger.info(f"üé• –ù–∞–π–¥–µ–Ω–æ –≤–∏–¥–µ–æ—Ñ–∞–π–ª–æ–≤: {len(video_files)}")
#         self.pipeline_stats['total_videos'] = len(video_files)
        
#         total_frames = 0
        
#         for video_file in video_files:
#             self.logger.info(f"‚öôÔ∏è –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤–∏–¥–µ–æ: {video_file.name}")
            
#             try:
#                 # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–∞–¥—Ä–æ–≤
#                 frames = self.video_processor.extract_frames(
#                     video_path=video_file,
#                     output_dir=frames_output_dir,
#                     fps=self.config['video_processing']['fps_extraction'],
#                     max_frames=self.config['video_processing']['max_frames_per_video']
#                 )
                
#                 extracted_count = len(frames) if frames else 0
#                 total_frames += extracted_count
                
#                 self.logger.info(f"‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–æ –∫–∞–¥—Ä–æ–≤ –∏–∑ {video_file.name}: {extracted_count}")
                
#             except Exception as e:
#                 self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {video_file}: {e}")
#                 self.pipeline_stats['errors'].append(f"Video processing error: {video_file}: {e}")
        
#         self.pipeline_stats['total_frames'] = total_frames
#         self.logger.info(f"üé¨ –ó–∞–≤–µ—Ä—à–µ–Ω–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –≤–∏–¥–µ–æ. –í—Å–µ–≥–æ –∫–∞–¥—Ä–æ–≤: {total_frames}")
        
#         return frames_output_dir
    
#     def _create_professional_annotations(self, frames_dir: Path):
#         """–°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã—Ö –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π"""
#         self.logger.info("üß† –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã—Ö –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π...")
        
#         # –ü–æ–∏—Å–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
#         image_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.tiff'}
#         image_files = []
#         for ext in image_extensions:
#             image_files.extend(list(frames_dir.glob(f"*{ext}")))
#             image_files.extend(list(frames_dir.glob(f"*{ext.upper()}")))
        
#         if not image_files:
#             raise ValueError(f"–ù–µ –Ω–∞–π–¥–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ {frames_dir}")
        
#         self.logger.info(f"üñºÔ∏è –ù–∞–π–¥–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏: {len(image_files)}")
        
#         # –°–æ–∑–¥–∞–Ω–∏–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π –¥–ª—è train, val, test
#         for split in ['train', 'val', 'test']:
#             split_images_dir = self.dataset_dir / split / 'images'
#             split_labels_dir = self.dataset_dir / split / 'labels'
            
#             # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π
#             split_images_dir.mkdir(parents=True, exist_ok=True)
#             split_labels_dir.mkdir(parents=True, exist_ok=True)
            
#             # –í—Ä–µ–º–µ–Ω–Ω–æ–µ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏
#             # (–±—É–¥–µ—Ç –ø–µ—Ä–µ–æ—Ä–≥–∞–Ω–∏–∑–æ–≤–∞–Ω–æ –≤ dataset_builder)
            
#         # –û—Å–Ω–æ–≤–Ω–∞—è –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è
#         annotations_output_dir = self.annotations_dir / "auto_generated"
#         annotations_output_dir.mkdir(parents=True, exist_ok=True)
        
#         # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∞–Ω–Ω–æ—Ç–∞—Ç–æ—Ä–∞
#         annotation_config = {
#             'models': {
#                 'yolo11n': {'weight': 0.3, 'confidence': 0.15},
#                 'yolo11s': {'weight': 0.4, 'confidence': 0.2},
#                 'yolo11m': {'weight': 0.3, 'confidence': 0.25}
#             },
#             'filtering': {
#                 'min_confidence': self.config['annotation']['confidence_threshold'],
#                 'min_area': 200,
#                 'max_area_ratio': 0.9,
#                 'min_aspect_ratio': 0.1,
#                 'max_aspect_ratio': 10.0,
#                 'edge_threshold': 10
#             },
#             'restaurant_classes': [
#                 'person', 'chair', 'dining table', 'cup', 'fork', 'knife',
#                 'spoon', 'bowl', 'bottle', 'wine glass', 'sandwich', 'pizza',
#                 'cake', 'apple', 'banana', 'orange', 'cell phone', 'laptop', 'book'
#             ]
#         }
        
#         # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –∞–Ω–Ω–æ—Ç–∞—Ç–æ—Ä–∞
#         config_path = annotations_output_dir / "annotator_config.json"
#         with open(config_path, 'w', encoding='utf-8') as f:
#             json.dump(annotation_config, f, ensure_ascii=False, indent=2)
        
#         # –ó–∞–ø—É—Å–∫ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏
#         annotation_stats = self.annotator.annotate_dataset(
#             images_dir=frames_dir,
#             output_dir=annotations_output_dir,
#             batch_size=self.config['annotation']['batch_size'],
#             num_workers=self.config['annotation']['num_workers']
#         )
        
#         self.pipeline_stats['total_annotations'] = annotation_stats.get('total_detections', 0)
        
#         # –í–∞–ª–∏–¥–∞—Ü–∏—è –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π
#         if self.config['annotation']['auto_validation']:
#             self._validate_annotations(annotations_output_dir, frames_dir)
        
#         self.logger.info("‚úÖ –ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–∞—è –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
#         self.logger.info(f"üìä –°–æ–∑–¥–∞–Ω–æ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π: {annotation_stats.get('processed_images', 0)}")
#         self.logger.info(f"üéØ –í—Å–µ–≥–æ –¥–µ—Ç–µ–∫—Ü–∏–π: {annotation_stats.get('total_detections', 0)}")
    
#     def _validate_annotations(self, annotations_dir: Path, images_dir: Path):
#         """–í–∞–ª–∏–¥–∞—Ü–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π"""
#         self.logger.info("üîç –í–∞–ª–∏–¥–∞—Ü–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π...")
        
#         from src.data.annotator import AnnotationValidator
        
#         validator = AnnotationValidator()
        
#         # –ü–æ–ª—É—á–µ–Ω–∏–µ –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π
#         annotation_files = list(annotations_dir.glob("*.txt"))
        
#         validation_results = {
#             'total_files': len(annotation_files),
#             'valid_files': 0,
#             'invalid_files': 0,
#             'issues': []
#         }
        
#         for annotation_file in annotation_files:
#             # –ü–æ–∏—Å–∫ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
#             image_file = None
#             for ext in ['.jpg', '.jpeg', '.png', '.bmp']:
#                 potential_image = images_dir / f"{annotation_file.stem}{ext}"
#                 if potential_image.exists():
#                     image_file = potential_image
#                     break
            
#             # –í–∞–ª–∏–¥–∞—Ü–∏—è
#             result = validator.validate_annotation_file(annotation_file, image_file)
            
#             if result['valid']:
#                 validation_results['valid_files'] += 1
#             else:
#                 validation_results['invalid_files'] += 1
#                 validation_results['issues'].extend(result['issues'])
        
#         # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
#         validation_path = annotations_dir / "validation_report.json"
#         with open(validation_path, 'w', encoding='utf-8') as f:
#             json.dump(validation_results, f, ensure_ascii=False, indent=2)
        
#         success_rate = validation_results['valid_files'] / validation_results['total_files'] * 100
#         self.logger.info(f"‚úÖ –í–∞–ª–∏–¥–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –£—Å–ø–µ—à–Ω–æ—Å—Ç—å: {success_rate:.1f}%")
        
#         if validation_results['invalid_files'] > 0:
#             self.logger.warning(f"‚ö†Ô∏è –ù–∞–π–¥–µ–Ω–æ –ø—Ä–æ–±–ª–µ–º –≤ {validation_results['invalid_files']} —Ñ–∞–π–ª–∞—Ö")
    
#     def _build_dataset(self):
#         """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞"""
#         self.logger.info("üèóÔ∏è –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞...")
        
#         # –ü—É—Ç–∏ –∫ –¥–∞–Ω–Ω—ã–º
#         frames_dir = self.processed_dir / "frames"
#         annotations_dir = self.annotations_dir / "auto_generated"
#         final_dataset_dir = self.dataset_dir / "restaurant_detection"
        
#         # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è dataset builder
#         dataset_config = {
#             'train_split': self.config['dataset']['train_ratio'],
#             'val_split': self.config['dataset']['val_ratio'],
#             'test_split': self.config['dataset']['test_ratio'],
#             'stratify': self.config['dataset']['stratify_by_class'],
#             'min_images_per_split': self.config['dataset']['min_images_per_split']
#         }
        
#         # –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞
#         try:
#             dataset_info = self.dataset_builder.build_dataset(
#                 images_dir=frames_dir,
#                 annotations_dir=annotations_dir,
#                 output_dir=final_dataset_dir,
#                 train_split=dataset_config['train_split'],
#                 val_split=dataset_config['val_split'],
#                 test_split=dataset_config['test_split']
#             )
            
#             self.logger.info("‚úÖ –î–∞—Ç–∞—Å–µ—Ç —É—Å–ø–µ—à–Ω–æ –ø–æ—Å—Ç—Ä–æ–µ–Ω")
#             self.logger.info(f"üìà –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞:")
#             for split, info in dataset_info.get('splits', {}).items():
#                 self.logger.info(f"  - {split}: {info.get('images_count', 0)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
            
#         except Exception as e:
#             self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞: {e}")
#             raise
    
#     def _validate_dataset_quality(self):
#         """–í–∞–ª–∏–¥–∞—Ü–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞"""
#         self.logger.info("üî¨ –í–∞–ª–∏–¥–∞—Ü–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞...")
        
#         dataset_dir = self.dataset_dir / "restaurant_detection"
        
#         # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
#         required_dirs = [
#             dataset_dir / "train" / "images",
#             dataset_dir / "train" / "labels",
#             dataset_dir / "val" / "images", 
#             dataset_dir / "val" / "labels",
#             dataset_dir / "test" / "images",
#             dataset_dir / "test" / "labels"
#         ]
        
#         structure_ok = True
#         for required_dir in required_dirs:
#             if not required_dir.exists():
#                 self.logger.error(f"‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {required_dir}")
#                 structure_ok = False
        
#         if not structure_ok:
#             raise ValueError("–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ –Ω–∞—Ä—É—à–µ–Ω–∞")
        
#         # –ü—Ä–æ–≤–µ—Ä–∫–∞ dataset.yaml
#         dataset_yaml = dataset_dir / "dataset.yaml"
#         if not dataset_yaml.exists():
#             self.logger.warning("‚ö†Ô∏è –û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç dataset.yaml, —Å–æ–∑–¥–∞–Ω–∏–µ...")
#             self._create_dataset_yaml(dataset_dir)
        
#         # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ splits
#         for split in ['train', 'val', 'test']:
#             images_dir = dataset_dir / split / "images"
#             labels_dir = dataset_dir / split / "labels"
            
#             image_count = len(list(images_dir.glob("*")))
#             label_count = len(list(labels_dir.glob("*.txt")))
            
#             self.logger.info(f"üìä {split.upper()}: {image_count} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, {label_count} –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π")
            
#             if image_count != label_count:
#                 self.logger.warning(f"‚ö†Ô∏è –ù–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –≤ {split}: –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π={image_count}, –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π={label_count}")
        
#         self.logger.info("‚úÖ –í–∞–ª–∏–¥–∞—Ü–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
    
#     def _create_dataset_yaml(self, dataset_dir: Path):
#         """–°–æ–∑–¥–∞–Ω–∏–µ —Ñ–∞–π–ª–∞ dataset.yaml"""
#         from src.data.annotator import create_dataset_yaml
        
#         # –ü–æ–ª—É—á–µ–Ω–∏–µ –º–∞–ø–ø–∏–Ω–≥–∞ –∫–ª–∞—Å—Å–æ–≤
#         class_mapping = {
#             'person': 0, 'chair': 1, 'dining table': 2, 'cup': 3, 'fork': 4, 'knife': 5,
#             'spoon': 6, 'bowl': 7, 'bottle': 8, 'wine glass': 9, 'sandwich': 10, 'pizza': 11,
#             'cake': 12, 'apple': 13, 'banana': 14, 'orange': 15, 'cell phone': 16, 'laptop': 17, 'book': 18
#         }
        
#         yaml_path = create_dataset_yaml(dataset_dir, class_mapping)
#         self.logger.info(f"üìÑ –°–æ–∑–¥–∞–Ω dataset.yaml: {yaml_path}")
    
#     def _generate_pipeline_reports(self):
#         """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–æ–≤ –æ —Ä–∞–±–æ—Ç–µ –ø–∞–π–ø–ª–∞–π–Ω–∞"""
#         self.logger.info("üìã –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–æ–≤...")
        
#         reports_dir = Path("outputs/reports")
#         reports_dir.mkdir(parents=True, exist_ok=True)
        
#         # –û—Ç—á–µ—Ç –æ –ø–∞–π–ø–ª–∞–π–Ω–µ
#         pipeline_report = {
#             'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
#             'pipeline_stats': self.pipeline_stats,
#             'configuration': self.config,
#             'dataset_location': str(self.dataset_dir / "restaurant_detection"),
#             'total_processing_time': self.pipeline_stats.get('end_time', time.time()) - self.pipeline_stats.get('start_time', 0)
#         }
        
#         report_path = reports_dir / "pipeline_report.json"
#         with open(report_path, 'w', encoding='utf-8') as f:
#             json.dump(pipeline_report, f, ensure_ascii=False, indent=2)
        
#         self.logger.info(f"üìä –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {report_path}")
    
#     def _finalize_pipeline(self):
#         """–§–∏–Ω–∞–ª–∏–∑–∞—Ü–∏—è –ø–∞–π–ø–ª–∞–π–Ω–∞"""
#         self.logger.info("üéØ –§–∏–Ω–∞–ª–∏–∑–∞—Ü–∏—è –ø–∞–π–ø–ª–∞–π–Ω–∞...")
        
#         # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
#         dataset_dir = self.dataset_dir / "restaurant_detection"
#         dataset_yaml = dataset_dir / "dataset.yaml"
        
#         if dataset_yaml.exists():
#             self.logger.info(f"‚úÖ –î–∞—Ç–∞—Å–µ—Ç –≥–æ—Ç–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è YOLO11!")
#             self.logger.info(f"üìÇ –ü—É—Ç—å –∫ –¥–∞—Ç–∞—Å–µ—Ç—É: {dataset_dir}")
#             self.logger.info(f"üìÑ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è: {dataset_yaml}")
#             self.logger.info(f"üöÄ –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è: python scripts/train_model.py --data {dataset_yaml}")
#         else:
#             self.logger.error("‚ùå –§–∞–π–ª dataset.yaml –Ω–µ –Ω–∞–π–¥–µ–Ω!")
    
#     def _log_pipeline_summary(self):
#         """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏—Ç–æ–≥–æ–≤ —Ä–∞–±–æ—Ç—ã –ø–∞–π–ø–ª–∞–π–Ω–∞"""
#         total_time = self.pipeline_stats.get('end_time', time.time()) - self.pipeline_stats.get('start_time', 0)
        
#         self.logger.info("\n" + "="*60)
#         self.logger.info("üìã –ò–¢–û–ì–ò –†–ê–ë–û–¢–´ –ü–ê–ô–ü–õ–ê–ô–ù–ê")
#         self.logger.info("="*60)
#         self.logger.info(f"‚è±Ô∏è –û–±—â–µ–µ –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {total_time/60:.1f} –º–∏–Ω—É—Ç")
#         self.logger.info(f"üé¨ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –≤–∏–¥–µ–æ: {self.pipeline_stats['total_videos']}")
#         self.logger.info(f"üñºÔ∏è –ò–∑–≤–ª–µ—á–µ–Ω–æ –∫–∞–¥—Ä–æ–≤: {self.pipeline_stats['total_frames']}")
#         self.logger.info(f"üéØ –°–æ–∑–¥–∞–Ω–æ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π: {self.pipeline_stats['total_annotations']}")
#         self.logger.info(f"‚úÖ –ó–∞–≤–µ—Ä—à–µ–Ω–Ω—ã–µ —ç—Ç–∞–ø—ã: {', '.join(self.pipeline_stats['stages_completed'])}")
        
#         if self.pipeline_stats['errors']:
#             self.logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∏: {len(self.pipeline_stats['errors'])}")
#             for error in self.pipeline_stats['errors']:
#                 self.logger.warning(f"   - {error}")
        
#         self.logger.info("="*60)


# def main():
#     """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
#     parser = argparse.ArgumentParser(
#         description="–ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è YOLO11",
#         formatter_class=argparse.RawDescriptionHelpFormatter,
#         epilog="""
# –ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
#     # –ü–æ–ª–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω —Å –≤–∏–¥–µ–æ
#     python scripts/prepare_data.py --input data/raw/videos --config config/pipeline_config.json
    
#     # –¢–æ–ª—å–∫–æ –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
#     python scripts/prepare_data.py --input data/processed/frames --skip-video-processing
    
#     # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –ø–µ—Ä–µ–æ–±—Ä–∞–±–æ—Ç–∫–∞
#     python scripts/prepare_data.py --input data/raw --force-reprocess
#         """
#     )
    
#     parser.add_argument(
#         "--input", 
#         type=str, 
#         required=True,
#         help="–ü—É—Ç—å –∫ –≤—Ö–æ–¥–Ω—ã–º –¥–∞–Ω–Ω—ã–º (–≤–∏–¥–µ–æ –∏–ª–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è)"
#     )
    
#     parser.add_argument(
#         "--config",
#         type=str,
#         default=None,
#         help="–ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ (JSON)"
#     )
    
#     parser.add_argument(
#         "--force-reprocess",
#         action="store_true",
#         help="–ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –ø–µ—Ä–µ–æ–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö"
#     )
    
#     parser.add_argument(
#         "--skip-video-processing",
#         action="store_true", 
#         help="–ü—Ä–æ–ø—É—Å—Ç–∏—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫—É –≤–∏–¥–µ–æ (—Ä–∞–±–æ—Ç–∞—Ç—å —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –∫–∞–¥—Ä–∞–º–∏)"
#     )
    
#     parser.add_argument(
#         "--skip-annotation",
#         action="store_true",
#         help="–ü—Ä–æ–ø—É—Å—Ç–∏—Ç—å –∞–Ω–Ω–æ—Ç–∞—Ü–∏—é (–∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏)"
#     )
    
#     parser.add_argument(
#         "--verbose",
#         action="store_true",
#         help="–ü–æ–¥—Ä–æ–±–Ω—ã–π –≤—ã–≤–æ–¥"
#     )
    
#     args = parser.parse_args()
    
#     # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
#     if args.verbose:
#         logging.getLogger().setLevel(logging.DEBUG)
    
#     try:
#         # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–∞–π–ø–ª–∞–π–Ω–∞
#         config_path = Path(args.config) if args.config else None
#         pipeline = EnhancedDataPipeline(config_path)
        
#         # –ó–∞–ø—É—Å–∫ –ø–∞–π–ø–ª–∞–π–Ω–∞
#         success = pipeline.run_complete_pipeline(
#             input_path=Path(args.input),
#             force_reprocess=args.force_reprocess,
#             skip_video_processing=args.skip_video_processing,
#             skip_annotation=args.skip_annotation
#         )
        
#         if success:
#             print("\nüéâ –ü–∞–π–ø–ª–∞–π–Ω –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω!")
#             print("üöÄ –¢–µ–ø–µ—Ä—å –º–æ–∂–Ω–æ –∑–∞–ø—É—Å–∫–∞—Ç—å –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏:")
#             print("   python scripts/train_model.py --data data/datasets/restaurant_detection/dataset.yaml")
#             sys.exit(0)
#         else:
#             print("\n‚ùå –ü–∞–π–ø–ª–∞–π–Ω –∑–∞–≤–µ—Ä—à–∏–ª—Å—è —Å –æ—à–∏–±–∫–∞–º–∏!")
#             sys.exit(1)
            
#     except KeyboardInterrupt:
#         print("\n‚ö†Ô∏è –ü–∞–π–ø–ª–∞–π–Ω –ø—Ä–µ—Ä–≤–∞–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
#         sys.exit(1)
#     except Exception as e:
#         print(f"\n‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
#         sys.exit(1)


# if __name__ == "__main__":
#     main()



"""
–£–ª—É—á—à–µ–Ω–Ω—ã–π —Å–∫—Ä–∏–ø—Ç –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö —Å –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–π –∞–Ω–Ω–æ—Ç–∞—Ü–∏–µ–π
–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–æ–∑–¥–∞–µ—Ç –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è YOLO11
"""

import sys
import logging
import argparse
import os
import json
import shutil
import cv2
import numpy as np
from pathlib import Path
from typing import Dict, Any, List, Tuple
from tqdm import tqdm
import time

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–Ω–µ–≤—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –ø—Ä–æ–µ–∫—Ç–∞ –≤ sys.path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# –ò–º–ø–æ—Ä—Ç —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã—Ö –º–æ–¥—É–ª–µ–π
from src.utils.logger import setup_logger

def create_directory_structure():
    """–°–æ–∑–¥–∞–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π –ø—Ä–æ–µ–∫—Ç–∞"""
    logger = setup_logger(__name__)
    
    directories = [
        "data/raw",
        "data/processed/dataset/train/images",
        "data/processed/dataset/train/labels", 
        "data/processed/dataset/val/images",
        "data/processed/dataset/val/labels",
        "data/processed/dataset/test/images",
        "data/processed/dataset/test/labels",
        "data/annotations",
        "outputs/experiments",
        "outputs/inference", 
        "outputs/reports",
        "logs"
    ]
    
    for directory in directories:
        Path(directory).mkdir(parents=True, exist_ok=True)
        logger.info(f"‚úÖ –°–æ–∑–¥–∞–Ω–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {directory}")
    
    logger.info("üèóÔ∏è –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π —Å–æ–∑–¥–∞–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")


def extract_frames_from_videos(video_dir: Path, output_dir: Path, fps: float = 2.0) -> List[Path]:
    """
    –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–∞–¥—Ä–æ–≤ –∏–∑ –≤–∏–¥–µ–æ —Ñ–∞–π–ª–æ–≤
    
    Args:
        video_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å –≤–∏–¥–µ–æ —Ñ–∞–π–ª–∞–º–∏
        output_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∫–∞–¥—Ä–æ–≤
        fps: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∞–¥—Ä–æ–≤ –≤ —Å–µ–∫—É–Ω–¥—É –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è
        
    Returns:
        –°–ø–∏—Å–æ–∫ –ø—É—Ç–µ–π –∫ –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–º –∫–∞–¥—Ä–∞–º
    """
    logger = setup_logger(__name__)
    
    # –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã –≤–∏–¥–µ–æ
    video_extensions = {'.mp4', '.avi', '.mov', '.mkv', '.wmv', '.flv', '.webm'}
    
    # –ü–æ–∏—Å–∫ –≤–∏–¥–µ–æ —Ñ–∞–π–ª–æ–≤
    video_files = []
    for ext in video_extensions:
        video_files.extend(list(video_dir.glob(f"*{ext}")))
        video_files.extend(list(video_dir.glob(f"*{ext.upper()}")))
    
    if not video_files:
        logger.error(f"‚ùå –í–∏–¥–µ–æ —Ñ–∞–π–ª—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ {video_dir}")
        logger.info(f"–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã: {', '.join(video_extensions)}")
        return []
    
    logger.info(f"üé¨ –ù–∞–π–¥–µ–Ω–æ {len(video_files)} –≤–∏–¥–µ–æ —Ñ–∞–π–ª–æ–≤")
    
    extracted_frames = []
    total_frames = 0
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –≤—ã—Ö–æ–¥–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
    output_dir.mkdir(parents=True, exist_ok=True)
    
    for video_path in tqdm(video_files, desc="–û–±—Ä–∞–±–æ—Ç–∫–∞ –≤–∏–¥–µ–æ"):
        try:
            # –û—Ç–∫—Ä—ã—Ç–∏–µ –≤–∏–¥–µ–æ
            cap = cv2.VideoCapture(str(video_path))
            
            if not cap.isOpened():
                logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–∫—Ä—ã—Ç—å –≤–∏–¥–µ–æ: {video_path}")
                continue
            
            # –ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –≤–∏–¥–µ–æ
            video_fps = cap.get(cv2.CAP_PROP_FPS)
            total_video_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            duration = total_video_frames / video_fps if video_fps > 0 else 0
            
            logger.info(f"üìπ {video_path.name}: {duration:.1f}—Å, {video_fps:.1f} FPS")
            
            # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–∞–¥—Ä–æ–≤
            frame_interval = int(video_fps / fps) if video_fps > fps else 1
            
            frame_count = 0
            extracted_count = 0
            
            while True:
                ret, frame = cap.read()
                if not ret:
                    break
                
                # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–∞–¥—Ä–∞ —Å –∑–∞–¥–∞–Ω–Ω—ã–º –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–º
                if frame_count % frame_interval == 0:
                    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
                    frame_filename = f"{video_path.stem}_frame_{extracted_count:06d}.jpg"
                    frame_path = output_dir / frame_filename
                    
                    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–∞–¥—Ä–∞
                    cv2.imwrite(str(frame_path), frame)
                    extracted_frames.append(frame_path)
                    extracted_count += 1
                
                frame_count += 1
            
            cap.release()
            total_frames += extracted_count
            logger.info(f"‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–æ {extracted_count} –∫–∞–¥—Ä–æ–≤ –∏–∑ {video_path.name}")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {video_path}: {e}")
            continue
    
    logger.info(f"üéØ –í—Å–µ–≥–æ –∏–∑–≤–ª–µ—á–µ–Ω–æ {total_frames} –∫–∞–¥—Ä–æ–≤ –∏–∑ {len(video_files)} –≤–∏–¥–µ–æ")
    return extracted_frames


def create_basic_annotations(image_paths: List[Path], labels_dir: Path):
    """
    –°–æ–∑–¥–∞–Ω–∏–µ –±–∞–∑–æ–≤—ã—Ö –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π (–ø—É—Å—Ç—ã—Ö —Ñ–∞–π–ª–æ–≤) –¥–ª—è –≤—Å–µ—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
    
    Args:
        image_paths: –°–ø–∏—Å–æ–∫ –ø—É—Ç–µ–π –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º
        labels_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π
    """
    logger = setup_logger(__name__)
    
    labels_dir.mkdir(parents=True, exist_ok=True)
    
    logger.info(f"üìù –°–æ–∑–¥–∞–Ω–∏–µ –±–∞–∑–æ–≤—ã—Ö –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π –¥–ª—è {len(image_paths)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π...")
    
    for image_path in tqdm(image_paths, desc="–°–æ–∑–¥–∞–Ω–∏–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π"):
        # –°–æ–∑–¥–∞–Ω–∏–µ –ø—É—Å—Ç–æ–≥–æ —Ñ–∞–π–ª–∞ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏
        label_filename = image_path.stem + ".txt"
        label_path = labels_dir / label_filename
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –ø—É—Å—Ç–æ–≥–æ —Ñ–∞–π–ª–∞ (–±–µ–∑ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π)
        with open(label_path, 'w', encoding='utf-8') as f:
            pass  # –ü—É—Å—Ç–æ–π —Ñ–∞–π–ª
    
    logger.info(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(image_paths)} —Ñ–∞–π–ª–æ–≤ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π")


def split_dataset(image_dir: Path, labels_dir: Path, output_base_dir: Path, 
                 train_ratio: float = 0.7, val_ratio: float = 0.2, test_ratio: float = 0.1):
    """
    –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞ –Ω–∞ train/val/test
    
    Args:
        image_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏
        labels_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è–º–∏
        output_base_dir: –ë–∞–∑–æ–≤–∞—è –≤—ã—Ö–æ–¥–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è
        train_ratio: –î–æ–ª—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        val_ratio: –î–æ–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö  
        test_ratio: –î–æ–ª—è —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    """
    logger = setup_logger(__name__)
    
    # –ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
    image_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.tiff'}
    images = []
    
    for ext in image_extensions:
        images.extend(list(image_dir.glob(f"*{ext}")))
        images.extend(list(image_dir.glob(f"*{ext.upper()}")))
    
    if not images:
        logger.error(f"‚ùå –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ {image_dir}")
        return
    
    logger.info(f"üñºÔ∏è –ù–∞–π–¥–µ–Ω–æ {len(images)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è")
    
    # –ü–µ—Ä–µ–º–µ—à–∏–≤–∞–Ω–∏–µ –∏ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ
    import random
    random.seed(42)  # –î–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
    random.shuffle(images)
    
    total = len(images)
    train_end = int(total * train_ratio)
    val_end = train_end + int(total * val_ratio)
    
    splits = {
        'train': images[:train_end],
        'val': images[train_end:val_end],
        'test': images[val_end:]
    }
    
    logger.info(f"üìä –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ: train={len(splits['train'])}, val={len(splits['val'])}, test={len(splits['test'])}")
    
    # –ö–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–æ–≤
    for split_name, image_list in splits.items():
        split_img_dir = output_base_dir / split_name / "images"
        split_lbl_dir = output_base_dir / split_name / "labels"
        
        split_img_dir.mkdir(parents=True, exist_ok=True)
        split_lbl_dir.mkdir(parents=True, exist_ok=True)
        
        logger.info(f"üìÅ –ö–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ {split_name} –¥–∞–Ω–Ω—ã—Ö...")
        
        for image_path in tqdm(image_list, desc=f"–ö–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ {split_name}"):
            # –ö–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            dst_image = split_img_dir / image_path.name
            shutil.copy2(image_path, dst_image)
            
            # –ö–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏
            label_filename = image_path.stem + ".txt"
            src_label = labels_dir / label_filename
            dst_label = split_lbl_dir / label_filename
            
            if src_label.exists():
                shutil.copy2(src_label, dst_label)
            else:
                # –°–æ–∑–¥–∞–Ω–∏–µ –ø—É—Å—Ç–æ–≥–æ —Ñ–∞–π–ª–∞ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
                with open(dst_label, 'w', encoding='utf-8') as f:
                    pass
    
    logger.info("‚úÖ –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")


def create_dataset_yaml(dataset_dir: Path, class_names: List[str] = None):
    """
    –°–æ–∑–¥–∞–Ω–∏–µ YAML –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –¥–ª—è YOLO
    
    Args:
        dataset_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞
        class_names: –°–ø–∏—Å–æ–∫ –∏–º–µ–Ω –∫–ª–∞—Å—Å–æ–≤
    """
    logger = setup_logger(__name__)
    
    # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –∫–ª–∞—Å—Å—ã –¥–ª—è —Ä–µ—Å—Ç–æ—Ä–∞–Ω–Ω–æ–π —Å—Ä–µ–¥—ã
    if class_names is None:
        class_names = [
            'person',       # –õ—é–¥–∏
            'chair',        # –°—Ç—É–ª—å—è
            'dining_table', # –°—Ç–æ–ª—ã
            'cup',          # –ß–∞—à–∫–∏
            'bowl',         # –ú–∏—Å–∫–∏
            'bottle',       # –ë—É—Ç—ã–ª–∫–∏
            'wine_glass',   # –ë–æ–∫–∞–ª—ã
            'fork',         # –í–∏–ª–∫–∏
            'knife',        # –ù–æ–∂–∏
            'spoon',        # –õ–æ–∂–∫–∏
            'plate',        # –¢–∞—Ä–µ–ª–∫–∏
            'food',         # –ï–¥–∞
            'phone',        # –¢–µ–ª–µ—Ñ–æ–Ω—ã
            'book',         # –ö–Ω–∏–≥–∏
            'laptop'        # –ù–æ—É—Ç–±—É–∫–∏
        ]
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    yaml_config = {
        'path': str(dataset_dir.absolute()),
        'train': 'train/images',
        'val': 'val/images', 
        'test': 'test/images',
        'nc': len(class_names),
        'names': class_names
    }
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ YAML —Ñ–∞–π–ª–∞
    yaml_path = dataset_dir / "dataset.yaml"
    
    import yaml
    with open(yaml_path, 'w', encoding='utf-8') as f:
        yaml.dump(yaml_config, f, default_flow_style=False, allow_unicode=True)
    
    logger.info(f"‚úÖ –°–æ–∑–¥–∞–Ω YAML –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π —Ñ–∞–π–ª: {yaml_path}")
    logger.info(f"üìã –ö–ª–∞—Å—Å—ã ({len(class_names)}): {', '.join(class_names[:5])}...")


def load_config(config_path: Path = None) -> Dict[str, Any]:
    """–ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –ø–∞–π–ø–ª–∞–π–Ω–∞"""
    default_config = {
        'video_processing': {
            'fps_extraction': 2.0,
            'max_frames_per_video': 1000,
            'resize_frames': True,
            'target_size': [640, 640]
        },
        'annotation': {
            'confidence_threshold': 0.25,
            'create_empty_annotations': True,
            'use_ensemble_models': False,  # –û—Ç–∫–ª—é—á–µ–Ω–æ –¥–ª—è –±–∞–∑–æ–≤–æ–π –≤–µ—Ä—Å–∏–∏
            'ensemble_models': ['yolov8n.pt', 'yolov8s.pt']
        },
        'dataset': {
            'train_ratio': 0.7,
            'val_ratio': 0.2,
            'test_ratio': 0.1,
            'class_names': [
                'person', 'chair', 'dining_table', 'cup', 'bowl',
                'bottle', 'wine_glass', 'fork', 'knife', 'spoon',
                'plate', 'food', 'phone', 'book', 'laptop'
            ]
        }
    }
    
    if config_path and config_path.exists():
        logger = setup_logger(__name__)
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                user_config = json.load(f)
            
            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
            def update_nested_dict(base_dict, update_dict):
                for key, value in update_dict.items():
                    if key in base_dict and isinstance(base_dict[key], dict) and isinstance(value, dict):
                        update_nested_dict(base_dict[key], value)
                    else:
                        base_dict[key] = value
            
            update_nested_dict(default_config, user_config)
            logger.info(f"üìÅ –ó–∞–≥—Ä—É–∂–µ–Ω–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∏–∑ {config_path}")
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏: {e}")
            logger.info("üîß –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é")
    
    return default_config


class DataPipelineProcessor:
    """–û—Å–Ω–æ–≤–Ω–æ–π –∫–ª–∞—Å—Å –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.logger = setup_logger(self.__class__.__name__)
        
        # –î–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        self.raw_data_dir = Path("data/raw")
        self.processed_dir = Path("data/processed")
        self.dataset_dir = Path("data/processed/dataset")
        self.temp_frames_dir = Path("data/temp/frames")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.stats = {
            'start_time': time.time(),
            'total_videos': 0,
            'total_frames': 0,
            'total_annotations': 0,
            'stages_completed': []
        }
    
    def run_pipeline(self, input_dir: Path = None):
        """–ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ –ø–∞–π–ø–ª–∞–π–Ω–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö"""
        
        self.logger.info("üöÄ –ó–∞–ø—É—Å–∫ –ø–∞–π–ø–ª–∞–π–Ω–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö")
        
        try:
            # 1. –°–æ–∑–¥–∞–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π
            self.logger.info("üèóÔ∏è –≠—Ç–∞–ø 1: –°–æ–∑–¥–∞–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π")
            create_directory_structure()
            self.stats['stages_completed'].append('directory_structure')
            
            # 2. –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            if input_dir is None:
                input_dir = self.raw_data_dir
            
            input_dir = Path(input_dir)
            if not input_dir.exists():
                raise FileNotFoundError(f"–í—Ö–æ–¥–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {input_dir}")
            
            # 3. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–∞–¥—Ä–æ–≤ –∏–∑ –≤–∏–¥–µ–æ
            self.logger.info("üé¨ –≠—Ç–∞–ø 2: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–∞–¥—Ä–æ–≤ –∏–∑ –≤–∏–¥–µ–æ")
            self.temp_frames_dir.mkdir(parents=True, exist_ok=True)
            
            extracted_frames = extract_frames_from_videos(
                video_dir=input_dir,
                output_dir=self.temp_frames_dir,
                fps=self.config['video_processing']['fps_extraction']
            )
            
            if not extracted_frames:
                raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –∫–∞–¥—Ä—ã –∏–∑ –≤–∏–¥–µ–æ")
            
            self.stats['total_frames'] = len(extracted_frames)
            self.stats['stages_completed'].append('frame_extraction')
            
            # 4. –°–æ–∑–¥–∞–Ω–∏–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π
            self.logger.info("üìù –≠—Ç–∞–ø 3: –°–æ–∑–¥–∞–Ω–∏–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π")
            temp_labels_dir = Path("data/temp/labels")
            
            create_basic_annotations(
                image_paths=extracted_frames,
                labels_dir=temp_labels_dir
            )
            
            self.stats['total_annotations'] = len(extracted_frames)
            self.stats['stages_completed'].append('annotation_creation')
            
            # 5. –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞
            self.logger.info("üìä –≠—Ç–∞–ø 4: –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞")
            split_dataset(
                image_dir=self.temp_frames_dir,
                labels_dir=temp_labels_dir,
                output_base_dir=self.dataset_dir,
                train_ratio=self.config['dataset']['train_ratio'],
                val_ratio=self.config['dataset']['val_ratio'],
                test_ratio=self.config['dataset']['test_ratio']
            )
            
            self.stats['stages_completed'].append('dataset_split')
            
            # 6. –°–æ–∑–¥–∞–Ω–∏–µ YAML –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
            self.logger.info("‚öôÔ∏è –≠—Ç–∞–ø 5: –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞")
            create_dataset_yaml(
                dataset_dir=self.dataset_dir,
                class_names=self.config['dataset']['class_names']
            )
            
            self.stats['stages_completed'].append('yaml_creation')
            
            # 7. –û—á–∏—Å—Ç–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
            self.logger.info("üßπ –≠—Ç–∞–ø 6: –û—á–∏—Å—Ç–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤")
            self._cleanup_temp_files()
            
            # 8. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞
            self._generate_completion_report()
            
            self.logger.info("üéâ –ü–∞–π–ø–ª–∞–π–Ω –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ!")
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ –ø–∞–π–ø–ª–∞–π–Ω–µ: {e}")
            self._generate_error_report(e)
            raise
    
    def _cleanup_temp_files(self):
        """–û—á–∏—Å—Ç–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤"""
        temp_dirs = [
            Path("data/temp/frames"),
            Path("data/temp/labels"),
            Path("data/temp")
        ]
        
        for temp_dir in temp_dirs:
            if temp_dir.exists():
                shutil.rmtree(temp_dir)
                self.logger.info(f"üóëÔ∏è –£–¥–∞–ª–µ–Ω–∞ –≤—Ä–µ–º–µ–Ω–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {temp_dir}")
    
    def _generate_completion_report(self):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞ –æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏"""
        total_time = time.time() - self.stats['start_time']
        
        report = {
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
            'pipeline_status': 'completed',
            'total_execution_time_seconds': round(total_time, 2),
            'statistics': {
                'total_frames_processed': self.stats['total_frames'],
                'total_annotations_created': self.stats['total_annotations'],
                'stages_completed': self.stats['stages_completed']
            },
            'output_structure': {
                'dataset_directory': str(self.dataset_dir),
                'train_images': len(list((self.dataset_dir / 'train' / 'images').glob('*'))),
                'val_images': len(list((self.dataset_dir / 'val' / 'images').glob('*'))),
                'test_images': len(list((self.dataset_dir / 'test' / 'images').glob('*')))
            },
            'next_steps': [
                "–ó–∞–ø—É—Å—Ç–∏—Ç–µ train_model.py –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏",
                "–ü—Ä–æ–≤–µ—Ä—å—Ç–µ dataset.yaml –≤ data/processed/dataset/",
                "–ü—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –æ—Ç—Ä–µ–¥–∞–∫—Ç–∏—Ä—É–π—Ç–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ –≤—Ä—É—á–Ω—É—é"
            ]
        }
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–∞
        report_path = Path("data/processed/preparation_report.json")
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        self.logger.info(f"üìã –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {report_path}")
        self.logger.info(f"‚è±Ô∏è –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {total_time:.2f} —Å–µ–∫—É–Ω–¥")
        self.logger.info(f"üìä –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {self.stats['total_frames']} –∫–∞–¥—Ä–æ–≤")
    
    def _generate_error_report(self, error: Exception):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞ –æ–± –æ—à–∏–±–∫–µ"""
        total_time = time.time() - self.stats['start_time']
        
        error_report = {
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
            'pipeline_status': 'failed',
            'execution_time_seconds': round(total_time, 2),
            'error': {
                'type': type(error).__name__,
                'message': str(error),
                'stages_completed': self.stats['stages_completed']
            },
            'troubleshooting': [
                "–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –Ω–∞–ª–∏—á–∏–µ –≤–∏–¥–µ–æ—Ñ–∞–π–ª–æ–≤ –≤ data/raw/",
                "–£–±–µ–¥–∏—Ç–µ—Å—å –≤ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏",
                "–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ª–æ–≥–∏ –≤ —Ñ–∞–π–ª–µ logs/",
                "–£–±–µ–¥–∏—Ç–µ—Å—å –≤ –Ω–∞–ª–∏—á–∏–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π"
            ]
        }
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–∞ –æ–± –æ—à–∏–±–∫–µ
        error_report_path = Path("data/processed/error_report.json")
        with open(error_report_path, 'w', encoding='utf-8') as f:
            json.dump(error_report, f, ensure_ascii=False, indent=2)
        
        self.logger.error(f"üìã –û—Ç—á–µ—Ç –æ–± –æ—à–∏–±–∫–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {error_report_path}")


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    parser = argparse.ArgumentParser(
        description="–°–∫—Ä–∏–ø—Ç –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è YOLO11",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
–ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:

1. –ë–∞–∑–æ–≤–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
   python scripts/prepare_data.py --input "data/raw"

2. –° –∫–∞—Å—Ç–æ–º–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π:
   python scripts/prepare_data.py --input "data/raw" --config "config/my_config.json"

3. –° –∏–∑–º–µ–Ω–µ–Ω–∏–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è:
   python scripts/prepare_data.py --input "data/raw" --fps 1.5

–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø–æ—Å–ª–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è:
data/processed/dataset/
‚îú‚îÄ‚îÄ train/images & labels/
‚îú‚îÄ‚îÄ val/images & labels/
‚îú‚îÄ‚îÄ test/images & labels/
‚îî‚îÄ‚îÄ dataset.yaml
        """
    )
    
    parser.add_argument(
        '--input', 
        type=str, 
        default="data/raw",
        help='–ü—É—Ç—å –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —Å –≤—Ö–æ–¥–Ω—ã–º–∏ –≤–∏–¥–µ–æ —Ñ–∞–π–ª–∞–º–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: data/raw)'
    )
    
    parser.add_argument(
        '--config',
        type=str,
        help='–ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ JSON (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)'
    )
    
    parser.add_argument(
        '--fps',
        type=float,
        default=2.0,
        help='–ß–∞—Å—Ç–æ—Ç–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–∞–¥—Ä–æ–≤ (–∫–∞–¥—Ä–æ–≤/—Å–µ–∫—É–Ω–¥—É, –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 2.0)'
    )
    
    parser.add_argument(
        '--output',
        type=str,
        default="data/processed/dataset",
        help='–ü—É—Ç—å –∫ –≤—ã—Ö–æ–¥–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞'
    )
    
    args = parser.parse_args()
    
    try:
        # –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        config_path = Path(args.config) if args.config else None
        config = load_config(config_path)
        
        # –ü–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ FPS –µ—Å–ª–∏ –∑–∞–¥–∞–Ω–æ
        if args.fps != 2.0:
            config['video_processing']['fps_extraction'] = args.fps
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –∏ –∑–∞–ø—É—Å–∫ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞
        processor = DataPipelineProcessor(config)
        processor.dataset_dir = Path(args.output)
        
        # –ó–∞–ø—É—Å–∫ –ø–∞–π–ø–ª–∞–π–Ω–∞
        processor.run_pipeline(Path(args.input))
        
        print("\n" + "="*50)
        print("üéâ –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–• –ó–ê–í–ï–†–®–ï–ù–ê –£–°–ü–ï–®–ù–û!")
        print("="*50)
        print(f"üìÅ –î–∞—Ç–∞—Å–µ—Ç —Å–æ–∑–¥–∞–Ω –≤: {args.output}")
        print(f"‚öôÔ∏è –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è: {args.output}/dataset.yaml")
        print(f"üìã –û—Ç—á–µ—Ç: data/processed/preparation_report.json")
        print("\nüöÄ –°–ª–µ–¥—É—é—â–∏–π —à–∞–≥: –∑–∞–ø—É—Å—Ç–∏—Ç–µ train_model.py")
        print("="*50)
        
    except Exception as e:
        print(f"\n‚ùå –û–®–ò–ë–ö–ê: {e}")
        print("üìã –ü—Ä–æ–≤–µ—Ä—å—Ç–µ error_report.json –¥–ª—è –¥–µ—Ç–∞–ª–µ–π")
        sys.exit(1)


if __name__ == "__main__":
    main()