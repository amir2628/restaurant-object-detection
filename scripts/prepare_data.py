"""
–£–ª—É—á—à–µ–Ω–Ω—ã–π —Å–∫—Ä–∏–ø—Ç –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö —Å –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–π –∞–Ω–Ω–æ—Ç–∞—Ü–∏–µ–π
–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–æ–∑–¥–∞–µ—Ç –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è YOLO11
"""

import sys
import logging
import argparse
import os
import json
import shutil
import cv2
import numpy as np
from pathlib import Path
from typing import Dict, Any, List, Tuple
from tqdm import tqdm
import time

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–Ω–µ–≤—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –ø—Ä–æ–µ–∫—Ç–∞ –≤ sys.path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# –ò–º–ø–æ—Ä—Ç —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã—Ö –º–æ–¥—É–ª–µ–π
from src.utils.logger import setup_logger

def create_directory_structure():
    """–°–æ–∑–¥–∞–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π –ø—Ä–æ–µ–∫—Ç–∞"""
    logger = setup_logger(__name__)
    
    directories = [
        "data/raw",
        "data/processed/dataset/train/images",
        "data/processed/dataset/train/labels", 
        "data/processed/dataset/val/images",
        "data/processed/dataset/val/labels",
        "data/processed/dataset/test/images",
        "data/processed/dataset/test/labels",
        "data/annotations",
        "outputs/experiments",
        "outputs/inference", 
        "outputs/reports",
        "logs"
    ]
    
    for directory in directories:
        Path(directory).mkdir(parents=True, exist_ok=True)
        logger.info(f"‚úÖ –°–æ–∑–¥–∞–Ω–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {directory}")
    
    logger.info("üèóÔ∏è –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π —Å–æ–∑–¥–∞–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")


def extract_frames_from_videos(video_dir: Path, output_dir: Path, fps: float = 2.0) -> List[Path]:
    """
    –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–∞–¥—Ä–æ–≤ –∏–∑ –≤–∏–¥–µ–æ —Ñ–∞–π–ª–æ–≤
    
    Args:
        video_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å –≤–∏–¥–µ–æ —Ñ–∞–π–ª–∞–º–∏
        output_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∫–∞–¥—Ä–æ–≤
        fps: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∞–¥—Ä–æ–≤ –≤ —Å–µ–∫—É–Ω–¥—É –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è
        
    Returns:
        –°–ø–∏—Å–æ–∫ –ø—É—Ç–µ–π –∫ –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–º –∫–∞–¥—Ä–∞–º
    """
    logger = setup_logger(__name__)
    
    # –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã –≤–∏–¥–µ–æ
    video_extensions = {'.mp4', '.avi', '.mov', '.mkv', '.wmv', '.flv', '.webm'}
    
    # –ü–æ–∏—Å–∫ –≤–∏–¥–µ–æ —Ñ–∞–π–ª–æ–≤
    video_files = []
    for ext in video_extensions:
        video_files.extend(list(video_dir.glob(f"*{ext}")))
        video_files.extend(list(video_dir.glob(f"*{ext.upper()}")))
    
    if not video_files:
        logger.error(f"‚ùå –í–∏–¥–µ–æ —Ñ–∞–π–ª—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ {video_dir}")
        logger.info(f"–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã: {', '.join(video_extensions)}")
        return []
    
    logger.info(f"üé¨ –ù–∞–π–¥–µ–Ω–æ {len(video_files)} –≤–∏–¥–µ–æ —Ñ–∞–π–ª–æ–≤")
    
    extracted_frames = []
    total_frames = 0
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –≤—ã—Ö–æ–¥–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
    output_dir.mkdir(parents=True, exist_ok=True)
    
    for video_path in tqdm(video_files, desc="–û–±—Ä–∞–±–æ—Ç–∫–∞ –≤–∏–¥–µ–æ"):
        try:
            # –û—Ç–∫—Ä—ã—Ç–∏–µ –≤–∏–¥–µ–æ
            cap = cv2.VideoCapture(str(video_path))
            
            if not cap.isOpened():
                logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–∫—Ä—ã—Ç—å –≤–∏–¥–µ–æ: {video_path}")
                continue
            
            # –ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –≤–∏–¥–µ–æ
            video_fps = cap.get(cv2.CAP_PROP_FPS)
            total_video_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            duration = total_video_frames / video_fps if video_fps > 0 else 0
            
            logger.info(f"üìπ {video_path.name}: {duration:.1f}—Å, {video_fps:.1f} FPS")
            
            # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–∞–¥—Ä–æ–≤
            frame_interval = int(video_fps / fps) if video_fps > fps else 1
            
            frame_count = 0
            extracted_count = 0
            
            while True:
                ret, frame = cap.read()
                if not ret:
                    break
                
                # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–∞–¥—Ä–∞ —Å –∑–∞–¥–∞–Ω–Ω—ã–º –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–º
                if frame_count % frame_interval == 0:
                    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
                    frame_filename = f"{video_path.stem}_frame_{extracted_count:06d}.jpg"
                    frame_path = output_dir / frame_filename
                    
                    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–∞–¥—Ä–∞
                    cv2.imwrite(str(frame_path), frame)
                    extracted_frames.append(frame_path)
                    extracted_count += 1
                
                frame_count += 1
            
            cap.release()
            total_frames += extracted_count
            logger.info(f"‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–æ {extracted_count} –∫–∞–¥—Ä–æ–≤ –∏–∑ {video_path.name}")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {video_path}: {e}")
            continue
    
    logger.info(f"üéØ –í—Å–µ–≥–æ –∏–∑–≤–ª–µ—á–µ–Ω–æ {total_frames} –∫–∞–¥—Ä–æ–≤ –∏–∑ {len(video_files)} –≤–∏–¥–µ–æ")
    return extracted_frames


def create_basic_annotations(image_paths: List[Path], labels_dir: Path):
    """
    –°–æ–∑–¥–∞–Ω–∏–µ –±–∞–∑–æ–≤—ã—Ö –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π (–ø—É—Å—Ç—ã—Ö —Ñ–∞–π–ª–æ–≤) –¥–ª—è –≤—Å–µ—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
    
    Args:
        image_paths: –°–ø–∏—Å–æ–∫ –ø—É—Ç–µ–π –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º
        labels_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π
    """
    logger = setup_logger(__name__)
    
    labels_dir.mkdir(parents=True, exist_ok=True)
    
    logger.info(f"üìù –°–æ–∑–¥–∞–Ω–∏–µ –±–∞–∑–æ–≤—ã—Ö –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π –¥–ª—è {len(image_paths)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π...")
    
    for image_path in tqdm(image_paths, desc="–°–æ–∑–¥–∞–Ω–∏–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π"):
        # –°–æ–∑–¥–∞–Ω–∏–µ –ø—É—Å—Ç–æ–≥–æ —Ñ–∞–π–ª–∞ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏
        label_filename = image_path.stem + ".txt"
        label_path = labels_dir / label_filename
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –ø—É—Å—Ç–æ–≥–æ —Ñ–∞–π–ª–∞ (–±–µ–∑ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π)
        with open(label_path, 'w', encoding='utf-8') as f:
            pass  # –ü—É—Å—Ç–æ–π —Ñ–∞–π–ª
    
    logger.info(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(image_paths)} —Ñ–∞–π–ª–æ–≤ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π")


def split_dataset(image_dir: Path, labels_dir: Path, output_base_dir: Path, 
                 train_ratio: float = 0.7, val_ratio: float = 0.2, test_ratio: float = 0.1):
    """
    –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞ –Ω–∞ train/val/test
    
    Args:
        image_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏
        labels_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è–º–∏
        output_base_dir: –ë–∞–∑–æ–≤–∞—è –≤—ã—Ö–æ–¥–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è
        train_ratio: –î–æ–ª—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        val_ratio: –î–æ–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö  
        test_ratio: –î–æ–ª—è —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    """
    logger = setup_logger(__name__)
    
    # –ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
    image_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.tiff'}
    images = []
    
    for ext in image_extensions:
        images.extend(list(image_dir.glob(f"*{ext}")))
        images.extend(list(image_dir.glob(f"*{ext.upper()}")))
    
    if not images:
        logger.error(f"‚ùå –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ {image_dir}")
        return
    
    logger.info(f"üñºÔ∏è –ù–∞–π–¥–µ–Ω–æ {len(images)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è")
    
    # –ü–µ—Ä–µ–º–µ—à–∏–≤–∞–Ω–∏–µ –∏ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ
    import random
    random.seed(42)  # –î–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
    random.shuffle(images)
    
    total = len(images)
    train_end = int(total * train_ratio)
    val_end = train_end + int(total * val_ratio)
    
    splits = {
        'train': images[:train_end],
        'val': images[train_end:val_end],
        'test': images[val_end:]
    }
    
    logger.info(f"üìä –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ: train={len(splits['train'])}, val={len(splits['val'])}, test={len(splits['test'])}")
    
    # –ö–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–æ–≤
    for split_name, image_list in splits.items():
        split_img_dir = output_base_dir / split_name / "images"
        split_lbl_dir = output_base_dir / split_name / "labels"
        
        split_img_dir.mkdir(parents=True, exist_ok=True)
        split_lbl_dir.mkdir(parents=True, exist_ok=True)
        
        logger.info(f"üìÅ –ö–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ {split_name} –¥–∞–Ω–Ω—ã—Ö...")
        
        for image_path in tqdm(image_list, desc=f"–ö–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ {split_name}"):
            # –ö–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            dst_image = split_img_dir / image_path.name
            shutil.copy2(image_path, dst_image)
            
            # –ö–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏
            label_filename = image_path.stem + ".txt"
            src_label = labels_dir / label_filename
            dst_label = split_lbl_dir / label_filename
            
            if src_label.exists():
                shutil.copy2(src_label, dst_label)
            else:
                # –°–æ–∑–¥–∞–Ω–∏–µ –ø—É—Å—Ç–æ–≥–æ —Ñ–∞–π–ª–∞ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
                with open(dst_label, 'w', encoding='utf-8') as f:
                    pass
    
    logger.info("‚úÖ –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")


def create_dataset_yaml(dataset_dir: Path, class_names: List[str] = None):
    """
    –°–æ–∑–¥–∞–Ω–∏–µ YAML –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –¥–ª—è YOLO
    
    Args:
        dataset_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞
        class_names: –°–ø–∏—Å–æ–∫ –∏–º–µ–Ω –∫–ª–∞—Å—Å–æ–≤
    """
    logger = setup_logger(__name__)
    
    # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –∫–ª–∞—Å—Å—ã –¥–ª—è —Ä–µ—Å—Ç–æ—Ä–∞–Ω–Ω–æ–π —Å—Ä–µ–¥—ã
    if class_names is None:
        class_names = [
            'person',       # –õ—é–¥–∏
            'chair',        # –°—Ç—É–ª—å—è
            'dining_table', # –°—Ç–æ–ª—ã
            'cup',          # –ß–∞—à–∫–∏
            'bowl',         # –ú–∏—Å–∫–∏
            'bottle',       # –ë—É—Ç—ã–ª–∫–∏
            'wine_glass',   # –ë–æ–∫–∞–ª—ã
            'fork',         # –í–∏–ª–∫–∏
            'knife',        # –ù–æ–∂–∏
            'spoon',        # –õ–æ–∂–∫–∏
            'plate',        # –¢–∞—Ä–µ–ª–∫–∏
            'food',         # –ï–¥–∞
            'phone',        # –¢–µ–ª–µ—Ñ–æ–Ω—ã
            'book',         # –ö–Ω–∏–≥–∏
            'laptop'        # –ù–æ—É—Ç–±—É–∫–∏
        ]
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    yaml_config = {
        'path': str(dataset_dir.absolute()),
        'train': 'train/images',
        'val': 'val/images', 
        'test': 'test/images',
        'nc': len(class_names),
        'names': class_names
    }
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ YAML —Ñ–∞–π–ª–∞
    yaml_path = dataset_dir / "dataset.yaml"
    
    import yaml
    with open(yaml_path, 'w', encoding='utf-8') as f:
        yaml.dump(yaml_config, f, default_flow_style=False, allow_unicode=True)
    
    logger.info(f"‚úÖ –°–æ–∑–¥–∞–Ω YAML –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π —Ñ–∞–π–ª: {yaml_path}")
    logger.info(f"üìã –ö–ª–∞—Å—Å—ã ({len(class_names)}): {', '.join(class_names[:5])}...")


def load_config(config_path: Path = None) -> Dict[str, Any]:
    """–ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –ø–∞–π–ø–ª–∞–π–Ω–∞"""
    default_config = {
        'video_processing': {
            'fps_extraction': 2.0,
            'max_frames_per_video': 1000,
            'resize_frames': True,
            'target_size': [640, 640]
        },
        'annotation': {
            'confidence_threshold': 0.25,
            'create_empty_annotations': True,
            'use_ensemble_models': False,  # –û—Ç–∫–ª—é—á–µ–Ω–æ –¥–ª—è –±–∞–∑–æ–≤–æ–π –≤–µ—Ä—Å–∏–∏
            'ensemble_models': ['yolov8n.pt', 'yolov8s.pt']
        },
        'dataset': {
            'train_ratio': 0.7,
            'val_ratio': 0.2,
            'test_ratio': 0.1,
            'class_names': [
                'person', 'chair', 'dining_table', 'cup', 'bowl',
                'bottle', 'wine_glass', 'fork', 'knife', 'spoon',
                'plate', 'food', 'phone', 'book', 'laptop'
            ]
        }
    }
    
    if config_path and config_path.exists():
        logger = setup_logger(__name__)
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                user_config = json.load(f)
            
            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
            def update_nested_dict(base_dict, update_dict):
                for key, value in update_dict.items():
                    if key in base_dict and isinstance(base_dict[key], dict) and isinstance(value, dict):
                        update_nested_dict(base_dict[key], value)
                    else:
                        base_dict[key] = value
            
            update_nested_dict(default_config, user_config)
            logger.info(f"üìÅ –ó–∞–≥—Ä—É–∂–µ–Ω–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∏–∑ {config_path}")
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏: {e}")
            logger.info("üîß –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é")
    
    return default_config


class DataPipelineProcessor:
    """–û—Å–Ω–æ–≤–Ω–æ–π –∫–ª–∞—Å—Å –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.logger = setup_logger(self.__class__.__name__)
        
        # –î–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        self.raw_data_dir = Path("data/raw")
        self.processed_dir = Path("data/processed")
        self.dataset_dir = Path("data/processed/dataset")
        self.temp_frames_dir = Path("data/temp/frames")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.stats = {
            'start_time': time.time(),
            'total_videos': 0,
            'total_frames': 0,
            'total_annotations': 0,
            'stages_completed': []
        }
    
    def run_pipeline(self, input_dir: Path = None):
        """–ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ –ø–∞–π–ø–ª–∞–π–Ω–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö"""
        
        self.logger.info("üöÄ –ó–∞–ø—É—Å–∫ –ø–∞–π–ø–ª–∞–π–Ω–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö")
        
        try:
            # 1. –°–æ–∑–¥–∞–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π
            self.logger.info("üèóÔ∏è –≠—Ç–∞–ø 1: –°–æ–∑–¥–∞–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π")
            create_directory_structure()
            self.stats['stages_completed'].append('directory_structure')
            
            # 2. –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            if input_dir is None:
                input_dir = self.raw_data_dir
            
            input_dir = Path(input_dir)
            if not input_dir.exists():
                raise FileNotFoundError(f"–í—Ö–æ–¥–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {input_dir}")
            
            # 3. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–∞–¥—Ä–æ–≤ –∏–∑ –≤–∏–¥–µ–æ
            self.logger.info("üé¨ –≠—Ç–∞–ø 2: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–∞–¥—Ä–æ–≤ –∏–∑ –≤–∏–¥–µ–æ")
            self.temp_frames_dir.mkdir(parents=True, exist_ok=True)
            
            extracted_frames = extract_frames_from_videos(
                video_dir=input_dir,
                output_dir=self.temp_frames_dir,
                fps=self.config['video_processing']['fps_extraction']
            )
            
            if not extracted_frames:
                raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –∫–∞–¥—Ä—ã –∏–∑ –≤–∏–¥–µ–æ")
            
            self.stats['total_frames'] = len(extracted_frames)
            self.stats['stages_completed'].append('frame_extraction')
            
            # 4. –°–æ–∑–¥–∞–Ω–∏–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π
            self.logger.info("üìù –≠—Ç–∞–ø 3: –°–æ–∑–¥–∞–Ω–∏–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π")
            temp_labels_dir = Path("data/temp/labels")
            
            create_basic_annotations(
                image_paths=extracted_frames,
                labels_dir=temp_labels_dir
            )
            
            self.stats['total_annotations'] = len(extracted_frames)
            self.stats['stages_completed'].append('annotation_creation')
            
            # 5. –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞
            self.logger.info("üìä –≠—Ç–∞–ø 4: –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞")
            split_dataset(
                image_dir=self.temp_frames_dir,
                labels_dir=temp_labels_dir,
                output_base_dir=self.dataset_dir,
                train_ratio=self.config['dataset']['train_ratio'],
                val_ratio=self.config['dataset']['val_ratio'],
                test_ratio=self.config['dataset']['test_ratio']
            )
            
            self.stats['stages_completed'].append('dataset_split')
            
            # 6. –°–æ–∑–¥–∞–Ω–∏–µ YAML –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
            self.logger.info("‚öôÔ∏è –≠—Ç–∞–ø 5: –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞")
            create_dataset_yaml(
                dataset_dir=self.dataset_dir,
                class_names=self.config['dataset']['class_names']
            )
            
            self.stats['stages_completed'].append('yaml_creation')
            
            # 7. –û—á–∏—Å—Ç–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
            self.logger.info("üßπ –≠—Ç–∞–ø 6: –û—á–∏—Å—Ç–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤")
            self._cleanup_temp_files()
            
            # 8. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞
            self._generate_completion_report()
            
            self.logger.info("üéâ –ü–∞–π–ø–ª–∞–π–Ω –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ!")
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ –ø–∞–π–ø–ª–∞–π–Ω–µ: {e}")
            self._generate_error_report(e)
            raise
    
    def _cleanup_temp_files(self):
        """–û—á–∏—Å—Ç–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤"""
        temp_dirs = [
            Path("data/temp/frames"),
            Path("data/temp/labels"),
            Path("data/temp")
        ]
        
        for temp_dir in temp_dirs:
            if temp_dir.exists():
                shutil.rmtree(temp_dir)
                self.logger.info(f"üóëÔ∏è –£–¥–∞–ª–µ–Ω–∞ –≤—Ä–µ–º–µ–Ω–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {temp_dir}")
    
    def _generate_completion_report(self):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞ –æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏"""
        total_time = time.time() - self.stats['start_time']
        
        report = {
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
            'pipeline_status': 'completed',
            'total_execution_time_seconds': round(total_time, 2),
            'statistics': {
                'total_frames_processed': self.stats['total_frames'],
                'total_annotations_created': self.stats['total_annotations'],
                'stages_completed': self.stats['stages_completed']
            },
            'output_structure': {
                'dataset_directory': str(self.dataset_dir),
                'train_images': len(list((self.dataset_dir / 'train' / 'images').glob('*'))),
                'val_images': len(list((self.dataset_dir / 'val' / 'images').glob('*'))),
                'test_images': len(list((self.dataset_dir / 'test' / 'images').glob('*')))
            },
            'next_steps': [
                "–ó–∞–ø—É—Å—Ç–∏—Ç–µ train_model.py –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏",
                "–ü—Ä–æ–≤–µ—Ä—å—Ç–µ dataset.yaml –≤ data/processed/dataset/",
                "–ü—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –æ—Ç—Ä–µ–¥–∞–∫—Ç–∏—Ä—É–π—Ç–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ –≤—Ä—É—á–Ω—É—é"
            ]
        }
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–∞
        report_path = Path("data/processed/preparation_report.json")
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        self.logger.info(f"üìã –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {report_path}")
        self.logger.info(f"‚è±Ô∏è –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {total_time:.2f} —Å–µ–∫—É–Ω–¥")
        self.logger.info(f"üìä –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {self.stats['total_frames']} –∫–∞–¥—Ä–æ–≤")
    
    def _generate_error_report(self, error: Exception):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞ –æ–± –æ—à–∏–±–∫–µ"""
        total_time = time.time() - self.stats['start_time']
        
        error_report = {
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
            'pipeline_status': 'failed',
            'execution_time_seconds': round(total_time, 2),
            'error': {
                'type': type(error).__name__,
                'message': str(error),
                'stages_completed': self.stats['stages_completed']
            },
            'troubleshooting': [
                "–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –Ω–∞–ª–∏—á–∏–µ –≤–∏–¥–µ–æ—Ñ–∞–π–ª–æ–≤ –≤ data/raw/",
                "–£–±–µ–¥–∏—Ç–µ—Å—å –≤ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏",
                "–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ª–æ–≥–∏ –≤ —Ñ–∞–π–ª–µ logs/",
                "–£–±–µ–¥–∏—Ç–µ—Å—å –≤ –Ω–∞–ª–∏—á–∏–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π"
            ]
        }
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–∞ –æ–± –æ—à–∏–±–∫–µ
        error_report_path = Path("data/processed/error_report.json")
        with open(error_report_path, 'w', encoding='utf-8') as f:
            json.dump(error_report, f, ensure_ascii=False, indent=2)
        
        self.logger.error(f"üìã –û—Ç—á–µ—Ç –æ–± –æ—à–∏–±–∫–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {error_report_path}")


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    parser = argparse.ArgumentParser(
        description="–°–∫—Ä–∏–ø—Ç –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è YOLO11",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
–ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:

1. –ë–∞–∑–æ–≤–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
   python scripts/prepare_data.py --input "data/raw"

2. –° –∫–∞—Å—Ç–æ–º–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π:
   python scripts/prepare_data.py --input "data/raw" --config "config/my_config.json"

3. –° –∏–∑–º–µ–Ω–µ–Ω–∏–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è:
   python scripts/prepare_data.py --input "data/raw" --fps 1.5

–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø–æ—Å–ª–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è:
data/processed/dataset/
‚îú‚îÄ‚îÄ train/images & labels/
‚îú‚îÄ‚îÄ val/images & labels/
‚îú‚îÄ‚îÄ test/images & labels/
‚îî‚îÄ‚îÄ dataset.yaml
        """
    )
    
    parser.add_argument(
        '--input', 
        type=str, 
        default="data/raw",
        help='–ü—É—Ç—å –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —Å –≤—Ö–æ–¥–Ω—ã–º–∏ –≤–∏–¥–µ–æ —Ñ–∞–π–ª–∞–º–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: data/raw)'
    )
    
    parser.add_argument(
        '--config',
        type=str,
        help='–ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ JSON (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)'
    )
    
    parser.add_argument(
        '--fps',
        type=float,
        default=2.0,
        help='–ß–∞—Å—Ç–æ—Ç–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–∞–¥—Ä–æ–≤ (–∫–∞–¥—Ä–æ–≤/—Å–µ–∫—É–Ω–¥—É, –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 2.0)'
    )
    
    parser.add_argument(
        '--output',
        type=str,
        default="data/processed/dataset",
        help='–ü—É—Ç—å –∫ –≤—ã—Ö–æ–¥–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞'
    )
    
    args = parser.parse_args()
    
    try:
        # –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        config_path = Path(args.config) if args.config else None
        config = load_config(config_path)
        
        # –ü–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ FPS –µ—Å–ª–∏ –∑–∞–¥–∞–Ω–æ
        if args.fps != 2.0:
            config['video_processing']['fps_extraction'] = args.fps
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –∏ –∑–∞–ø—É—Å–∫ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞
        processor = DataPipelineProcessor(config)
        processor.dataset_dir = Path(args.output)
        
        # –ó–∞–ø—É—Å–∫ –ø–∞–π–ø–ª–∞–π–Ω–∞
        processor.run_pipeline(Path(args.input))
        
        print("\n" + "="*50)
        print("üéâ –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–• –ó–ê–í–ï–†–®–ï–ù–ê –£–°–ü–ï–®–ù–û!")
        print("="*50)
        print(f"üìÅ –î–∞—Ç–∞—Å–µ—Ç —Å–æ–∑–¥–∞–Ω –≤: {args.output}")
        print(f"‚öôÔ∏è –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è: {args.output}/dataset.yaml")
        print(f"üìã –û—Ç—á–µ—Ç: data/processed/preparation_report.json")
        print("\nüöÄ –°–ª–µ–¥—É—é—â–∏–π —à–∞–≥: –∑–∞–ø—É—Å—Ç–∏—Ç–µ train_model.py")
        print("="*50)
        
    except Exception as e:
        print(f"\n‚ùå –û–®–ò–ë–ö–ê: {e}")
        print("üìã –ü—Ä–æ–≤–µ—Ä—å—Ç–µ error_report.json –¥–ª—è –¥–µ—Ç–∞–ª–µ–π")
        sys.exit(1)


if __name__ == "__main__":
    main()