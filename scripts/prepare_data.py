"""
–£–ª—É—á—à–µ–Ω–Ω—ã–π —Å–∫—Ä–∏–ø—Ç –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö —Å GroundingDINO –∞–Ω–Ω–æ—Ç–∞—Ü–∏–µ–π
–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–æ–∑–¥–∞–µ—Ç –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è YOLO11
"""

import sys
import logging
import argparse
import os
import json
import shutil
import cv2
import numpy as np
from pathlib import Path
from typing import Dict, Any, List, Tuple
from tqdm import tqdm
import time

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–Ω–µ–≤—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –ø—Ä–æ–µ–∫—Ç–∞ –≤ sys.path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# –ò–º–ø–æ—Ä—Ç —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã—Ö –º–æ–¥—É–ª–µ–π
from src.utils.logger import setup_logger

def create_directory_structure():
    """–°–æ–∑–¥–∞–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π –ø—Ä–æ–µ–∫—Ç–∞"""
    logger = setup_logger(__name__)
    
    directories = [
        "data/raw",
        "data/processed/dataset/train/images",
        "data/processed/dataset/train/labels", 
        "data/processed/dataset/val/images",
        "data/processed/dataset/val/labels",
        "data/processed/dataset/test/images",
        "data/processed/dataset/test/labels",
        "data/annotations",
        "outputs/experiments",
        "outputs/inference", 
        "outputs/reports",
        "logs"
    ]
    
    for directory in directories:
        Path(directory).mkdir(parents=True, exist_ok=True)
        logger.info(f"‚úÖ –°–æ–∑–¥–∞–Ω–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {directory}")
    
    logger.info("üèóÔ∏è –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π —Å–æ–∑–¥–∞–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")


def create_dataset_yaml(dataset_dir: Path, class_names: List[str] = None):
    """
    –°–æ–∑–¥–∞–Ω–∏–µ YAML –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –¥–ª—è YOLO —Å GroundingDINO –∫–ª–∞—Å—Å–∞–º–∏
    
    Args:
        dataset_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞
        class_names: –°–ø–∏—Å–æ–∫ –∏–º–µ–Ω –∫–ª–∞—Å—Å–æ–≤
    """
    logger = setup_logger(__name__)
    
    # –°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∫–ª–∞—Å—Å—ã –¥–ª—è —Ä–µ—Å—Ç–æ—Ä–∞–Ω–Ω–æ–π —Å—Ä–µ–¥—ã (GroundingDINO)
    if class_names is None:
        class_names = [
            'chicken',     # –ö—É—Ä–∏—Ü–∞
            'meat',        # –ú—è—Å–æ
            'salad',       # –°–∞–ª–∞—Ç
            'soup',        # –°—É–ø
            'cup',         # –ß–∞—à–∫–∞
            'plate',       # –¢–∞—Ä–µ–ª–∫–∞
            'bowl',        # –ú–∏—Å–∫–∞
            'spoon',       # –õ–æ–∂–∫–∞
            'fork',        # –í–∏–ª–∫–∞
            'knife'        # –ù–æ–∂
        ]
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    yaml_config = {
        'path': str(dataset_dir.absolute()),
        'train': 'train/images',
        'val': 'val/images', 
        'test': 'test/images',
        'nc': len(class_names),
        'names': class_names
    }
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ YAML —Ñ–∞–π–ª–∞
    yaml_path = dataset_dir / "dataset.yaml"
    
    import yaml
    with open(yaml_path, 'w', encoding='utf-8') as f:
        yaml.dump(yaml_config, f, default_flow_style=False, allow_unicode=True)
    
    logger.info(f"‚úÖ –°–æ–∑–¥–∞–Ω YAML –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π —Ñ–∞–π–ª: {yaml_path}")
    logger.info(f"üìã –ö–ª–∞—Å—Å—ã ({len(class_names)}): {', '.join(class_names[:5])}...")


def load_config(config_path: Path = None) -> Dict[str, Any]:
    """–ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –ø–∞–π–ø–ª–∞–π–Ω–∞"""
    default_config = {
        'video_processing': {
            'fps_extraction': 2.0,
            'max_frames_per_video': 1000,
            'resize_frames': True,
            'target_size': [640, 640]
        },
        'annotation': {
            'method': 'groundingdino',
            'confidence_threshold': 0.25,
            'text_threshold': 0.25,
            'box_threshold': 0.25,
            'create_empty_annotations': True,
            'groundingdino_checkpoint': 'groundingdino_swinb_cogcoor.pth',
            'detection_prompt': 'chicken . meat . salad . soup . cup . plate . bowl . spoon . fork . knife .'
        },
        'dataset': {
            'train_ratio': 0.7,
            'val_ratio': 0.2,
            'test_ratio': 0.1,
            'class_names': [
                'chicken', 'meat', 'salad', 'soup', 'cup',
                'plate', 'bowl', 'spoon', 'fork', 'knife'
            ]
        },
        'quality_control': {
            'min_detection_size': 0.01,
            'max_detection_size': 0.8,
            'min_confidence': 0.15
        }
    }
    
    if config_path and config_path.exists():
        logger = setup_logger(__name__)
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                user_config = json.load(f)
            
            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
            def update_nested_dict(base_dict, update_dict):
                for key, value in update_dict.items():
                    if key in base_dict and isinstance(base_dict[key], dict) and isinstance(value, dict):
                        update_nested_dict(base_dict[key], value)
                    else:
                        base_dict[key] = value
            
            update_nested_dict(default_config, user_config)
            logger.info(f"‚úÖ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑: {config_path}")
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏: {e}")
            logger.info("–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é")
    
    return default_config


class DataPipelineProcessor:
    """–û—Å–Ω–æ–≤–Ω–æ–π –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –ø–∞–π–ø–ª–∞–π–Ω–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö —Å GroundingDINO"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.logger = setup_logger(self.__class__.__name__)
        self.dataset_dir = Path("data/processed/dataset")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.stats = {
            'start_time': time.time(),
            'total_videos': 0,
            'total_frames': 0,
            'total_annotations': 0,
            'stages_completed': []
        }
    
    def run_pipeline(self, input_dir: Path):
        """–ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ –ø–∞–π–ø–ª–∞–π–Ω–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö"""
        self.logger.info("üöÄ –ó–∞–ø—É—Å–∫ –ø–∞–π–ø–ª–∞–π–Ω–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö —Å GroundingDINO")
        self.stats['start_time'] = time.time()
        
        try:
            # 1. –°–æ–∑–¥–∞–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π
            self.logger.info("üèóÔ∏è –≠—Ç–∞–ø 1: –°–æ–∑–¥–∞–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π")
            create_directory_structure()
            self.stats['stages_completed'].append('directory_structure')
            
            # 2. –ü–æ–∏—Å–∫ –∏ –≤–∞–ª–∏–¥–∞—Ü–∏—è –≤–∏–¥–µ–æ —Ñ–∞–π–ª–æ–≤
            self.logger.info("üîç –≠—Ç–∞–ø 2: –ü–æ–∏—Å–∫ –≤–∏–¥–µ–æ —Ñ–∞–π–ª–æ–≤")
            video_files = self._find_video_files(input_dir)
            self.stats['total_videos'] = len(video_files)
            
            if not video_files:
                raise ValueError(f"–í–∏–¥–µ–æ —Ñ–∞–π–ª—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏: {input_dir}")
            
            # 3. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–∞–¥—Ä–æ–≤ –∏–∑ –≤–∏–¥–µ–æ
            self.logger.info("üé¨ –≠—Ç–∞–ø 3: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–∞–¥—Ä–æ–≤")
            all_frames = self._extract_frames_from_videos(video_files)
            self.stats['total_frames'] = len(all_frames)
            self.stats['stages_completed'].append('frame_extraction')
            
            # 4. –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/val/test
            self.logger.info("üìÇ –≠—Ç–∞–ø 4: –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/val/test")
            self._split_and_organize_dataset(all_frames)
            self.stats['stages_completed'].append('dataset_split')
            
            # 5. –ê–Ω–Ω–æ—Ç–∞—Ü–∏—è —Å GroundingDINO
            self.logger.info("üß† –≠—Ç–∞–ø 5: –ê–Ω–Ω–æ—Ç–∞—Ü–∏—è —Å GroundingDINO")
            self._annotate_with_groundingdino()
            self.stats['stages_completed'].append('annotation')
            
            # 6. –°–æ–∑–¥–∞–Ω–∏–µ YAML –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
            self.logger.info("üìÑ –≠—Ç–∞–ø 6: –°–æ–∑–¥–∞–Ω–∏–µ dataset.yaml")
            create_dataset_yaml(self.dataset_dir, self.config['dataset']['class_names'])
            self.stats['stages_completed'].append('yaml_creation')
            
            # 7. –û—á–∏—Å—Ç–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
            self.logger.info("üßπ –≠—Ç–∞–ø 7: –û—á–∏—Å—Ç–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤")
            self._cleanup_temp_files()
            
            # 8. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞
            self._generate_completion_report()
            
            self.logger.info("üéâ –ü–∞–π–ø–ª–∞–π–Ω –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ!")
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ –ø–∞–π–ø–ª–∞–π–Ω–µ: {e}")
            self._generate_error_report(e)
            raise
    
    def _find_video_files(self, input_dir: Path) -> List[Path]:
        """–ü–æ–∏—Å–∫ –≤–∏–¥–µ–æ —Ñ–∞–π–ª–æ–≤ –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏"""
        video_extensions = ['.mp4', '.avi', '.mov', '.mkv', '.wmv', '.flv', '.webm']
        video_files = []
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º set –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
        found_files = set()
        
        for ext in video_extensions:
            # –ü–æ–∏—Å–∫ —Ñ–∞–π–ª–æ–≤ —Å –Ω–∏–∂–Ω–∏–º —Ä–µ–≥–∏—Å—Ç—Ä–æ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è
            for file_path in input_dir.glob(f"*{ext}"):
                found_files.add(file_path)
            # –ü–æ–∏—Å–∫ —Ñ–∞–π–ª–æ–≤ —Å –≤–µ—Ä—Ö–Ω–∏–º —Ä–µ–≥–∏—Å—Ç—Ä–æ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è
            for file_path in input_dir.glob(f"*{ext.upper()}"):
                found_files.add(file_path)
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è set –≤ –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫
        video_files = sorted(list(found_files))
        
        self.logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(video_files)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –≤–∏–¥–µ–æ —Ñ–∞–π–ª–æ–≤")
        for video_file in video_files:
            self.logger.info(f"  - {video_file.name}")
        
        return video_files
    
    def _extract_frames_from_videos(self, video_files: List[Path]) -> List[Path]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–∞–¥—Ä–æ–≤ –∏–∑ –≤—Å–µ—Ö –≤–∏–¥–µ–æ"""
        all_frames = []
        fps_extraction = self.config['video_processing']['fps_extraction']
        max_frames = self.config['video_processing']['max_frames_per_video']
        
        temp_frames_dir = Path("data/temp/frames")
        temp_frames_dir.mkdir(parents=True, exist_ok=True)
        
        for video_file in tqdm(video_files, desc="–û–±—Ä–∞–±–æ—Ç–∫–∞ –≤–∏–¥–µ–æ"):
            try:
                frames = self._extract_frames_from_single_video(
                    video_file, temp_frames_dir, fps_extraction, max_frames
                )
                all_frames.extend(frames)
                
            except Exception as e:
                self.logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤–∏–¥–µ–æ {video_file}: {e}")
                continue
        
        self.logger.info(f"–ò–∑–≤–ª–µ—á–µ–Ω–æ {len(all_frames)} –∫–∞–¥—Ä–æ–≤")
        return all_frames
    
    def _extract_frames_from_single_video(self, video_path: Path, output_dir: Path, 
                                        fps: float, max_frames: int) -> List[Path]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–∞–¥—Ä–æ–≤ –∏–∑ –æ–¥–Ω–æ–≥–æ –≤–∏–¥–µ–æ"""
        cap = cv2.VideoCapture(str(video_path))
        if not cap.isOpened():
            raise ValueError(f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–∫—Ä—ã—Ç—å –≤–∏–¥–µ–æ: {video_path}")
        
        # –ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –≤–∏–¥–µ–æ
        video_fps = cap.get(cv2.CAP_PROP_FPS)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        
        # –†–∞—Å—á–µ—Ç –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è
        frame_interval = int(video_fps / fps) if fps > 0 else 1
        
        extracted_frames = []
        frame_count = 0
        saved_count = 0
        
        while True:
            ret, frame = cap.read()
            if not ret or saved_count >= max_frames:
                break
            
            if frame_count % frame_interval == 0:
                # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–∞–¥—Ä–∞
                frame_filename = f"{video_path.stem}_frame_{frame_count:06d}.jpg"
                frame_path = output_dir / frame_filename
                
                # –ò–∑–º–µ–Ω–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ –µ—Å–ª–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ
                if self.config['video_processing']['resize_frames']:
                    target_size = self.config['video_processing']['target_size']
                    frame = cv2.resize(frame, target_size)
                
                cv2.imwrite(str(frame_path), frame)
                extracted_frames.append(frame_path)
                saved_count += 1
            
            frame_count += 1
        
        cap.release()
        return extracted_frames
    
    def _split_and_organize_dataset(self, frame_paths: List[Path]):
        """–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –∫–∞–¥—Ä–æ–≤ –Ω–∞ train/val/test –∏ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è –≤ —Å—Ç—Ä—É–∫—Ç—É—Ä—É YOLO"""
        import random
        
        # –ü–µ—Ä–µ–º–µ—à–∏–≤–∞–Ω–∏–µ –∫–∞–¥—Ä–æ–≤
        frame_paths = frame_paths.copy()
        random.shuffle(frame_paths)
        
        # –†–∞—Å—á–µ—Ç —Ä–∞–∑–º–µ—Ä–æ–≤ splits
        total_frames = len(frame_paths)
        train_size = int(total_frames * self.config['dataset']['train_ratio'])
        val_size = int(total_frames * self.config['dataset']['val_ratio'])
        test_size = total_frames - train_size - val_size
        
        # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –∫–∞–¥—Ä–æ–≤
        train_frames = frame_paths[:train_size]
        val_frames = frame_paths[train_size:train_size + val_size]
        test_frames = frame_paths[train_size + val_size:]
        
        self.logger.info(f"–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ: train={len(train_frames)}, val={len(val_frames)}, test={len(test_frames)}")
        
        # –ö–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–æ–≤ –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        splits = {
            'train': train_frames,
            'val': val_frames,
            'test': test_frames
        }
        
        for split_name, frames in splits.items():
            split_images_dir = self.dataset_dir / split_name / "images"
            split_labels_dir = self.dataset_dir / split_name / "labels"
            
            split_images_dir.mkdir(parents=True, exist_ok=True)
            split_labels_dir.mkdir(parents=True, exist_ok=True)
            
            # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–π
            split_visualizations_dir = self.dataset_dir / split_name / "visualizations"
            split_visualizations_dir.mkdir(parents=True, exist_ok=True)
            
            for frame_path in tqdm(frames, desc=f"–ö–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ {split_name}"):
                # –ö–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                target_image_path = split_images_dir / frame_path.name
                shutil.copy2(frame_path, target_image_path)
                
                # –°–æ–∑–¥–∞–Ω–∏–µ –ø—É—Å—Ç–æ–≥–æ —Ñ–∞–π–ª–∞ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ (–±—É–¥–µ—Ç –∑–∞–ø–æ–ª–Ω–µ–Ω –Ω–∞ —Å–ª–µ–¥—É—é—â–µ–º —ç—Ç–∞–ø–µ)
                target_label_path = split_labels_dir / f"{frame_path.stem}.txt"
                target_label_path.touch()
    
    def _apply_massive_augmentation(self):
        """
        –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –º–∞—Å—Å–∏–≤–Ω–æ–π –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –¥–ª—è —É–≤–µ–ª–∏—á–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è –∞—É–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö
        """
        from src.data.augmentation import DataAugmentator
        
        self.logger.info("üé® –≠—Ç–∞–ø 5.5: –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –º–∞—Å—Å–∏–≤–Ω–æ–π –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞—É–≥–º–µ–Ω—Ç–∞—Ç–æ—Ä–∞
        augmentator = DataAugmentator()
        
        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö splits
        augmentation_config = {
            'train': {
                'factor': 6,  # –£–º–µ–Ω—å—à–∞–µ–º –¥–æ 6 –¥–ª—è –ª—É—á—à–µ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞
                'description': '–ò–Ω—Ç–µ–Ω—Å–∏–≤–Ω–∞—è –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è –¥–ª—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–≥–æ –Ω–∞–±–æ—Ä–∞'
            },
            'val': {
                'factor': 3,  # –£–º–µ—Ä–µ–Ω–Ω–∞—è –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è
                'description': '–£–º–µ—Ä–µ–Ω–Ω–∞—è –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ –Ω–∞–±–æ—Ä–∞'
            },
            'test': {
                'factor': 2,  # –õ–µ–≥–∫–∞—è –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è
                'description': '–õ–µ–≥–∫–∞—è –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è –¥–ª—è —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –Ω–∞–±–æ—Ä–∞'
            }
        }
        
        total_original_images = 0
        total_augmented_images = 0
        
        for split_name, aug_config in augmentation_config.items():
            self.logger.info(f"üîÑ –ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è {split_name} –Ω–∞–±–æ—Ä–∞ (—Ñ–∞–∫—Ç–æ—Ä: {aug_config['factor']})")
            
            # –ü—É—Ç–∏ –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è–º
            images_dir = self.dataset_dir / split_name / "images"
            labels_dir = self.dataset_dir / split_name / "labels"
            
            if not images_dir.exists():
                self.logger.warning(f"–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è {images_dir} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –ø—Ä–æ–ø—É—Å–∫...")
                continue
            
            # –ü–æ–∏—Å–∫ —Ç–æ–ª—å–∫–æ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π (–±–µ–∑ _aug_ –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏)
            all_images = list(images_dir.glob("*.jpg")) + list(images_dir.glob("*.png"))
            original_images = [img for img in all_images if "_aug_" not in img.name]
            original_count = len(original_images)
            total_original_images += original_count
            
            self.logger.info(f"  –ù–∞–π–¥–µ–Ω–æ {original_count} –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
            
            if original_count == 0:
                continue
            
            # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π
            augmented_count = 0
            factor = aug_config['factor']
            
            for original_image in tqdm(original_images, desc=f"–ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è {split_name}"):
                original_annotation = labels_dir / f"{original_image.stem}.txt"
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è –¥–ª—è –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                if not original_annotation.exists():
                    self.logger.warning(f"–ê–Ω–Ω–æ—Ç–∞—Ü–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –¥–ª—è {original_image}")
                    continue
                
                # –°–æ–∑–¥–∞–Ω–∏–µ multiple –∞—É–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –≤–µ—Ä—Å–∏–π
                for aug_idx in range(factor - 1):  # -1 –ø–æ—Ç–æ–º—É —á—Ç–æ –æ—Ä–∏–≥–∏–Ω–∞–ª —É–∂–µ –µ—Å—Ç—å
                    aug_image_name = f"{original_image.stem}_aug_{aug_idx:03d}{original_image.suffix}"
                    aug_annotation_name = f"{original_image.stem}_aug_{aug_idx:03d}.txt"
                    
                    aug_image_path = images_dir / aug_image_name
                    aug_annotation_path = labels_dir / aug_annotation_name
                    
                    try:
                        # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ —Å —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π
                        success = augmentator.augment_image_with_annotation(
                            str(original_image),
                            str(original_annotation),
                            str(aug_image_path),
                            str(aug_annotation_path)
                        )
                        
                        if success:
                            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ —Å–æ–∑–¥–∞–Ω–∞ –∏ –Ω–µ –ø—É—Å—Ç–∞
                            if aug_annotation_path.exists() and aug_annotation_path.stat().st_size > 0:
                                augmented_count += 1
                            else:
                                # –ï—Å–ª–∏ –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è –ø—É—Å—Ç–∞, –∫–æ–ø–∏—Ä—É–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—É—é –Ω–∞–ø—Ä—è–º—É—é
                                self.logger.warning(f"–ê–Ω–Ω–æ—Ç–∞—Ü–∏—è –ø–æ—Å–ª–µ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –ø—É—Å—Ç–∞ –¥–ª—è {original_image}, –∫–æ–ø–∏—Ä—É–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—É—é")
                                shutil.copy2(original_annotation, aug_annotation_path)
                                augmented_count += 1
                        else:
                            # –ï—Å–ª–∏ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è –Ω–µ —É–¥–∞–ª–∞—Å—å, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback –ø–æ–¥—Ö–æ–¥
                            self._fallback_augmentation(original_image, original_annotation, 
                                                       aug_image_path, aug_annotation_path)
                            augmented_count += 1
                            
                    except Exception as e:
                        self.logger.warning(f"–û—à–∏–±–∫–∞ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ {original_image}: {e}")
                        # –ò—Å–ø–æ–ª—å–∑—É–µ–º fallback –ø–æ–¥—Ö–æ–¥
                        try:
                            self._fallback_augmentation(original_image, original_annotation, 
                                                       aug_image_path, aug_annotation_path)
                            augmented_count += 1
                        except Exception as fallback_error:
                            self.logger.error(f"Fallback –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è —Ç–∞–∫–∂–µ –Ω–µ —É–¥–∞–ª–∞—Å—å –¥–ª—è {original_image}: {fallback_error}")
                            continue
            
            final_count = original_count + augmented_count
            total_augmented_images += final_count
            
            self.logger.info(f"  ‚úÖ {split_name}: {original_count} ‚Üí {final_count} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π "
                           f"(—É–≤–µ–ª–∏—á–µ–Ω–∏–µ –≤ {final_count/original_count:.1f} —Ä–∞–∑)")
        
        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        self.stats['total_frames'] = total_original_images
        self.stats['total_augmented_images'] = total_augmented_images
        self.stats['augmentation_factor'] = total_augmented_images / total_original_images if total_original_images > 0 else 1
        
        self.logger.info(f"üé® –ú–∞—Å—Å–∏–≤–Ω–∞—è –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
        self.logger.info(f"üìä –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
        self.logger.info(f"  - –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {total_original_images}")
        self.logger.info(f"  - –ò—Ç–æ–≥–æ–≤—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {total_augmented_images}")
        self.logger.info(f"  - –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —É–≤–µ–ª–∏—á–µ–Ω–∏—è: {self.stats['augmentation_factor']:.1f}x")
    
    def _fallback_augmentation(self, original_image: Path, original_annotation: Path,
                              aug_image_path: Path, aug_annotation_path: Path):
        """
        –ü—Ä–æ—Å—Ç–∞—è fallback –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è —Å –≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π
        """
        try:
            # –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø—Ä–æ—Å—Ç–∞—è –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            import cv2
            import random
            
            image = cv2.imread(str(original_image))
            if image is None:
                raise ValueError(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {original_image}")
            
            # –ü—Ä–æ—Å—Ç—ã–µ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ –Ω–∞—Ä—É—à–∞—é—Ç –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏
            augmented_image = image.copy()
            
            # –°–ª—É—á–∞–π–Ω–∞—è —è—Ä–∫–æ—Å—Ç—å
            if random.random() < 0.5:
                brightness = random.uniform(0.8, 1.2)
                augmented_image = np.clip(augmented_image * brightness, 0, 255).astype(np.uint8)
            
            # –°–ª—É—á–∞–π–Ω—ã–π –∫–æ–Ω—Ç—Ä–∞—Å—Ç
            if random.random() < 0.5:
                contrast = random.uniform(0.8, 1.2)
                mean = np.mean(augmented_image)
                augmented_image = np.clip((augmented_image - mean) * contrast + mean, 0, 255).astype(np.uint8)
            
            # –ù–µ–±–æ–ª—å—à–æ–µ —Ä–∞–∑–º—ã—Ç–∏–µ
            if random.random() < 0.3:
                augmented_image = cv2.GaussianBlur(augmented_image, (3, 3), 0)
            
            # –ì–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª—å–Ω–æ–µ –æ—Ç—Ä–∞–∂–µ–Ω–∏–µ —Å —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π
            flip_horizontal = random.random() < 0.5
            if flip_horizontal:
                augmented_image = cv2.flip(augmented_image, 1)
                # –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∏—Ä—É–µ–º –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ –¥–ª—è –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª—å–Ω–æ–≥–æ –æ—Ç—Ä–∞–∂–µ–Ω–∏—è
                self._flip_annotations_horizontal(original_annotation, aug_annotation_path)
            else:
                # –ü—Ä–æ—Å—Ç–æ –∫–æ–ø–∏—Ä—É–µ–º –∞–Ω–Ω–æ—Ç–∞—Ü–∏—é
                shutil.copy2(original_annotation, aug_annotation_path)
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∞—É–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            cv2.imwrite(str(aug_image_path), augmented_image)
            
            self.logger.debug(f"Fallback –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∞ –¥–ª—è {original_image}")
            
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –≤ fallback –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏: {e}")
            # –í –∫—Ä–∞–π–Ω–µ–º —Å–ª—É—á–∞–µ –ø—Ä–æ—Å—Ç–æ –∫–æ–ø–∏—Ä—É–µ–º —Ñ–∞–π–ª—ã
            shutil.copy2(original_image, aug_image_path)
            shutil.copy2(original_annotation, aug_annotation_path)
    
    def _flip_annotations_horizontal(self, original_annotation: Path, output_annotation: Path):
        """
        –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π –¥–ª—è –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª—å–Ω–æ–≥–æ –æ—Ç—Ä–∞–∂–µ–Ω–∏—è
        """
        try:
            with open(original_annotation, 'r', encoding='utf-8') as f:
                lines = f.readlines()
            
            transformed_lines = []
            for line in lines:
                line = line.strip()
                if line:
                    parts = line.split()
                    if len(parts) >= 5:
                        # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ class_id —ç—Ç–æ —Ü–µ–ª–æ–µ —á–∏—Å–ª–æ
                        try:
                            class_id = int(float(parts[0]))  # –°–Ω–∞—á–∞–ª–∞ float, –ø–æ—Ç–æ–º int –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ "4.0"
                        except ValueError:
                            class_id = int(parts[0])  # –ü—Ä—è–º–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –µ—Å–ª–∏ —ç—Ç–æ —É–∂–µ int
                            
                        x_center = float(parts[1])
                        y_center = float(parts[2])
                        width = float(parts[3])
                        height = float(parts[4])
                        
                        # –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è –¥–ª—è –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª—å–Ω–æ–≥–æ –æ—Ç—Ä–∞–∂–µ–Ω–∏—è
                        new_x_center = 1.0 - x_center
                        
                        # –°–æ—Ö—Ä–∞–Ω—è–µ–º class_id –∫–∞–∫ —Ü–µ–ª–æ–µ —á–∏—Å–ª–æ (–±–µ–∑ .0)
                        transformed_line = f"{class_id} {new_x_center:.6f} {y_center:.6f} {width:.6f} {height:.6f}\n"
                        transformed_lines.append(transformed_line)
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π
            with open(output_annotation, 'w', encoding='utf-8') as f:
                f.writelines(transformed_lines)
                
            self.logger.debug(f"–¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–æ {len(transformed_lines)} –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π –¥–ª—è {output_annotation}")
                
        except Exception as e:
            self.logger.warning(f"–û—à–∏–±–∫–∞ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π: {e}, –∫–æ–ø–∏—Ä—É–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª")
            shutil.copy2(original_annotation, output_annotation)
    
    def _validate_annotations(self):
        """
        –í–∞–ª–∏–¥–∞—Ü–∏—è –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π –ø–æ—Å–ª–µ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏
        """
        self.logger.info("üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π...")
        
        splits = ['train', 'val', 'test']
        validation_stats = {
            'total_images': 0,
            'images_with_annotations': 0,
            'empty_annotations': 0,
            'augmented_with_annotations': 0,
            'original_with_annotations': 0
        }
        
        for split in splits:
            images_dir = self.dataset_dir / split / "images"
            labels_dir = self.dataset_dir / split / "labels"
            
            if not images_dir.exists():
                continue
            
            # –ü–æ–∏—Å–∫ –≤—Å–µ—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
            image_files = list(images_dir.glob("*.jpg")) + list(images_dir.glob("*.png"))
            
            for image_file in image_files:
                validation_stats['total_images'] += 1
                annotation_file = labels_dir / f"{image_file.stem}.txt"
                
                if annotation_file.exists() and annotation_file.stat().st_size > 0:
                    validation_stats['images_with_annotations'] += 1
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∞—É–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —ç—Ç–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏–ª–∏ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–µ
                    if "_aug_" in image_file.name:
                        validation_stats['augmented_with_annotations'] += 1
                    else:
                        validation_stats['original_with_annotations'] += 1
                else:
                    validation_stats['empty_annotations'] += 1
                    
                    # –ï—Å–ª–∏ —ç—Ç–æ –∞—É–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –±–µ–∑ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ - –ø—Ä–æ–±–ª–µ–º–∞
                    if "_aug_" in image_file.name:
                        self.logger.warning(f"–ê—É–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –±–µ–∑ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏: {image_file}")
        
        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
        total = validation_stats['total_images']
        if total > 0:
            ann_rate = (validation_stats['images_with_annotations'] / total) * 100
            self.logger.info(f"üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π:")
            self.logger.info(f"  - –í—Å–µ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {total}")
            self.logger.info(f"  - –° –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è–º–∏: {validation_stats['images_with_annotations']} ({ann_rate:.1f}%)")
            self.logger.info(f"  - –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã—Ö —Å –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è–º–∏: {validation_stats['original_with_annotations']}")
            self.logger.info(f"  - –ê—É–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è–º–∏: {validation_stats['augmented_with_annotations']}")
            self.logger.info(f"  - –ü—É—Å—Ç—ã—Ö –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π: {validation_stats['empty_annotations']}")
            
            if ann_rate < 50:
                self.logger.warning("‚ö†Ô∏è –ù–∏–∑–∫–∏–π –ø—Ä–æ—Ü–µ–Ω—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è–º–∏!")
            
        self.stats['validation_stats'] = validation_stats
    
    def _fix_annotation_format(self):
        """
        –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ñ–æ—Ä–º–∞—Ç–∞ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π (float class_id -> int class_id)
        """
        self.logger.info("üîß –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ñ–æ—Ä–º–∞—Ç–∞ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π...")
        
        splits = ['train', 'val', 'test']
        fixed_files = 0
        
        for split in splits:
            labels_dir = self.dataset_dir / split / "labels"
            
            if not labels_dir.exists():
                continue
            
            # –ü–æ–∏—Å–∫ –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π
            annotation_files = list(labels_dir.glob("*.txt"))
            
            for annotation_file in annotation_files:
                try:
                    if annotation_file.stat().st_size == 0:
                        continue  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—É—Å—Ç—ã–µ —Ñ–∞–π–ª—ã
                    
                    # –ß—Ç–µ–Ω–∏–µ —Ç–µ–∫—É—â–∏—Ö –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π
                    with open(annotation_file, 'r', encoding='utf-8') as f:
                        lines = f.readlines()
                    
                    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ñ–æ—Ä–º–∞—Ç–∞
                    fixed_lines = []
                    needs_fix = False
                    
                    for line in lines:
                        line = line.strip()
                        if line:
                            parts = line.split()
                            if len(parts) >= 5:
                                try:
                                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ class_id float (–Ω–∞–ø—Ä–∏–º–µ—Ä "4.0")
                                    class_id_str = parts[0]
                                    if '.' in class_id_str:
                                        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º float –≤ int
                                        class_id = int(float(class_id_str))
                                        needs_fix = True
                                    else:
                                        class_id = int(class_id_str)
                                    
                                    # –û—Å—Ç–∞–ª—å–Ω—ã–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã
                                    x_center = float(parts[1])
                                    y_center = float(parts[2])
                                    width = float(parts[3])
                                    height = float(parts[4])
                                    
                                    # –°–æ–∑–¥–∞–µ–º –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—É—é —Å—Ç—Ä–æ–∫—É
                                    fixed_line = f"{class_id} {x_center:.6f} {y_center:.6f} {width:.6f} {height:.6f}\n"
                                    fixed_lines.append(fixed_line)
                                    
                                except ValueError as e:
                                    self.logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∏—Å–ø—Ä–∞–≤–∏—Ç—å —Å—Ç—Ä–æ–∫—É '{line}' –≤ {annotation_file}: {e}")
                                    fixed_lines.append(line + '\n' if not line.endswith('\n') else line)
                    
                    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ –µ—Å–ª–∏ –±—ã–ª–∏ –∏–∑–º–µ–Ω–µ–Ω–∏—è
                    if needs_fix:
                        with open(annotation_file, 'w', encoding='utf-8') as f:
                            f.writelines(fixed_lines)
                        fixed_files += 1
                        self.logger.debug(f"–ò—Å–ø—Ä–∞–≤–ª–µ–Ω —Ñ–æ—Ä–º–∞—Ç –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π –≤ {annotation_file}")
                        
                except Exception as e:
                    self.logger.warning(f"–û—à–∏–±–∫–∞ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è {annotation_file}: {e}")
                    continue
        
        if fixed_files > 0:
            self.logger.info(f"üîß –ò—Å–ø—Ä–∞–≤–ª–µ–Ω —Ñ–æ—Ä–º–∞—Ç –≤ {fixed_files} —Ñ–∞–π–ª–∞—Ö –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π")
        else:
            self.logger.info("‚úÖ –í—Å–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ —É–∂–µ –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ")
    
    def _annotate_with_groundingdino(self):
        """–ê–Ω–Ω–æ—Ç–∞—Ü–∏—è –≤—Å–µ—Ö splits —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º GroundingDINO"""
        from src.data.annotator import SmartAnnotator
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–Ω–Ω–æ—Ç–∞—Ç–æ—Ä–∞
        annotator = SmartAnnotator()
        
        splits = ['train', 'val', 'test']
        total_annotations = 0
        
        for split in splits:
            self.logger.info(f"–ê–Ω–Ω–æ—Ç–∞—Ü–∏—è {split} –Ω–∞–±–æ—Ä–∞...")
            
            images_dir = self.dataset_dir / split / "images"
            labels_dir = self.dataset_dir / split / "labels"
            
            if not images_dir.exists():
                self.logger.warning(f"–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è {images_dir} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –ø—Ä–æ–ø—É—Å–∫...")
                continue
            
            # –ó–∞–ø—É—Å–∫ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏
            try:
                stats = annotator.annotate_dataset(
                    images_dir=images_dir,
                    output_dir=labels_dir,
                    batch_size=self.config.get('performance', {}).get('batch_size', 1),
                    num_workers=self.config.get('performance', {}).get('num_workers', 1)
                )
                
                annotations_count = stats.get('total_detections', 0)
                total_annotations += annotations_count
                
                self.logger.info(f"‚úÖ {split}: {stats['processed_images']} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, "
                               f"{annotations_count} –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π")
                
            except Exception as e:
                self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ {split}: {e}")
                continue
        
        self.stats['total_annotations'] = total_annotations
        self.logger.info(f"üéØ –í—Å–µ–≥–æ —Å–æ–∑–¥–∞–Ω–æ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π: {total_annotations}")
    
    def run_pipeline(self, input_dir: Path):
        """–ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ –ø–∞–π–ø–ª–∞–π–Ω–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö —Å –º–∞—Å—Å–∏–≤–Ω–æ–π –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–µ–π"""
        self.logger.info("üöÄ –ó–∞–ø—É—Å–∫ –ø–∞–π–ø–ª–∞–π–Ω–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö —Å GroundingDINO –∏ –º–∞—Å—Å–∏–≤–Ω–æ–π –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–µ–π")
        self.stats['start_time'] = time.time()
        
        try:
            # 1. –°–æ–∑–¥–∞–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π
            self.logger.info("üèóÔ∏è –≠—Ç–∞–ø 1: –°–æ–∑–¥–∞–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π")
            create_directory_structure()
            self.stats['stages_completed'].append('directory_structure')
            
            # 2. –ü–æ–∏—Å–∫ –∏ –≤–∞–ª–∏–¥–∞—Ü–∏—è –≤–∏–¥–µ–æ —Ñ–∞–π–ª–æ–≤
            self.logger.info("üîç –≠—Ç–∞–ø 2: –ü–æ–∏—Å–∫ –≤–∏–¥–µ–æ —Ñ–∞–π–ª–æ–≤")
            video_files = self._find_video_files(input_dir)
            self.stats['total_videos'] = len(video_files)
            
            if not video_files:
                raise ValueError(f"–í–∏–¥–µ–æ —Ñ–∞–π–ª—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏: {input_dir}")
            
            # 3. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–∞–¥—Ä–æ–≤ –∏–∑ –≤–∏–¥–µ–æ
            self.logger.info("üé¨ –≠—Ç–∞–ø 3: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–∞–¥—Ä–æ–≤")
            all_frames = self._extract_frames_from_videos(video_files)
            self.stats['total_frames'] = len(all_frames)
            self.stats['stages_completed'].append('frame_extraction')
            
            # 4. –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/val/test
            self.logger.info("üìÇ –≠—Ç–∞–ø 4: –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/val/test")
            self._split_and_organize_dataset(all_frames)
            self.stats['stages_completed'].append('dataset_split')
            
            # 5. –ê–Ω–Ω–æ—Ç–∞—Ü–∏—è —Å GroundingDINO (–ø–µ—Ä–≤–æ–Ω–∞—á–∞–ª—å–Ω–∞—è)
            self.logger.info("üß† –≠—Ç–∞–ø 5: –ê–Ω–Ω–æ—Ç–∞—Ü–∏—è —Å GroundingDINO")
            self._annotate_with_groundingdino()
            self.stats['stages_completed'].append('annotation')
            
            # 5.5. –ú–ê–°–°–ò–í–ù–ê–Ø –ê–£–ì–ú–ï–ù–¢–ê–¶–ò–Ø (–Ω–æ–≤—ã–π —ç—Ç–∞–ø)
            self._apply_massive_augmentation()
            self.stats['stages_completed'].append('massive_augmentation')
            
            # 6. –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ñ–æ—Ä–º–∞—Ç–∞ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π
            self.logger.info("üîß –≠—Ç–∞–ø 6: –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ñ–æ—Ä–º–∞—Ç–∞ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π")
            self._fix_annotation_format()
            self.stats['stages_completed'].append('format_fix')
            
            # 7. –í–∞–ª–∏–¥–∞—Ü–∏—è –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π –ø–æ—Å–ª–µ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏
            self.logger.info("üîç –≠—Ç–∞–ø 7: –í–∞–ª–∏–¥–∞—Ü–∏—è –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π")
            self._validate_annotations()
            self.stats['stages_completed'].append('validation')
            
            # 8. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–π —Å bounding boxes
            self.logger.info("üñºÔ∏è –≠—Ç–∞–ø 8: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–π")
            self._generate_visualizations()
            self._create_visualization_summary()
            self.stats['stages_completed'].append('visualizations')
            
            # 9. –°–æ–∑–¥–∞–Ω–∏–µ YAML –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
            self.logger.info("üìÑ –≠—Ç–∞–ø 9: –°–æ–∑–¥–∞–Ω–∏–µ dataset.yaml")
            create_dataset_yaml(self.dataset_dir, self.config['dataset']['class_names'])
            self.stats['stages_completed'].append('yaml_creation')
            
            # 8. –û—á–∏—Å—Ç–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
            self.logger.info("üßπ –≠—Ç–∞–ø 8: –û—á–∏—Å—Ç–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤")
            self._cleanup_temp_files()
            
            # 9. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞
            self._generate_completion_report()
            
            self.logger.info("üéâ –ü–∞–π–ø–ª–∞–π–Ω –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö —Å –º–∞—Å—Å–∏–≤–Ω–æ–π –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–µ–π –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ!")
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ –ø–∞–π–ø–ª–∞–π–Ω–µ: {e}")
            self._generate_error_report(e)
            raise
    
    def _annotate_augmented_images(self):
        """
        –ê–Ω–Ω–æ—Ç–∞—Ü–∏—è —Ç–æ–ª—å–∫–æ –Ω–æ–≤—ã—Ö –∞—É–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π (–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è)
        """
        from src.data.annotator import SmartAnnotator
        
        self.logger.info("üîÑ –ê–Ω–Ω–æ—Ç–∞—Ü–∏—è –∞—É–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π...")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–Ω–Ω–æ—Ç–∞—Ç–æ—Ä–∞
        annotator = SmartAnnotator()
        
        splits = ['train', 'val', 'test']
        total_new_annotations = 0
        
        for split in splits:
            images_dir = self.dataset_dir / split / "images"
            labels_dir = self.dataset_dir / split / "labels"
            
            if not images_dir.exists():
                continue
            
            # –ü–æ–∏—Å–∫ —Ç–æ–ª—å–∫–æ –∞—É–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
            augmented_images = list(images_dir.glob("*_aug_*.jpg")) + list(images_dir.glob("*_aug_*.png"))
            
            if not augmented_images:
                self.logger.info(f"  {split}: –Ω–µ—Ç –∞—É–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏")
                continue
            
            self.logger.info(f"  {split}: –Ω–∞–π–¥–µ–Ω–æ {len(augmented_images)} –∞—É–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
            
            # –ê–Ω–Ω–æ—Ç–∞—Ü–∏—è —Ç–æ–ª—å–∫–æ –∞—É–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
            annotated_count = 0
            
            for aug_image in tqdm(augmented_images, desc=f"–ê–Ω–Ω–æ—Ç–∞—Ü–∏—è {split} –∞—É–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö"):
                aug_annotation = labels_dir / f"{aug_image.stem}.txt"
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —É–∂–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è
                if aug_annotation.exists() and aug_annotation.stat().st_size > 0:
                    continue  # –ê–Ω–Ω–æ—Ç–∞—Ü–∏—è —É–∂–µ –µ—Å—Ç—å
                
                try:
                    # –ê–Ω–Ω–æ—Ç–∞—Ü–∏—è –æ–¥–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                    detections = annotator._process_single_frame(aug_image)
                    
                    if detections:
                        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏
                        annotator._save_yolo_annotation(detections, aug_annotation, aug_image)
                        annotated_count += 1
                        total_new_annotations += len(detections)
                    else:
                        # –°–æ–∑–¥–∞–Ω–∏–µ –ø—É—Å—Ç–æ–π –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏
                        aug_annotation.touch()
                        
                except Exception as e:
                    self.logger.warning(f"–û—à–∏–±–∫–∞ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ {aug_image}: {e}")
                    # –°–æ–∑–¥–∞–Ω–∏–µ –ø—É—Å—Ç–æ–π –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
                    aug_annotation.touch()
            
            self.logger.info(f"  ‚úÖ {split}: –∞–Ω–Ω–æ—Ç–∏—Ä–æ–≤–∞–Ω–æ {annotated_count} –∞—É–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
        
        self.stats['total_annotations'] += total_new_annotations
        self.logger.info(f"üéØ –°–æ–∑–¥–∞–Ω–æ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π: {total_new_annotations}")
    
    def _generate_visualizations(self):
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –≤–∏–∑—É–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ bounding boxes –¥–ª—è –≤—Å–µ—Ö splits
        """
        self.logger.info("üñºÔ∏è –≠—Ç–∞–ø 6.5: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–π —Å bounding boxes")
        
        # –¶–≤–µ—Ç–∞ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤ (RGB)
        class_colors = {
            'chicken': (255, 165, 0),    # –û—Ä–∞–Ω–∂–µ–≤—ã–π
            'meat': (139, 69, 19),       # –ö–æ—Ä–∏—á–Ω–µ–≤—ã–π
            'salad': (0, 128, 0),        # –ó–µ–ª–µ–Ω—ã–π
            'soup': (255, 215, 0),       # –ó–æ–ª–æ—Ç–æ–π
            'cup': (70, 130, 180),       # –°—Ç–∞–ª—å–Ω–æ–π –≥–æ–ª—É–±–æ–π
            'plate': (220, 220, 220),    # –°–≤–µ—Ç–ª–æ-—Å–µ—Ä—ã–π
            'bowl': (255, 192, 203),     # –†–æ–∑–æ–≤—ã–π
            'spoon': (192, 192, 192),    # –°–µ—Ä–µ–±—Ä—è–Ω—ã–π
            'fork': (169, 169, 169),     # –¢–µ–º–Ω–æ-—Å–µ—Ä—ã–π
            'knife': (105, 105, 105)     # –¢—É—Å–∫–ª–æ-—Å–µ—Ä—ã–π
        }
        
        class_names = self.config['dataset']['class_names']
        splits = ['train', 'val', 'test']
        total_visualizations = 0
        
        for split in splits:
            self.logger.info(f"üé® –°–æ–∑–¥–∞–Ω–∏–µ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–π –¥–ª—è {split} –Ω–∞–±–æ—Ä–∞...")
            
            images_dir = self.dataset_dir / split / "images"
            labels_dir = self.dataset_dir / split / "labels"
            visualizations_dir = self.dataset_dir / split / "visualizations"
            
            if not images_dir.exists():
                continue
            
            # –ü–æ–∏—Å–∫ –≤—Å–µ—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
            image_files = list(images_dir.glob("*.jpg")) + list(images_dir.glob("*.png"))
            split_visualizations = 0
            
            for image_file in tqdm(image_files, desc=f"–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è {split}"):
                try:
                    # –°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π —Ñ–∞–π–ª –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏
                    annotation_file = labels_dir / f"{image_file.stem}.txt"
                    
                    # –ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                    image = cv2.imread(str(image_file))
                    if image is None:
                        continue
                    
                    height, width = image.shape[:2]
                    
                    # –ß—Ç–µ–Ω–∏–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π
                    if annotation_file.exists() and annotation_file.stat().st_size > 0:
                        with open(annotation_file, 'r', encoding='utf-8') as f:
                            lines = f.readlines()
                        
                        # –û—Ç—Ä–∏—Å–æ–≤–∫–∞ –∫–∞–∂–¥–æ–≥–æ bounding box
                        for line in lines:
                            line = line.strip()
                            if not line:
                                continue
                            
                            try:
                                parts = line.split()
                                if len(parts) != 5:
                                    continue
                                
                                # –ü—Ä–∞–≤–∏–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ class_id –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è float->int –æ—à–∏–±–æ–∫
                                try:
                                    class_id = int(float(parts[0]))  # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º "4.0" –∫–∞–∫ 4
                                except ValueError:
                                    class_id = int(parts[0])  # –ü—Ä—è–º–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –µ—Å–ª–∏ —ç—Ç–æ —É–∂–µ int
                                    
                                x_center = float(parts[1])
                                y_center = float(parts[2])
                                bbox_width = float(parts[3])
                                bbox_height = float(parts[4])
                                
                                # –í–∞–ª–∏–¥–∞—Ü–∏—è class_id
                                if 0 <= class_id < len(class_names):
                                    class_name = class_names[class_id]
                                    color = class_colors.get(class_name, (255, 255, 255))  # –ë–µ–ª—ã–π –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
                                    
                                    # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –∏–∑ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç –≤ –ø–∏–∫—Å–µ–ª–∏
                                    x1 = int((x_center - bbox_width/2) * width)
                                    y1 = int((y_center - bbox_height/2) * height)
                                    x2 = int((x_center + bbox_width/2) * width)
                                    y2 = int((y_center + bbox_height/2) * height)
                                    
                                    # –û–±–µ—Å–ø–µ—á–µ–Ω–∏–µ –≥—Ä–∞–Ω–∏—Ü –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                                    x1 = max(0, min(x1, width-1))
                                    y1 = max(0, min(y1, height-1))
                                    x2 = max(0, min(x2, width-1))
                                    y2 = max(0, min(y2, height-1))
                                    
                                    # –û—Ç—Ä–∏—Å–æ–≤–∫–∞ –ø—Ä—è–º–æ—É–≥–æ–ª—å–Ω–∏–∫–∞
                                    cv2.rectangle(image, (x1, y1), (x2, y2), color, 2)
                                    
                                    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–µ–∫—Å—Ç–∞ —Å –Ω–∞–∑–≤–∞–Ω–∏–µ–º –∫–ª–∞—Å—Å–∞
                                    label_text = f"{class_name}"
                                    label_size = cv2.getTextSize(label_text, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)[0]
                                    
                                    # –§–æ–Ω –¥–ª—è —Ç–µ–∫—Å—Ç–∞
                                    cv2.rectangle(image, 
                                                (x1, y1 - label_size[1] - 10), 
                                                (x1 + label_size[0], y1), 
                                                color, -1)
                                    
                                    # –¢–µ–∫—Å—Ç
                                    cv2.putText(image, label_text, 
                                              (x1, y1 - 5), 
                                              cv2.FONT_HERSHEY_SIMPLEX, 0.6, 
                                              (255, 255, 255), 2)
                                else:
                                    self.logger.warning(f"–ù–µ–≤–µ—Ä–Ω—ã–π class_id {class_id} –≤ {annotation_file}")
                                
                            except (ValueError, IndexError) as e:
                                self.logger.warning(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ –≤ {annotation_file}: {e}")
                                continue
                    
                    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤–∏–∑—É–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                    visualization_path = visualizations_dir / f"{image_file.stem}_visualized{image_file.suffix}"
                    cv2.imwrite(str(visualization_path), image)
                    split_visualizations += 1
                    
                except Exception as e:
                    self.logger.warning(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –¥–ª—è {image_file}: {e}")
                    continue
            
            total_visualizations += split_visualizations
            self.logger.info(f"  ‚úÖ {split}: —Å–æ–∑–¥–∞–Ω–æ {split_visualizations} –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–π")
        
        self.stats['total_visualizations'] = total_visualizations
        self.logger.info(f"üñºÔ∏è –í—Å–µ–≥–æ —Å–æ–∑–¥–∞–Ω–æ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–π: {total_visualizations}")
        self.logger.info(f"üìÅ –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è—Ö visualizations/ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ split")
    
    def _create_visualization_summary(self):
        """
        –°–æ–∑–¥–∞–Ω–∏–µ —Å–≤–æ–¥–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è—Ö
        """
        summary = {
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
            'visualization_info': {
                'total_visualizations': self.stats.get('total_visualizations', 0),
                'class_colors': {
                    'chicken': '#FFA500',    # –û—Ä–∞–Ω–∂–µ–≤—ã–π
                    'meat': '#8B4513',       # –ö–æ—Ä–∏—á–Ω–µ–≤—ã–π
                    'salad': '#008000',      # –ó–µ–ª–µ–Ω—ã–π
                    'soup': '#FFD700',       # –ó–æ–ª–æ—Ç–æ–π
                    'cup': '#4682B4',        # –°—Ç–∞–ª—å–Ω–æ–π –≥–æ–ª—É–±–æ–π
                    'plate': '#DCDCDC',      # –°–≤–µ—Ç–ª–æ-—Å–µ—Ä—ã–π
                    'bowl': '#FFC0CB',       # –†–æ–∑–æ–≤—ã–π
                    'spoon': '#C0C0C0',      # –°–µ—Ä–µ–±—Ä—è–Ω—ã–π
                    'fork': '#A9A9A9',       # –¢–µ–º–Ω–æ-—Å–µ—Ä—ã–π
                    'knife': '#696969'       # –¢—É—Å–∫–ª–æ-—Å–µ—Ä—ã–π
                },
                'splits': {}
            },
            'usage_instructions': [
                "–û—Ç–∫—Ä–æ–π—Ç–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ train/visualizations/, val/visualizations/, test/visualizations/",
                "–ö–∞–∂–¥–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –æ—Ä–∏–≥–∏–Ω–∞–ª —Å –æ—Ç—Ä–∏—Å–æ–≤–∞–Ω–Ω—ã–º–∏ bounding boxes",
                "–¶–≤–µ—Ç–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—Ç —Ä–∞–∑–Ω—ã–º –∫–ª–∞—Å—Å–∞–º –æ–±—ä–µ–∫—Ç–æ–≤",
                "–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π",
                "–§–∞–π–ª—ã –Ω–∞–∑–≤–∞–Ω—ã –∫–∞–∫: original_name_visualized.jpg"
            ]
        }
        
        # –ü–æ–¥—Å—á–µ—Ç –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–π –ø–æ splits
        for split in ['train', 'val', 'test']:
            vis_dir = self.dataset_dir / split / "visualizations"
            if vis_dir.exists():
                vis_count = len(list(vis_dir.glob("*_visualized.*")))
                summary['visualization_info']['splits'][split] = vis_count
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–≤–æ–¥–∫–∏
        summary_path = self.dataset_dir / "visualization_summary.json"
        with open(summary_path, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        
        self.logger.info(f"üìã –°–≤–æ–¥–∫–∞ –ø–æ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {summary_path}")
    
    def _cleanup_temp_files(self):
        """–û—á–∏—Å—Ç–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤"""
        temp_dirs = [
            Path("data/temp/frames"),
            Path("data/temp/labels"),
            Path("data/temp")
        ]
        
        for temp_dir in temp_dirs:
            if temp_dir.exists():
                shutil.rmtree(temp_dir)
                self.logger.info(f"üóëÔ∏è –£–¥–∞–ª–µ–Ω–∞ –≤—Ä–µ–º–µ–Ω–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {temp_dir}")
    
    def _generate_completion_report(self):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞ –æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏"""
        total_time = time.time() - self.stats['start_time']
        
        report = {
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
            'pipeline_status': 'completed',
            'total_execution_time_seconds': round(total_time, 2),
            'annotation_method': 'GroundingDINO',
            'statistics': {
                'total_videos_processed': self.stats['total_videos'],
                'total_frames_processed': self.stats['total_frames'],
                'total_annotations_created': self.stats['total_annotations'],
                'total_augmented_images': self.stats.get('total_augmented_images', 0),
                'total_visualizations': self.stats.get('total_visualizations', 0),
                'augmentation_factor': self.stats.get('augmentation_factor', 1.0),
                'stages_completed': self.stats['stages_completed']
            },
            'output_structure': {
                'dataset_directory': str(self.dataset_dir),
                'train_images': len(list((self.dataset_dir / 'train' / 'images').glob('*'))),
                'val_images': len(list((self.dataset_dir / 'val' / 'images').glob('*'))),
                'test_images': len(list((self.dataset_dir / 'test' / 'images').glob('*'))),
                'train_visualizations': len(list((self.dataset_dir / 'train' / 'visualizations').glob('*'))) if (self.dataset_dir / 'train' / 'visualizations').exists() else 0,
                'val_visualizations': len(list((self.dataset_dir / 'val' / 'visualizations').glob('*'))) if (self.dataset_dir / 'val' / 'visualizations').exists() else 0,
                'test_visualizations': len(list((self.dataset_dir / 'test' / 'visualizations').glob('*'))) if (self.dataset_dir / 'test' / 'visualizations').exists() else 0
            },
            'classes_used': self.config['dataset']['class_names'],
            'groundingdino_config': {
                'checkpoint': self.config['annotation']['groundingdino_checkpoint'],
                'prompt': self.config['annotation']['detection_prompt'],
                'confidence_threshold': self.config['annotation']['confidence_threshold']
            },
            'next_steps': [
                "–ó–∞–ø—É—Å—Ç–∏—Ç–µ train_model.py –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏",
                "–ü—Ä–æ–≤–µ—Ä—å—Ç–µ dataset.yaml –≤ data/processed/dataset/",
                "–ü—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –æ—Ç—Ä–µ–¥–∞–∫—Ç–∏—Ä—É–π—Ç–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ –≤—Ä—É—á–Ω—É—é",
                "–£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ —Ñ–∞–π–ª groundingdino_swinb_cogcoor.pth –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ –∫–æ—Ä–Ω–µ –ø—Ä–æ–µ–∫—Ç–∞"
            ]
        }
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–∞
        report_path = Path("data/processed/preparation_report.json")
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        self.logger.info(f"üìã –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {report_path}")
        self.logger.info(f"‚è±Ô∏è –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {total_time:.2f} —Å–µ–∫—É–Ω–¥")
        self.logger.info(f"üìä –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {self.stats['total_frames']} –∫–∞–¥—Ä–æ–≤")
        self.logger.info(f"üéØ –°–æ–∑–¥–∞–Ω–æ {self.stats['total_annotations']} –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π")
    
    def _generate_error_report(self, error: Exception):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞ –æ–± –æ—à–∏–±–∫–µ"""
        total_time = time.time() - self.stats['start_time']
        
        error_report = {
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
            'pipeline_status': 'failed',
            'error_message': str(error),
            'error_type': type(error).__name__,
            'execution_time_seconds': round(total_time, 2),
            'stages_completed': self.stats['stages_completed'],
            'statistics_before_error': {
                'total_videos': self.stats['total_videos'],
                'total_frames': self.stats['total_frames'],
                'total_annotations': self.stats['total_annotations']
            },
            'troubleshooting_tips': [
                "–£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ —Ñ–∞–π–ª groundingdino_swinb_cogcoor.pth –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ –∫–æ—Ä–Ω–µ –ø—Ä–æ–µ–∫—Ç–∞",
                "–ü—Ä–æ–≤–µ—Ä—å—Ç–µ —É—Å—Ç–∞–Ω–æ–≤–∫—É groundingdino-py: pip install groundingdino-py",
                "–£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –µ—Å—Ç—å –≤–∏–¥–µ–æ —Ñ–∞–π–ª—ã –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ data/raw/",
                "–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –Ω–∞–ª–∏—á–∏–µ —Å–≤–æ–±–æ–¥–Ω–æ–≥–æ –º–µ—Å—Ç–∞ –Ω–∞ –¥–∏—Å–∫–µ",
                "–ü—Ä–∏ –ø—Ä–æ–±–ª–µ–º–∞—Ö —Å GPU –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å CPU —Ä–µ–∂–∏–º"
            ]
        }
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–∞ –æ–± –æ—à–∏–±–∫–µ
        error_report_path = Path("data/processed/error_report.json")
        with open(error_report_path, 'w', encoding='utf-8') as f:
            json.dump(error_report, f, ensure_ascii=False, indent=2)
        
        self.logger.error(f"üí• –û—Ç—á–µ—Ç –æ–± –æ—à–∏–±–∫–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {error_report_path}")


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    parser = argparse.ArgumentParser(
        description="–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è YOLO11 —Å GroundingDINO –∞–Ω–Ω–æ—Ç–∞—Ü–∏–µ–π",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
–ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:

1. –ë–∞–∑–æ–≤–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
   python scripts/prepare_data.py --input "data/raw"

2. –° –∫–∞—Å—Ç–æ–º–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π:
   python scripts/prepare_data.py --input "data/raw" --config "config/pipeline_config.json"

3. –° –∏–∑–º–µ–Ω–µ–Ω–∏–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è:
   python scripts/prepare_data.py --input "data/raw" --fps 1.5

–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è:
- –§–∞–π–ª groundingdino_swinb_cogcoor.pth –≤ –∫–æ—Ä–Ω–µ –ø—Ä–æ–µ–∫—Ç–∞
- –£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã–π groundingdino-py: pip install groundingdino-py
- –í–∏–¥–µ–æ —Ñ–∞–π–ª—ã –≤ —É–∫–∞–∑–∞–Ω–Ω–æ–π input –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏

–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø–æ—Å–ª–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è:
data/processed/dataset/
‚îú‚îÄ‚îÄ train/images & labels/
‚îú‚îÄ‚îÄ val/images & labels/  
‚îú‚îÄ‚îÄ test/images & labels/
‚îî‚îÄ‚îÄ dataset.yaml
        """
    )
    
    parser.add_argument(
        '--input', 
        type=str, 
        default="data/raw",
        help='–ü—É—Ç—å –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —Å –≤—Ö–æ–¥–Ω—ã–º–∏ –≤–∏–¥–µ–æ —Ñ–∞–π–ª–∞–º–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: data/raw)'
    )
    
    parser.add_argument(
        '--config',
        type=str,
        help='–ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ JSON (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)'
    )
    
    parser.add_argument(
        '--fps',
        type=float,
        default=2.0,
        help='–ß–∞—Å—Ç–æ—Ç–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–∞–¥—Ä–æ–≤ (–∫–∞–¥—Ä–æ–≤/—Å–µ–∫—É–Ω–¥—É, –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 2.0)'
    )
    
    parser.add_argument(
        '--output',
        type=str,
        default="data/processed/dataset",
        help='–ü—É—Ç—å –∫ –≤—ã—Ö–æ–¥–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞'
    )
    
    parser.add_argument(
        '--confidence',
        type=float,
        default=0.25,
        help='–ü–æ—Ä–æ–≥ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –¥–ª—è GroundingDINO (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 0.25)'
    )
    
    args = parser.parse_args()
    
    try:
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è GroundingDINO –º–æ–¥–µ–ª–∏
        groundingdino_path = Path("groundingdino_swinb_cogcoor.pth")
        if not groundingdino_path.exists():
            print("\n‚ùå –û–®–ò–ë–ö–ê: –§–∞–π–ª –º–æ–¥–µ–ª–∏ GroundingDINO –Ω–µ –Ω–∞–π–¥–µ–Ω!")
            print(f"–û–∂–∏–¥–∞–µ—Ç—Å—è —Ñ–∞–π–ª: {groundingdino_path.absolute()}")
            print("\n–°–∫–∞—á–∞–π—Ç–µ –º–æ–¥–µ–ª—å:")
            print("wget https://github.com/IDEA-Research/GroundingDINO/releases/download/v0.1.0-alpha/groundingdino_swinb_cogcoor.pth")
            sys.exit(1)
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—Ö–æ–¥–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        input_dir = Path(args.input)
        if not input_dir.exists():
            print(f"\n‚ùå –û–®–ò–ë–ö–ê: –í—Ö–æ–¥–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {input_dir}")
            print("–°–æ–∑–¥–∞–π—Ç–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –∏ –ø–æ–º–µ—Å—Ç–∏—Ç–µ –≤ –Ω–µ–µ –≤–∏–¥–µ–æ —Ñ–∞–π–ª—ã")
            sys.exit(1)
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        config_path = Path(args.config) if args.config else None
        config = load_config(config_path)
        
        # –ü–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏–∑ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤
        if args.fps != 2.0:
            config['video_processing']['fps_extraction'] = args.fps
        
        if args.confidence != 0.25:
            config['annotation']['confidence_threshold'] = args.confidence
            config['annotation']['text_threshold'] = args.confidence
            config['annotation']['box_threshold'] = args.confidence
        
        # –í–∫–ª—é—á–µ–Ω–∏–µ –º–∞—Å—Å–∏–≤–Ω–æ–π –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        if 'augmentation' not in config:
            config['augmentation'] = {'enabled': True}
        
        config['dataset']['enable_massive_augmentation'] = True
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –∏ –∑–∞–ø—É—Å–∫ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞
        processor = DataPipelineProcessor(config)
        processor.dataset_dir = Path(args.output)
        
        # –ó–∞–ø—É—Å–∫ –ø–∞–π–ø–ª–∞–π–Ω–∞
        processor.run_pipeline(input_dir)
        
        print("\n" + "="*60)
        print("üéâ –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–• –° GROUNDINGDINO –ó–ê–í–ï–†–®–ï–ù–ê –£–°–ü–ï–®–ù–û!")
        print("="*60)
        print(f"üìÅ –î–∞—Ç–∞—Å–µ—Ç —Å–æ–∑–¥–∞–Ω –≤: {args.output}")
        print(f"‚öôÔ∏è –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è: {args.output}/dataset.yaml")
        print(f"üìã –û—Ç—á–µ—Ç: data/processed/preparation_report.json")
        print(f"üß† –ú–µ—Ç–æ–¥ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏: GroundingDINO")
        print(f"üéØ –ö–ª–∞—Å—Å—ã: chicken, meat, salad, soup, cup, plate, bowl, spoon, fork, knife")
        print("\nüöÄ –°–ª–µ–¥—É—é—â–∏–π —à–∞–≥: –∑–∞–ø—É—Å—Ç–∏—Ç–µ train_model.py")
        print("="*60)
        
    except Exception as e:
        print(f"\n‚ùå –û–®–ò–ë–ö–ê: {e}")
        print("üìã –ü—Ä–æ–≤–µ—Ä—å—Ç–µ error_report.json –¥–ª—è –¥–µ—Ç–∞–ª–µ–π")
        print("\nüí° –ß–∞—Å—Ç—ã–µ –ø—Ä–æ–±–ª–µ–º—ã:")
        print("- –û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç —Ñ–∞–π–ª groundingdino_swinb_cogcoor.pth")
        print("- –ù–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω groundingdino-py")
        print("- –ù–µ—Ç –≤–∏–¥–µ–æ —Ñ–∞–π–ª–æ–≤ –≤ –≤—Ö–æ–¥–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏")
        print("- –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –º–µ—Å—Ç–∞ –Ω–∞ –¥–∏—Å–∫–µ")
        sys.exit(1)


if __name__ == "__main__":
    main()